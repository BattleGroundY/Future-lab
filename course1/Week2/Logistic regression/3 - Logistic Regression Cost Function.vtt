WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.530
이전 동영상에서

2
00 : 00 : 01.530 -> 00 : 00 : 04.227
logistic regression 모델에 대해서 이해했습니다.

3
00 : 00 : 04.227 -> 00 : 00 : 07.526
logistic regression 모델의 
매개 변수 W와 b를 훈련하기 위해서는,

4
00 : 00 : 07.526 -> 00 : 00 : 10.570
Cost 함수를 정의 해야합니다.

5
00 : 00 : 10.570 -> 00 : 00 : 14.430
그럼 logistic regression 학습에 사용할 
cost 함수에 대해 알아 보겠습니다.

6
00 : 00 : 14.430 -> 00 : 00 : 18.195
복습해보면, 이것이 이전의 슬라이드에서 발췌 한 식입니다.

7
00 : 00 : 18.195 -> 00 : 00 : 20.792
출력 y hat 은

8
00 : 00 : 20.792 -> 00 : 00 : 24.690
w^T x + b에 sigmoid 함수를 적용한 것으로, 
sigmoid에서 z의 정의는 이것 입니다.

9
00 : 00 : 24.690 -> 00 : 00 : 27.600
여기서 당신 모델의 parameter를 학습하기 위해

10
00 : 00 : 27.600 -> 00 : 00 : 31.200
m 개의 샘플을 포함 training 데이터 세트가 주어지고 있습니다.

11
00 : 00 : 31.200 -> 00 : 00 : 34.060
당신이 매개 변수 W와 b를 찾기 위해서는

12
00 : 00 : 34.060 -> 00 : 00 : 37.781
최소한 그 training 데이터에 포함 된 
출력 값이 있어야 합니다.

13
00 : 00 : 37.781 -> 00 : 00 : 40.225
training 데이터로부터 얻어진 예측 값은

14
00 : 00 : 40.225 -> 00 : 00 : 43.260
여기에서 y^hat (i)로 표기하고 또

15
00 : 00 : 43.260 -> 00 : 00 : 47.720
training 데이터에 포함 된 정답 값을 
나타내는 라벨을 y(i)라고 기재합니다.

16
00 : 00 : 47.720 -> 00 : 00 : 52.110
식의 처음을 좀 더 자세히 확인 해 보면,

17
00 : 00 : 52.110 -> 00 : 00 : 56.205
처음의 y hat은 training 샘플 x에 앞서 정의한 것이지만,

18
00 : 00 : 56.205 -> 00 : 01 : 00.930
물론 각각의 training 샘플에 대해 정의한다면,

19
00 : 01 : 00.930 -> 00 : 01 : 03.240
각 샘플을 구별하기 위해,

20
00 : 01 : 03.240 -> 00 : 01 : 07.710
인덱스 괄호를 위 첨자 아래 첨자로 사용합니다.

21
00 : 01 : 07.710 -> 00 : 01 : 12.870
i 번째의 training 샘플에 대한 예측 값, 즉 y hat (i)는

22
00 : 01 : 12.870 -> 00 : 01 : 18.835
sigmoid 함수를 W의 전치 행렬과 training 샘플의 입력 값 x(i)

23
00 : 01 : 18.835 -> 00 : 01 : 25.905
의 곱에 b를 더한 값에 적용하면 얻을 수 있습니다. 
여기서, z (i)는 다음과 같이 정의 할 수 있습니다.

24
00 : 01 : 25.905 -> 00 : 01 : 30.110
z (i) = wT 곱하기 x (i) + b.

25
00 : 01 : 30.110 -> 00 : 01 : 31.350
이 강좌 전반에 걸쳐

26
00 : 01 : 31.350 -> 00 : 01 : 33.966
우리는 지금까지 언급 한 표기법을 사용할 것입니다. 

27
00 : 01 : 33.966 -> 00 : 01 : 41.605
즉, 괄호 i의 상단 첨자는

28
00 : 01 : 41.605 -> 00 : 01 : 47.615
training 데이터의 i 번째, i 번째 샘플과 연관된

29
00 : 01 : 47.615 -> 00 : 01 : 50.885
x, y, z 또는 기타 변수를 나타냅니다.

30
00 : 01 : 50.885 -> 00 : 01 : 54.840
위 첨자 아래 첨자 괄호 (i)의 의미에 대한 설명은 이상입니다.

31
00 : 01 : 54.840 -> 00 : 01 : 57.630
그럼 앞으로 어떤 loss 함수 또는 error 함수를 사용하면

32
00 : 01 : 57.630 -> 00 : 02 : 01.315
우리의 알고리즘을 잘 평가할 수 있는지 함께 살펴 보자.

33
00 : 02 : 01.315 -> 00 : 02 : 06.015
당신이 할 수있는 한 가지는 알고리즘이 
y-hat을 출력 할 때의 loss를 정의하는 것이며, 

34
00 : 02 : 06.015 -> 00 : 02 : 12.320
Loss를 Y와 같은 실제 레이블과 y hat의 차이의 제곱 또는 
1/2 제곱이 되도록 정의하는 것입니다. 


35
00 : 02 : 12.320 -> 00 : 02 : 14.975
이와 같이 정의할 수 있다는 것을 알았지만,
36
00 : 02 : 14.975 -> 00 : 02 : 17.670
logistic regression에서 일반적으로 이 정의는 사용되지 않습니다.

37
00 : 02 : 17.670 -> 00 : 02 : 21.000
왜냐하면, 만일 이 loss로 파라미터를 학습 시키려고하면

38
00 : 02 : 21.000 -> 00 : 02 : 25.682
나중에 non-convex와 같은 최적화 문제가 생기기 때문입니다.

39
00 : 02 : 25.682 -> 00 : 02 : 30.105
그러면 결과적으로는 복수의 극소점(미분)을 가지고 
최적화 문제를 취급하게되어,

40
00 : 02 : 30.105 -> 00 : 02 : 33.285
gradient descent는 하나의 통일된 극소점을 찾을 수 없을 것입니다.

41
00 : 02 : 33.285 -> 00 : 02 : 35.580
만약 방금 말한 몇 가지 말을 이해하지 못해도
42
00 : 02 : 35.580 -> 00 : 02 : 38.320
걱정하지 마십시오. 이후 동영상에서 그 이야기를 다룰 예정입니다.

43
00 : 02 : 38.320 -> 00 : 02 : 40.990
그냥 감만 잡으라는 의미에서 
아래와 같이 생각하시면 됩니다.

44
00 : 02 : 40.990 -> 00 : 02 : 44.620
loss 함수라는 함수 L은

45
00 : 02 : 44.620 -> 00 : 02 : 51.265
실제 레이블이 y의 경우 실제 출력 값 y hat이 
얼마나 정확한지를 측정하기 위해서 정의해야합니다.

46
00 : 02 : 51.265 -> 00 : 02 : 54.345
위에서처럼 제곱의 오차를 채택하는 것은 
합리적인 생각이 있습니다 하지만,

47
00 : 02 : 54.345 -> 00 : 02 : 58.160
gradient descent가 잘 움직이지 않는 경우는 그렇지 않습니다.

48
00 : 02 : 58.160 -> 00 : 03 : 0.500
따라서, logistic regression에서 실제로

49
00 : 03 : 0.500 -> 00 : 03 : 05.695
제곱 오차와 비슷한 역할을 가진 다른 loss 함수를 사용합니다.

50
00 : 03 : 05.695 -> 00 : 03 : 08.910
그 loss 함수를 사용하면, 우리는 convex한 
최적화 문제를 얻을 수 있고,

51
00 : 03 : 08.910 -> 00 : 03 : 13.530
이후 동영상에서 볼 수 있듯이 더 쉬운 방법으로 최적화 할 수 있습니다.

52
00 : 03 : 13.530 -> 00 : 03 : 17.310
로지스틱 회귀는 사실,

53
00 : 03 : 17.310 -> 00 : 03 : 21.795
다음과 같은 loss 함수를 사용합니다. 지금 여기에서 말씀드리자면

54
00 : 03 : 21.795 -> 00 : 03 : 31.740
-ylog (y hat) + (1-y) (1-log (y hat))

55
00 : 03 : 31.740 -> 00 : 03 : 34.600
입니다.

56
00 : 03 : 34.600 -> 00 : 03 : 38.785
왜 이 loss 함수들을 쓰는 것 인지, 
직관적으로만 설명 하겠습니다.

57
00 : 03 : 38.785 -> 00 : 03 : 41.285
유의해야 할 점은

58
00 : 03 : 41.285 -> 00 : 03 : 45.820
만약 제곱 오차를 사용한다면, 우리는 그 오차를 
가급적 작게 하려고 한다는 것입니다.

59
00 : 03 : 45.820 -> 00 : 03 : 48.680
즉,이 logistic regression loss 함수에 대해서도

60
00 : 03 : 48.680 -> 00 : 03 : 51.495
마찬가지로 우리는 그 값을 가급적 작게 하려고하는 것입니다.

61
00 : 03 : 51.495 -> 00 : 03 : 53.508
왜 이 식이 맞는지를 이해하기 위해,

62
00 : 03 : 53.508 -> 00 : 03 : 55.260
다음 두 가지 경우를 생각해보십시오.

63
00 : 03 : 55.260 -> 00 : 03 : 56.570
첫 번째 경우는,

64
00 : 03 : 56.570 -> 00 : 03 : 59.430
y가 1 인 경우에 그 때 loss 함수

65
00 : 03 : 59.430 -> 00 : 04 : 05.415
(y hat, y)는 정확하게 마이너스 부호로 쓸 수 있기 때문에,

66
00 : 04 : 05.415 -> 00 : 04 : 08.735
-log (y hat)입니다.

67
00 : 04 : 08.735 -> 00 : 04 : 10.770
만약 y가 1과 동일하면, y가 1이므로

68
00 : 04 : 10.770 -> 00 : 04 : 14.070
두 번째 항 (1-y)는 0이되기 때문입니다.

69
00 : 04 : 14.070 -> 00 : 04 : 19.880
즉, 만약 y가 1이면 가급적 log (y hat)를 크게하려고합니다.

70
00 : 04 : 19.880 -> 00 : 04 : 26.040
log (y hat)를 크게하고 싶은 것은

71
00 : 04 : 26.040 -> 00 : 04 : 32.935
y hat을 가급적 크게 하고 싶다는 것을 의미 하지요.

72
00 : 04 : 32.935 -> 00 : 04 : 35.170
그러나 y hat은 아시다시피,

73
00 : 04 : 35.170 -> 00 : 04 : 38.440
sigmoid 함수이며, 1보다 큰 것은 있을 수 없습니다.

74
00 : 04 : 38.440 -> 00 : 04 : 41.850
그러면 만약 y가 1과 동일하다면,

75
00 : 04 : 41.850 -> 00 : 04 : 44.050
y hat을 가급적 크게하고 싶은 한편,

76
00 : 04 : 44.050 -> 00 : 04 : 48.220
그 값이 1을 넘는 것은 결코 없고, 
이를테면 가급적 1에 접근 하려고하는 것입니다.

77
00 : 04 : 48.220 -> 00 : 04 : 50.740
다른 케이스는 y가 0 인 경우입니다.

78
00 : 04 : 50.740 -> 00 : 04 : 55.375
만약 y가 0과 동일하면, y는 0이기 때문에 loss 함수의 첫 번째 항은 0입니다.

79
00 : 04 : 55.375 -> 00 : 05 : 01.290
그러면 loss 함수의 값은 두 번째 항에서 정해집니다.

80
00 : 05 : 01.290 -> 00 : 05 : 07.210
즉 loss 함수는 -log (1-y hat)로,

81
00 : 05 : 07.210 -> 00 : 05 : 11.480
학습시키는 단계에서 loss 함수를 작게하려고한다면,

82
00 : 05 : 11.480 -> 00 : 05 : 19.450
log (1-y hat)을 크게하는 것을 의미합니다.

83
00 : 05 : 19.450 -> 00 : 05 : 22.050
왜냐하면 거기에 마이너스 부호가 있기 때문입니다.

84
00 : 05 : 22.050 -> 00 : 05 : 24.660
그리고 같은 이유로 결론적으로,

85
00 : 05 : 24.660 -> 00 : 05 : 30.870
이 loss 함수 y hat을 가급적 작게 하려고 하는 것입니다.

86
00 : 05 : 30.870 -> 00 : 05 : 34.320
그리고 다시, y hat은 0과 1 사이의
값이어야 하기 때문에

87
00 : 05 : 34.320 -> 00 : 05 : 38.155
만약 y가 0과 동일하다면,

88
00 : 05 : 38.155 -> 00 : 05 : 43.790
y hat이 가능한 0에 가까워 지도록 loss 함수의
매개 변수가 변화 할 수 있습니다.

89
00 : 05 : 43.790 -> 00 : 05 : 48.305
여기서, y가 1과 동일한 때에는

90
00 : 05 : 48.305 -> 00 : 05 : 52.950
y hat은 확대하려고하고 y 값이 0이면 y hat를 
작게하려고하는 같은 함수는 많이 존재합니다.

91
00 : 05 : 52.950 -> 00 : 05 : 55.150
우리는 방금 녹색으로 필기한 부분에서 

92
00 : 05 : 55.150 -> 00 : 05 : 59.920
loss 함수에 대한 대략적인 개념만 잡아 봤습니다. 

93
00 : 05 : 59.920 -> 00 : 06 : 03.970
나중에 logistic regression에서 왜 이런 형태의 

94
00 : 06 : 03.970 -> 00 : 06 : 08.500
loss 함수를 사용하는지에 대해서 보다 자세한 설명을 위해 
추가 비디오를 제공 할 것입니다. 

95
00 : 06 : 08.500 -> 00 : 06 : 13.630
마지막으로, 손실 함수는 하나의 training 예제와 
관련되어 정의 되어 있습니다.

96
00 : 06 : 13.630 -> 00 : 06 : 16.760
샘플 하나에 대해 얼마나 잘 학습 하고 있는지를 측정합니다.

97
00 : 06 : 16.760 -> 00 : 06 : 21.148
저는 cost 함수라는 것을 여기에서 정의 하려고 합니다.

98
00 : 06 : 21.148 -> 00 : 06 : 24.690
cost 함수는 전체 training 데이터에 대해 
얼마나 잘 학습하고 있는지를 측정합니다.

99
00 : 06 : 24.690 -> 00 : 06 : 28.660
그래서 cost 함수 J는

100
00 : 06 : 28.660 -> 00 : 06 : 33.130
매개 변수 W와 b에 대해서 적용되고,

101
00 : 06 : 33.130 -> 00 : 06 : 43.270
각 training 샘플에 대한 loss 함수의 총합을 1 / m 배 한,
즉 평균치가 될 것입니다.

102
00 : 06 : 43.270 -> 00 : 06 : 45.435
한편, 여기서 y hat은 물론

103
00 : 06 : 45.435 -> 00 : 06 : 49.570
logistic regression 알고리즘에 의한 예측 결과이며,

104
00 : 06 : 49.570 -> 00 : 06 : 52.430
아시다시피, 특정 매개 변수 W와 b의 세트를 사용한 겁니다.

105
00 : 06 : 52.430 -> 00 : 06 : 54.480
그리고 이 식을 그대로 확장하면

106
00 : 06 : 54.480 -> 00 : 06 : 58.010
마이너스 1 / m를 걸어

107
00 : 06 : 58.010 -> 00 : 07 : 03.550
시그마 i = 1 m 시그마 안은 J의 정의식입니다.

108
00 : 07 : 03.550 -> 00 : 07 : 07.530
그래서 y (i) log (y hat) (i)

109
00 : 07 : 07.530 -> 00 : 07 : 14.530
플러스 (1-y (i)) log (1-y hat (i))

110
00 : 07 : 14.530 -> 00 : 07 : 17.880
아마도 내가 대괄호를 여기에서 쓸 것이다.

111
00 : 07 : 17.880 -> 00 : 07 : 20.945
그리고 마이너스 부호를 통째로 외부에 배치합니다.

112
00 : 07 : 20.945 -> 00 : 07 : 23.665
제가 전문적인 표현을 하나만 쓰자면

113
00 : 07 : 23.665 -> 00 : 07 : 29.120
loss 함수는 단지 하나의 training 샘플에 적용한다는 것.

114
00 : 07 : 29.120 -> 00 : 07 : 33.010
그리고 cost 함수는 모든 매개 변수에 대한 비용이라는 것.

115
00 : 07 : 33.010 -> 00 : 07 : 36.115
그래서 logistic regression의 학습에서는

116
00 : 07 : 36.115 -> 00 : 07 : 38.980
우리는 하단에 기재 한 바와 같이, 
machine J에 걸친 전체적인 비용을 최소화하는

117
00 : 07 : 38.980 -> 00 : 07 : 43.475
매개 변수 W와 b를 찾을 수 있습니다.

118
00 : 07 : 43.475 -> 00 : 07 : 48.040
여기까지 로지스틱 회귀 알고리즘,

119
00 : 07 : 48.040 -> 00 : 07 : 50.770
training 데이터에 대한 손실 함수,

120
00 : 07 : 50.770 -> 00 : 07 : 54.190
그리고 알고리즘 내의 모든 파라미터에 걸친
cost 함수의 설계 방법을 보고 왔습니다.

121
00 : 07 : 54.190 -> 00 : 07 : 59.485
사실 logistic regression은 무척 작은 신경망으로 파악할 수 있습니다.

122
00 : 07 : 59.485 -> 00 : 08 : 01.905
다음 동영상에서 우리는

123
00 : 08 : 01.905 -> 00 : 08 : 04.965
신경망에서 무엇을하고 있는지 직관적으로 이해하기 위한 
고려 사항에 대해 학습할 겁니다.

124
00 : 08 : 04.965 -> 00 : 08 : 08.230
그래서 부디 정신차리시고 
다음 동영상으로 이동해주세요

125
00 : 08 : 08.230 -> 00 : 08 : 11.630
logistic regression을 어떻게 아주 작은 
신경망 으로 보는지에 대해 얘기해 보겠습니다.