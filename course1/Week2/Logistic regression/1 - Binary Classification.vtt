WEBVTT

1
00 : 00 : 00.920 -> 00 : 00 : 02.860
안녕하세요, 그리고 어서 오세요.

2
00 : 00 : 02.860 -> 00 : 00 : 08.860
이번주는 신경망 프로그래밍
기초를 배웁니다.

3
00 : 00 : 08.860 -> 00 : 00 : 11.990
신경망을 구현하는 경우에는

4
00 : 00 : 11.990 -> 00 : 00 : 16.260
일부 구현 테크닉이 중요 해지고 있습니다.

5
00 : 00 : 16.260 -> 00 : 00 : 21.150
예를 들어, m 개의 training 샘플이 있으면,

6
00 : 00 : 21.150 -> 00 : 00 : 25.110
그 m 개의 샘플을 for 루프에서

7
00 : 00 : 25.110 -> 00 : 00 : 28.240
처리 할지도 모릅니다.

8
00 : 00 : 28.240 -> 00 : 00 : 31.260
그러나 신경망을 구현할 때는

9
00 : 00 : 31.260 -> 00 : 00 : 34.540
일반적으로 for 루프를 사용하지 않고
전체 training세트를

10
00 : 00 : 34.540 -> 00 : 00 : 39.040
사용해야 합니다.

11
00 : 00 : 39.040 -> 00 : 00 : 42.940
그 방법을 이번 주에 학습 합시다.

12
00 : 00 : 42.940 -> 00 : 00 : 47.700
이번주에는 또 다른 것들도 배우게 됩니다.
네트워크에서의 연산을 구성할 때,

13
00 : 00 : 47.700 -> 00 : 00 : 51.670
여러분은 forward pause, forward propagation step,

14
00 : 00 : 51.670 -> 00 : 00 : 56.100
backward pause, backward propagation step을
만들 수 있습니다.

15
00 : 00 : 56.100 -> 00 : 01 : 00.010
이번 주에는 왜 신경망의 학습이

16
00 : 01 : 00.010 -> 00 : 01 : 04.830
forward propagation과 
분리된 backward propagation에 의해

17
00 : 01 : 04.830 -> 00 : 01 : 08.010
구성되는지에 대해서도 배웁니다.

18
00 : 01 : 09.100 -> 00 : 01 : 12.620
이번 주에는 이해를 돕기 위해
이러한 것을

19
00 : 01 : 12.620 -> 00 : 01 : 16.170
logistic regression을 사용하여 알려 드립니다.

20
00 : 01 : 16.170 -> 00 : 01 : 19.970
만약 logistic regression을 이미 본 적이 있어도,

21
00 : 01 : 19.970 -> 00 : 01 : 23.845
이번 주에
새로운 재미있는 아이디어를 얻을 수 있을 것입니다.

22
00 : 01 : 23.845 -> 00 : 01 : 25.815
이제 시작합시다.

23
00 : 01 : 25.815 -> 00 : 01 : 30.605
logistic regression은
binary classification을 위한 알고리즘입니다.

24
00 : 01 : 30.605 -> 00 : 01 : 33.145
먼저 문제를 설정합시다.

25
00 : 01 : 33.145 -> 00 : 01 : 36.925
이것이 binary classification의 예입니다.

26
00 : 01 : 36.925 -> 00 : 01 : 41.545
이러한 이미지의 입력이 있고,

27
00 : 01 : 41.545 -> 00 : 01 : 47.260
이 이미지를 고양이로 인식하면 1이라는 라벨을 출력하고,

28
00 : 01 : 47.260 -> 00 : 01 : 52.140
그렇지 않으면 0을 출력하고자합니다.

29
00 : 01 : 52.140 -> 00 : 01 : 57.740
그리고 출력되는 라벨을 y로 나타냅니다.

30
00 : 01 : 57.740 -> 00 : 02 : 01.550
컴퓨터에서 이미지가
어떻게 표현되는지 살펴 봅시다.

31
00 : 02 : 01.550 -> 00 : 02 : 05.680
이미지를 저장하기 위해서,
컴퓨터는 이 이미지의 빨강, 녹색, 파랑 색상 채널의

32
00 : 02 : 05.680 -> 00 : 02 : 09.890
세가지 다른 행렬을 저장합니다.

33
00 : 02 : 10.990 -> 00 : 02 : 15.900
입력 이미지가 64 픽셀 × 64 픽셀이라면,

34
00 : 02 : 15.900 -> 00 : 02 : 21.700
이미지의 빨강, 녹색 및 파랑 픽셀화소

35
00 : 02 : 21.700 -> 00 : 02 : 27.230
값에 해당하는 3 x 64 x 64 행렬이됩니다.

36
00 : 02 : 27.230 -> 00 : 02 : 31.290
하지만 제가 이 작은 슬라이드를 만들기 위해
행렬을 최대한 축소해서 표현했기 때문에

37
00 : 02 : 31.290 -> 00 : 02 : 35.320
64 × 64가 아닌
5 × 4가되어 버렸습니다.

38
00 : 02 : 35.320 -> 00 : 02 : 41.640
픽셀의 화소 값을
feature vector화 하기 위해,

39
00 : 02 : 41.640 -> 00 : 02 : 48.000
이 값 들을 모두 일렬로 정리하고
feature vector x에 넣어 보겠습니다.

40
00 : 02 : 48.000 -> 00 : 02 : 53.782
세 개의 픽셀 데이터를 feature vector로 만들기 위해,

41
00 : 02 : 53.782 -> 00 : 02 : 59.580
이미지에 관한 feature vector를 
다음과 같이 정의합니다.

42
00 : 02 : 59.580 -> 00 : 03 : 03.960
모든 픽셀 데이터, 255,231 등을 나열합니다.

43
00 : 03 : 03.960 -> 00 : 03 : 10.827
모든 붉은 픽셀을 나열할 때까지 계속합니다.

44
00 : 03 : 10.827 -> 00 : 03 : 15.737
그리고 255,134,255,134 계속됩니다.

45
00 : 03 : 15.737 -> 00 : 03 : 20.952
결국, 빨강, 녹색, 파랑의 픽셀 데이터들이

46
00 : 03 : 20.952 -> 00 : 03 : 25.570
모두가 일렬로 세워진 긴 feature vector가 완성됩니다.

47
00 : 03 : 25.570 -> 00 : 03 : 31.043
이미지가 64 × 64면

48
00 : 03 : 31.043 -> 00 : 03 : 36.401
벡터 x의 차수는 64 × 64 × 3입니다.

49
00 : 03 : 36.401 -> 00 : 03 : 41.320
왜냐하면 이것은 세 행렬 개수의 합이기 때문입니다.

50
00 : 03 : 41.320 -> 00 : 03 : 44.097
이 경우 12,288입니다.

51
00 : 03 : 44.097 -> 00 : 03 : 47.330
이러한 숫자를 곱한 결과가 그렇게되는 것입니다.

52
00 : 03 : 47.330 -> 00 : 03 : 51.870
그래서, Nx = 12288이라는 것을

53
00 : 03 : 51.870 -> 00 : 03 : 55.080
입력되는 feature vector X의 차수를 
표현하는 방법으로 사용 하겠습니다.

54
00 : 03 : 55.080 -> 00 : 03 : 59.280
간결하게하기 위해 소문자 n를 사용하여

55
00 : 03 : 59.280 -> 00 : 04 : 02.720
입력되는 feature vector의 
차원 수를 나타낼 수도 있습니다.

56
00 : 04 : 02.720 -> 00 : 04 : 07.510
Binary classification을 사용하는 우리의 목적은

57
00 : 04 : 07.510 -> 00 : 04 : 10.760
feature vector X로 표시되는 이미지를 입력 할 수있는
분류기를 학습 하는 것입니다.

58
00 : 04 : 10.760 -> 00 : 04 : 15.460
그리고 거기에 대응하는 라벨 y가
1 또는 0인지를 예측합니다.

59
00 : 04 : 15.460 -> 00 : 04 : 19.000
즉, 이미지가 고양이인지 아닌지를
예측한다는 것입니다.

60
00 : 04 : 19.000 -> 00 : 04 : 21.560
이제이 과정의 나머지 부분에서 사용할

61
00 : 04 : 21.560 -> 00 : 04 : 23.820
표기법 중 일부를 설명해 보겠습니다.

62
00 : 04 : 23.820 -> 00 : 04 : 29.453
한개의  training샘플은 (x, y) 순서쌍으로 표현됩니다

63
00 : 04 : 29.453 -> 00 : 04 : 34.446
x는 feature vector의 차원 수 를 의미하고

64
00 : 04 : 34.446 -> 00 : 04 : 39.320
y는 0 또는 1 로 표현되는 레이블입니다.

65
00 : 04 : 39.320 -> 00 : 04 : 44.550
trainging 세트는 소문자 m 개의 trainging 샘플
로 구성됩니다.

66
00 : 04 : 44.550 -> 00 : 04 : 50.320
그래서 trainging 세트의 첫 번째 샘플은
(x(1), y(1))로 표현

67
00 : 04 : 50.320 -> 00 : 04 : 55.370
(x(2), y(2))가 두 번째로,

68
00 : 04 : 55.370 -> 00 : 05 : 01.980
마지막 샘플인 (x(m), y(m))까지 계속됩니다.

69
00 : 05 : 01.980 -> 00 : 05 : 05.650
이들이 모두 trainging 세트가 되는 것입니다.

70
00 : 05 : 05.650 -> 00 : 05 : 10.170
저는 이제 소문자 m을 trainging 샘플의 
수를 나타내는 데 사용합니다.

71
00 : 05 : 10.170 -> 00 : 05 : 14.418
그리고 이것이 trainging 샘플 수와 
같다는 것을 강조하기 위해

72
00 : 05 : 14.418 -> 00 : 05 : 16.437
가끔 m = m train 처럼 표현할 수 있습니다.

73
00 : 05 : 16.437 -> 00 : 05 : 18.692
테스트 세트에 대해 쓸 때,

74
00 : 05 : 18.692 -> 00 : 05 : 24.430
테스트 샘플의 수를 나타 내기 위해
m test라는 표기를 사용할 수 있습니다.

75
00 : 05 : 24.430 -> 00 : 05 : 27.430
이것이 테스트 샘플의 갯수 입니다.

76
00 : 05 : 27.430 -> 00 : 05 : 33.440
마지막으로,
모든 trainging 샘플을 컴팩트하게 나타 내기 위해

77
00 : 05 : 33.440 -> 00 : 05 : 36.840
대문자 X라는 행렬을 정의합니다.

78
00 : 05 : 36.840 -> 00 : 05 : 41.592
이것은 trainging 세트의 입력값
 x1, x2 ...을 가지고 와서

79
00 : 05 : 41.592 -> 00 : 05 : 44.568
그들을 열(행렬에서의 열)로 나란히 할 것입니다.

80
00 : 05 : 44.568 -> 00 : 05 : 49.958
x1을 따온이 행렬의 첫 번째 열에 넣고

81
00 : 05 : 49.958 -> 00 : 05 : 54.798
x2를 두 번째 열에 넣고 xm까지 계속합니다.

82
00 : 05 : 54.798 -> 00 : 05 : 58.000
이것이 대문자 X 행렬입니다.

83
00 : 05 : 58.000 -> 00 : 06 : 03.005
그래서,이 행렬 X는 trainging 샘플 
수와 같은 m 열로 구성되 있고,

84
00 : 06 : 03.005 -> 00 : 06 : 08.665
행의 수, 즉 높이는 nx입니다.

85
00 : 06 : 08.665 -> 00 : 06 : 14.400
다른 코스에서는 하나의 trainging 샘플을
(이것 처럼 열이 아니라) 행으로 쌓아올려서

86
00 : 06 : 14.400 -> 00 : 06 : 19.390
정의 된 행렬 X를 본 적이 있을지도 모릅니다.

87
00 : 06 : 19.390 -> 00 : 06 : 23.940
x1 transpose에서 xm transpose까지가 
일렬로 세워진 것입니다.
(transpose: 행렬의 열과 행을 바꿈)

88
00 : 06 : 23.940 -> 00 : 06 : 27.704
그러나 왼쪽에 쓴 것을 사용하는 것이,

89
00 : 06 : 27.704 -> 00 : 06 : 32.218
신경망을 구현할 때 더 편해질 것입니다.

90
00 : 06 : 32.218 -> 00 : 06 : 37.171
복습하면 X는 nx × m 차원의 행렬입니다.

91
00 : 06 : 37.171 -> 00 : 06 : 40.404
그리고 이걸 Python으로 구현 할 때는

92
00 : 06 : 40.404 -> 00 : 06 : 45.362
X.shape라는 것이
행렬의 형태를 확인하는 명령인데,

93
00 : 06 : 45.362 -> 00 : 06 : 50.325
이 명령어를 입력하면 nx, m으로 표시됩니다.

94
00 : 06 : 50.325 -> 00 : 06 : 53.255
nx × m 차원의 행렬이라는 의미입니다.

95
00 : 06 : 53.255 -> 00 : 06 : 58.785
이것이 trainging 샘플 x를 행렬로 묶는 방법입니다.

96
00 : 06 : 58.785 -> 00 : 07 : 01.315
그럼 출력 라벨 y는 어떨까요?

97
00 : 07 : 01.315 -> 00 : 07 : 04.815
신경망 구현을
더 쉽게하기 위해서는,

98
00 : 07 : 04.815 -> 00 : 07 : 10.030
y도 열로 정렬하는 것이 좋습니다.

99
00 : 07 : 10.030 -> 00 : 07 : 14.650
그래서 대문자 Y를 y1, y2에서 ym까지

100
00 : 07 : 14.650 -> 00 : 07 : 18.580
늘어 놓은 것으로 정의합니다.

101
00 : 07 : 18.580 -> 00 : 07 : 24.980
그래서, Y는 1 × m 차원의 행렬입니다.

102
00 : 07 : 24.980 -> 00 : 07 : 30.530
또, Python 명령을 사용하면
Y 의 shape는 1, m임을 알 수 있습니다.

103
00 : 07 : 30.530 -> 00 : 07 : 34.810
즉, 1 × m 차원의 행렬이라는 것입니다.

104
00 : 07 : 34.810 -> 00 : 07 : 39.660
추후에
신경망을 구현할 때

105
00 : 07 : 39.660 -> 00 : 07 : 43.630
다른 trainging 샘플 데이터에 관련된 데이터를
가져오는 것은 편리한 습관이 될 것입니다.

106
00 : 07 : 43.630 -> 00 : 07 : 48.580
여기서 말하는 데이터라는 것은 x와 y,
그리고 이후에 등장하는 다른 수량입니다.

107
00 : 07 : 48.580 -> 00 : 07 : 49.900
다른 trainging 샘플에 관련된 데이터를 가지고

108
00 : 07 : 49.900 -> 00 : 07 : 52.990
열에 쌓기 위해서는

109
00 : 07 : 52.990 -> 00 : 07 : 57.430
우리가 방금 x와 y에 대해 했던것 처럼 하시면 됩니다.

110
00 : 07 : 58.450 -> 00 : 08 : 01.380
그리고 이것 들이 Logistic regression과

111
00 : 08 : 01.380 -> 00 : 08 : 04.060
추후에 나오는 신경망에서 사용되는
표기법입니다.

112
00 : 08 : 04.060 -> 00 : 08 : 07.430
M 또는 N 또는 다른 것들의

113
00 : 08 : 07.430 -> 00 : 08 : 08.300
표기법의 의미를 잊어 버린 경우

114
00 : 08 : 08.300 -> 00 : 08 : 12.630
코스 웹 사이트에 표기법 가이드를 참고하여

115
00 : 08 : 12.630 -> 00 : 08 : 17.430
특정 표기법을 빠르게 찾아 볼 수 있습니다.

116
00 : 08 : 17.430 -> 00 : 08 : 20.890
자, 다음 동영상으로 이동하여

117
00 : 08 : 20.890 -> 00 : 08 : 23.190
이러한 표기법을 사용하여
logistic regression을 시작해 보겠습니다.