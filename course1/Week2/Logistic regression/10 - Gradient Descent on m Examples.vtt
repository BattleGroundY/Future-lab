WEBVTT

1
00 : 00 : 00.060 -> 00 : 00 : 03.750
이전 강좌에서 여러분은

2
00 : 00 : 01.890 -> 00 : 00 : 05.819
어떻게 미분을 계산하고 gradient descent를 실행하는지

3
00 : 00 : 03.750 -> 00 : 00 : 07.500
한개의 로지스틱 회귀 training 예시에서

4
00 : 00 : 05.819 -> 00 : 00 : 09.929
확인해 보았습니다.

5
00 : 00 : 07.500 -> 00 : 00 : 12.450
이제 우리는 m training 예시를 해볼 겁니다.

6
00 : 00 : 09.929 -> 00 : 00 : 14.429
시작하기 전에 먼저 

7
00 : 00 : 12.450 -> 00 : 00 : 17.460
코스트함수 J에 대한 복습을 해보겠습니다.

8
00 : 00 : 14.429 -> 00 : 00 : 19.380
우리가 신경 쓰는 비용 함수 J (w, b)

9
00 : 00 : 17.460 -> 00 : 00 : 22.699
는 평균값 입니다.

10
00 : 00 : 19.380 -> 00 : 00 : 25.350
1 / m 을 i = 1 부터 m 까지의 합에 곱합니다.

11
00 : 00 : 22.699 -> 00 : 00 : 29.519
이 loss 함수 L은

12
00 : 00 : 25.350 -> 00 : 00 : 33.510
L(a(i), y) 입니다.

13
00 : 00 : 29.519 -> 00 : 00 : 36.120
당신은 a^(i)가 training 샘플의 
예측 된 값(Y hat)이라는 것을 알 겁니다.

14
00 : 00 : 33.510 -> 00 : 00 : 40.620
그것은 σ (z ^(i)) 와 같고,

15
00 : 00 : 36.120 -> 00 : 00 : 46.800
σ (Wt x^(i) + b)

16
00 : 00 : 40.620 -> 00 : 00 : 48.510
와도 같습니다

17
00 : 00 : 46.800 -> 00 : 00 : 51.600
이전 슬라이드에서 설명드린 내용은

18
00 : 00 : 48.510 -> 00 : 00 : 55.620
오직 한개의 training 샘플에서

19
00 : 00 : 51.600 -> 00 : 01 : 00.180
미분 값을 계산하는 방법 이었습니다.

20
00 : 00 : 55.620 -> 00 : 01 : 03.809
우리는 그 계산에서 dw1, dw2 값 및

21
00 : 01 : 00.180 -> 00 : 01 : 06.689
Db(i) 를 

22
00 : 01 : 03.809 -> 00 : 01 : 08.369
얻어 냈을 겁니다.

23
00 : 01 : 06.689 -> 00 : 01 : 10.799
당신이 이전 슬라이드에서 우리가 했던 걸

24
00 : 01 : 08.369 -> 00 : 01 : 15.030
하려고 하는 거라면

25
00 : 01 : 10.799 -> 00 : 01 : 17.850
여기에는 하나의 훈련 표본 (x^i, y^i)만 사용됩니다

26
00 : 01 : 15.030 -> 00 : 01 : 20.759
아 여기도 (i)가 있어야 되네요, 놓쳐서 죄송합니다.

27
00 : 01 : 17.850 -> 00 : 01 : 22.530
당신은 이제 전체 비용함수의 합계가 

28
00 : 01 : 20.759 -> 00 : 01 : 26.220
평균 값인걸 아셨을 겁니다.

29
00 : 01 : 22.530 -> 00 : 01 : 29.369
왜냐면 1 / m 이 각 손실값마다 곱해져 있기 때문입니다.

30
00 : 01 : 26.220 -> 00 : 01 : 32.810
그래서 결과적으로

31
00 : 01 : 29.369 -> 00 : 01 : 38.600
전체 비용함수에서 w1을 미분한 값이나

32
00 : 01 : 32.810 -> 00 : 01 : 42.600
각각의 loss에 관한 식에서 w1 미분한 값이나

33
00 : 01 : 38.600 -> 00 : 01 : 46.170
같아집니다.

34
00 : 01 : 42.600 -> 00 : 01 : 48.240
우리는 이전 슬라이드에서

35
00 : 01 : 46.170 -> 00 : 01 : 54.119
이 부분에 대한 계산 방법을 보여 줬습니다.

36
00 : 01 : 48.240 -> 00 : 01 : 55.890
이렇게 제가 쓰겠습니다.

37
00 : 01 : 54.119 -> 00 : 01 : 57.659
이것이 이전 슬라이드 에서의 모습입니다.

38
00 : 01 : 55.890 -> 00 : 02 : 00.450
한 개의 training 샘플일 때 계산식 입니다.

39
00 : 01 : 57.659 -> 00 : 02 : 03.119
그래서 당신이 정말로해야 할 일은 

40
00 : 02 : 00.450 -> 00 : 02 : 04.680
이 미분값 들을 계산하는 것 입니다.

41
00 : 02 : 03.119 -> 00 : 02 : 07.350
이전 교육 예제에서 보여줬듯이 말입니다.

42
00 : 02 : 04.680 -> 00 : 02 : 10.379
그리고 이것들의 평균을 계산한다면.

43
00 : 02 : 07.350 -> 00 : 02 : 10.830
이것은 곧 전체 비용함수의 

44
00 : 02 : 10.379 -> 00 : 02 : 12.870
미분값이 될 것입니다.


45
00 : 02 : 10.830 -> 00 : 02 : 15.390
이 값은 당신이 gradient descent를 실행하는데
사용될 수 있을 겁니다.

46
00 : 02 : 12.870 -> 00 : 02 : 17.730
저도 세부사항이 많은건 알고 있지만

47
00 : 02 : 15.390 -> 00 : 02 : 19.920
이는 다음에 얘기하도록 하겠습니다

48
00 : 02 : 17.730 -> 00 : 02 : 21.690
그럼 다음으로 당신이 해야할 일은

49
00 : 02 : 19.920 -> 00 : 02 : 24.960
logistic regression과 gradient descent가 

50
00 : 02 : 21.690 -> 00 : 02 : 28.350
함께 실행 될 수 있도록 하는 것입니다.

51
00 : 02 : 24.960 -> 00 : 02 : 37.770
이제 할 일은 J를 0으로 초기화 하는 겁니다.

52
00 : 02 : 28.350 -> 00 : 02 : 40.140
Dw1도 0으로 dw2도 0으로 db도 0으로 초기화 합니다.

53
00 : 02 : 37.770 -> 00 : 02 : 43.190
우리가 할 일은 전체 training 샘플들에 대하여

54
00 : 02 : 40.140 -> 00 : 02 : 45.690
for문을 사용하는 겁니다.

55
00 : 02 : 43.190 -> 00 : 02 : 47.670
그리고 각각의 training example에 대한

56
00 : 02 : 45.690 -> 00 : 02 : 49.020
미분을 진행하고 다 더할 겁니다.

57
00 : 02 : 47.670 -> 00 : 02 : 51.480
좋아요.

58
00 : 02 : 49.020 -> 00 : 02 : 54.360
i = m 이고 여기서 m은 트레이닝 샘플의 수입니다.

59
00 : 02 : 51.480 -> 00 : 02 : 57.090
z^i는

60
00 : 02 : 54.360 -> 00 : 03 : 00.360
w traspose 곱하기 x^i + b 를합니다.

61
00 : 02 : 57.090 -> 00 : 03 : 04.020
a^i의 예측 된 값은 σ (z^i)와 동일하다.

62
00 : 03 : 00.360 -> 00 : 03 : 09.120
그런 다음 J를 계속 더해야 되겠군요.

63
00 : 03 : 04.020 -> 00 : 03 : 12.360
J +=(대입연산자)로 나타냅시다 

64
00 : 03 : 09.120 -> 00 : 03 : 14.010
(y ^ i) log(a ^ i) + (1-y ^ i) log (1-a ^ i) 

65
00 : 03 : 12.360 -> 00 : 03 : 15.959
음수 부호를 전체 수식 앞에 놓겠습니다.

66
00 : 03 : 14.010 -> 00 : 03 : 20.580
그런 다음 이전에 본 것처럼

67
00 : 03 : 15.959 -> 00 : 03 : 28.500
dz^i 는 a ^ i - y ^ i와 같습니다.

68
00 : 03 : 20.580 -> 00 : 03 : 33.180
Dw1 += (x1) ^ i 곱하기 dz ^ i 

69
00 : 03 : 28.500 -> 00 : 03 : 35.280
Dw2 += (x2) ^ i 곱하기 dz ^ i

70
00 : 03 : 33.180 -> 00 : 03 : 37.680
여기서는 w변수가 2가지 밖에 없다고

71
00 : 03 : 35.280 -> 00 : 03 : 41.070
가정 하겠습니다.

72
00 : 03 : 37.680 -> 00 : 03 : 45.480
그렇지 않으면 dw3 등이 필요합니다.

73
00 : 03 : 41.070 -> 00 : 03 : 47.430
db += dz ^ i  라고 하겠습니다.

74
00 : 03 : 45.480 -> 00 : 03 : 49.350
여기가 이 for loop의 끝이겠군요.

75
00 : 03 : 47.430 -> 00 : 03 : 51.900
마지막으로 모든 m 개의 training 샘플

76
00 : 03 : 49.350 -> 00 : 03 : 54.959
에 대한 계산을 수행 한 후에도

77
00 : 03 : 51.900 -> 00 : 03 : 56.880
평균을 계산 해야 되기 때문에

78
00 : 03 : 54.959 -> 00 : 04 : 01.920
전체 J를 m으로 나눕니다.
(여기서도 대입연산자 /= 사용)

79
00 : 03 : 56.880 -> 00 : 04 : 04.260
따라서 dw1 /= m, dw2 /= m

80
00 : 04 : 01.920 -> 00 : 04 : 07.019
Db /= m과 같습니다.

81
00 : 04 : 04.260 -> 00 : 04 : 09.060
당신이 이 계산을 모두 끝낸다면,

82
00 : 04 : 07.019 -> 00 : 04 : 11.160
당신은 W1, W2, B 3개의 변수에 대해서

83
00 : 04 : 09.060 -> 00 : 04 : 14.250
비용함수 J의 미분 값을

84
00 : 04 : 11.160 -> 00 : 04 : 17.010
계산한 것입니다.

85
00 : 04 : 14.250 -> 00 : 04 : 22.079
우리가 하고 있는 계산의 
세부 사항을 한번 보겠습니다.

86
00 : 04 : 17.010 -> 00 : 04 : 25.020
우리는 dw_1 dw_2와 db를

87
00 : 04 : 22.079 -> 00 : 04 : 28.169
accumulator로서만 활용할 겁니다.

88
00 : 04 : 25.020 -> 00 : 04 : 31.500
그래서 당신이 이 계산을 끝내면

89
00 : 04 : 28.169 -> 00 : 04 : 33.509
Dw1은 전체 비용 함수를 

90
00 : 04 : 31.500 -> 00 : 04 : 36.780
w1에 대해 미분한 값과 같습니다.

91
00 : 04 : 33.509 -> 00 : 04 : 39.720
dw_2 및 db도 마찬가지입니다.

92
00 : 04 : 36.780 -> 00 : 04 : 41.520
dw1과 dw2에는 위 첨자 i가 없습니다.

93
00 : 04 : 39.720 -> 00 : 04 : 43.379
우리가 이 코드 내에서는 이것 들을

94
00 : 04 : 41.520 -> 00 : 04 : 45.690
전체 training set를 더하기 위한 

95
00 : 04 : 43.379 -> 00 : 04 : 48.960
accumulator로서 사용하기 때문입니다.

96
00 : 04 : 45.690 -> 00 : 04 : 51.539
그러나 dz^i는

97
00 : 04 : 48.960 -> 00 : 04 : 53.490
단일 training 샘플에 해당합니다.

98
00 : 04 : 51.539 -> 00 : 04 : 55.740
이것이 위첨자 i가 있는 이유입니다.

99
00 : 04 : 53.490 -> 00 : 04 : 58.379
단일 training 샘플을 참고해 보시기 바랍니다.

100
00 : 04 : 55.740 -> 00 : 05 : 00.960
이 계산이 모두 끝난 후에는

101
00 : 04 : 58.379 -> 00 : 05 : 03.449
gradient descent를 향해 한 걸음 나아갑니다.

102
00 : 05 : 00.960 -> 00 : 05 : 06.360
w1 은 w1 - α 곱하기 dw1 으로 

103
00 : 05 : 03.449 -> 00 : 05 : 10.710
업데이트 될 것이고

104
00 : 05 : 06.360 -> 00 : 05 : 13.740
w2는 w2 - α 곱하기 dw2 로

105
00 : 05 : 10.710 -> 00 : 05 : 17.190
업데이트 될 겁니다.

106
00 : 05 : 13.740 -> 00 : 05 : 21.000
동시에 b는 b - α 곱하기 db 로 업데이트 됩니다.

107
00 : 05 : 17.190 -> 00 : 05 : 23.879
여기서 dw_1 dw_2와 db는 이전과 같이 계산됩니다

108
00 : 05 : 21.000 -> 00 : 05 : 27.000
여기있는 J값 또한 비용함수에

109
00 : 05 : 23.879 -> 00 : 05 : 28.590
알맞은 값으로 여기 있을겁니다.

110
00 : 05 : 27.000 -> 00 : 05 : 31.050
자 이 슬라이드의 모든 내용들은

111
00 : 05 : 28.590 -> 00 : 05 : 33.060
gradient descent를 한단계 진행하기 위해서 필요한 것들 입니다.

112
00 : 05 : 31.050 -> 00 : 05 : 35.699
필요한 것들 입니다.

113
00 : 05 : 33.060 -> 00 : 05 : 37.680
gradient descent를 여러번 반복하기 위해서는

114
00 : 05 : 35.699 -> 00 : 05 : 40.469
이 계산들을 여러번 반복하면 됩니다.

115
00 : 05 : 37.680 -> 00 : 05 : 41.819
이러한 세부 사항들이 조금

116
00 : 05 : 40.469 -> 00 : 05 : 43.830
복잡해 보일 수도 있습니다.

117
00 : 05 : 41.819 -> 00 : 05 : 45.960
그러나 지금은 너무 많이 걱정하지 마십시오.

118
00 : 05 : 43.830 -> 00 : 05 : 48.599
당신이 프로그래밍 숙제를 계속 하다보면

119
00 : 05 : 45.960 -> 00 : 05 : 50.520
이것들은 분명히

120
00 : 05 : 48.599 -> 00 : 05 : 54.120
익숙해질 것입니다.

121
00 : 05 : 50.520 -> 00 : 05 : 57.300
그러나 계산에 두 가지 단점이 있음을 보여줍니다.

122
00 : 05 : 54.120 -> 00 : 05 : 59.729
여기 있는 방법대로

123
00 : 05 : 57.300 -> 00 : 06 : 01.440
logistic regression을

124
00 : 05 : 59.729 -> 00 : 06 : 03.960
진행한다면

125
00 : 06 : 01.440 -> 00 : 06 : 05.490
두 개의 for 루프가 필요합니다. 첫 번째 for 루프는

126
00 : 06 : 03.960 -> 00 : 06 : 07.770
m 개의 트레이닝 샘플을 가져오는 루프입니다.

127
00 : 06 : 05.490 -> 00 : 06 : 10.919
두 번째 for 루프는

128
00 : 06 : 07.770 -> 00 : 06 : 13.139
for 루프의 모든 feature들을 반영합니다.

129
00 : 06 : 10.919 -> 00 : 06 : 15.930
이 예제에서는 두 가지 feature 만 있습니다.

130
00 : 06 : 13.139 -> 00 : 06 : 17.879
따라서 n은 2이고 n_x는 2와 같습니다.

131
00 : 06 : 15.930 -> 00 : 06 : 21.000
하지만 더 많은 feature가 있다면

132
00 : 06 : 17.879 -> 00 : 06 : 23.099
dw_1 dw_2를 작성한 것 처럼

133
00 : 06 : 21.000 -> 00 : 06 : 25.979
마찬가지로 dw_3을 계산하는 등의 작업을 수행합니다.

134
00 : 06 : 23.099 -> 00 : 06 : 29.009
dw_n 때까지 계산을 해야될 겁니다.

135
00 : 06 : 25.979 -> 00 : 06 : 31.279
따라서 모든 n개의 feature들을 반영하는

136
00 : 06 : 29.009 -> 00 : 06 : 33.199
for 구문이 필요할 겁니다.

137
00 : 06 : 31.279 -> 00 : 06 : 36.049
딥러닝 알고리즘을 구현할 때

138
00 : 06 : 33.199 -> 00 : 06 : 38.419
코드내에 for 루프를 명시하면

139
00 : 06 : 36.049 -> 00 : 06 : 41.839
명시하면

140
00 : 06 : 38.419 -> 00 : 06 : 44.149
알고리즘의 효율성이 떨어집니다.

141
00 : 06 : 41.839 -> 00 : 06 : 46.669
따라서 딥러닝 시대가

142
00 : 06 : 44.149 -> 00 : 06 : 48.649
점점 커지는 데이터 세트를 필요로 함에 따라

143
00 : 06 : 46.669 -> 00 : 06 : 50.779
for 루프를 명시적으로 사용하지 않고

144
00 : 06 : 48.649 -> 00 : 06 : 52.969
알고리즘을 구현하는 것이

145
00 : 06 : 50.779 -> 00 : 06 : 55.129
대용량 데이터 세트에 적합할 것이고

146
00 : 06 : 52.969 -> 00 : 06 : 56.719
매우 중요하며 도움이 될 것입니다.

147
00 : 06 : 55.129 -> 00 : 06 : 58.129
그래서 여기에 몇 가지 기술이 있습니다.

148
00 : 06 : 56.719 -> 00 : 07 : 01.159
vectorization이라는 기술입니다.

149
00 : 06 : 58.129 -> 00 : 07 : 03.559
이는 당신의 코드내에 명시적인 for 구문 들을

150
00 : 07 : 01.159 -> 00 : 07 : 06.169
제거 해줄 것입니다.

151
00 : 07 : 03.559 -> 00 : 07 : 08.199
딥러닝시대 이전에는.

152
00 : 07 : 06.169 -> 00 : 07 : 11.239
즉, 딥러닝이 일어나기 전에

153
00 : 07 : 08.199 -> 00 : 07 : 13.159
vectorization은 양날의 검 이었습니다.

154
00 : 07 : 11.239 -> 00 : 07 : 15.589
때로는 코드 속도를 높일 수도 있었고

155
00 : 07 : 13.159 -> 00 : 07 : 17.749
때로는 가속을 하지 못할 수도 있었습니다.

156
00 : 07 : 15.589 -> 00 : 07 : 20.029
그러나 딥러닝의 시대에는

157
00 : 07 : 17.749 -> 00 : 07 : 22.699
for 루프에서 빠져 나오게 해주는 vectorization은

158
00 : 07 : 20.029 -> 00 : 07 : 25.039
매우 중요 해졌습니다.

159
00 : 07 : 22.699 -> 00 : 07 : 26.989
왜냐하면 우리가 점점 더 많은 데이터를 이용해서

160
00 : 07 : 25.039 -> 00 : 07 : 29.239
훈련하고 있기 때문이죠.

161
00 : 07 : 26.989 -> 00 : 07 : 31.209
따라서 코드가 매우 효율적이어야합니다.

162
00 : 07 : 29.239 -> 00 : 07 : 34.219
다음 동영상에서는

163
00 : 07 : 31.209 -> 00 : 07 : 37.339
vectorization을 이용하여, 
단 한개의 for 구문도 사용하지 않고

164
00 : 07 : 34.219 -> 00 : 07 : 40.879
이것 들을 실행하는 방법에 대해서 이야기 하겠습니다.

165
00 : 07 : 37.339 -> 00 : 07 : 43.069
저는 여러분이 logistic regression과 gradient descent를

166
00 : 07 : 40.879 -> 00 : 07 : 44.299
어떻게 구동할지에 대해 

167
00 : 07 : 43.069 -> 00 : 07 : 46.339
어느 정도 감을 잡으셨길 바랍니다.

168
00 : 07 : 44.299 -> 00 : 07 : 47.959
이 모든 것은 프로그래밍 연습을 한 후에

169
00 : 07 : 46.339 -> 00 : 07 : 50.299
더 명확해질 것입니다.

170
00 : 07 : 47.959 -> 00 : 07 : 51.829
하지만 실제로 프로그래밍 연습을하기 전에

171
00 : 07 : 50.299 -> 00 : 07 : 54.079
vectorization에 대해 먼저 이야기 해 보겠습니다.

172
00 : 07 : 51.829 -> 00 : 07 : 56.419
그러면 당신은 단 한개의

173
00 : 07 : 54.079 -> 00 : 07 : 58.369
for 구문도 사용하지 않고

174
00 : 07 : 56.419 -> 00 : 08 : 01.479
이 모든 것들을 실행해서

175
00 : 07 : 58.369 -> 00 : 08 : 01.479
graidient descent의 한 단계 반복을 진행할 수 있을겁니다.