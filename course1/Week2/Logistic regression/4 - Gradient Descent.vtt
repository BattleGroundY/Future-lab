WEBVTT

1
00 : 00 : 00.590 -> 00 : 00 : 03.210
여러분은 logistic regression 모델을 보고 왔습니다.

2
00 : 00 : 03.210 -> 00 : 00 : 06.560
또한 단일 training 샘플에 대한 예측이
얼마나 잘하고 있는지를 측정하는

3
00 : 00 : 06.560 -> 00 : 00 : 08.780
loss 함수에 대해 학습 했습니다.

4
00 : 00 : 08.780 -> 00 : 00 : 13.530
또한 전체 training 샘플에 대해
파라미터 w와 b가 얼마나 잘하고 있는지를 측정하는

5
00 : 00 : 13.530 -> 00 : 00 : 16.590
cost 함수에 대해서도 보고 왔습니다.

6
00 : 00 : 16.590 -> 00 : 00 : 21.600
이제 여러분의 training set에서 파라미터 w와 b를
학습하기 위해

7
00 : 00 : 21.600 -> 00 : 00 : 25.730
gradient descent 알고리즘을 
어떻게 사용할 수 있는지를 알아 봅시다.

8
00 : 00 : 25.730 -> 00 : 00 : 30.030
복습해보면,
이것이 익숙한 logistic regression 알고리즘입니다.

9
00 : 00 : 31.130 -> 00 : 00 : 34.700
그리고 두 번째 줄에는 cost 함수 J가 있고,

10
00 : 00 : 34.700 -> 00 : 00 : 37.879
이것은 파라미터 w와 b의 함수입니다.

11
00 : 00 : 37.879 -> 00 : 00 : 39.960
이 함수는 loss 함수의 평균으로 정의되어 있습니다.

12
00 : 00 : 39.960 -> 00 : 00 : 44.140
따라서 loss 함수의 총합 m 분의 1로되어 있습니다.

13
00 : 00 : 44.140 -> 00 : 00 : 48.470
즉, loss 함수는 알고리즘이

14
00 : 00 : 48.470 -> 00 : 00 : 53.170
각각의 training 샘플에 대한
실제 레이블  y (i)와 비교해서

15
00 : 00 : 53.170 -> 00 : 00 : 58.000
얼마나 잘 y hat (i)을 예측할 수 있는지를 측정합니다.

16
00 : 00 : 58.000 -> 00 : 01 : 00.886
오른쪽에 전개 된 수식이 적혀 있습니다.

17
00 : 01 : 00.886 -> 00 : 01 : 04.130
cost 함수는 training 데이터에서

18
00 : 01 : 04.130 -> 00 : 01 : 06.760
파라미터 w와 b가 잘 작동하고 있는지를 측정합니다.

19
00 : 01 : 06.760 -> 00 : 01 : 11.510
파라미터 w와 b를 학습하기 위해,

20
00 : 01 : 11.510 -> 00 : 01 : 17.930
우리는 cost 함수 J (w, b)를 가능한 한 최소화하는
w와 b를 발견하고 싶어하는 것이 자연스러울 겁니다.

21
00 : 01 : 17.930 -> 00 : 01 : 21.320
이것이 gradient descent의 그림입니다.

22
00 : 01 : 21.320 -> 00 : 01 : 25.320
이 그림에서 가로 축 두개는

23
00 : 01 : 25.320 -> 00 : 01 : 28.510
각각 파라미터 w와 b를 나타냅니다.

24
00 : 01 : 28.510 -> 00 : 01 : 32.350
실제로 사용할 때는 w는 더 고차원의 수 가 되지만,
이번 그래프를 그려볼 목적으로,

25
00 : 01 : 32.350 -> 00 : 01 : 38.190
w를 하나의 실수로,
b도 하나의 실수로 둡시다.

26
00 : 01 : 38.190 -> 00 : 01 : 40.770
cost 함수 J (w, b)는

27
00 : 01 : 40.770 -> 00 : 01 : 45.130
수평 축 w와 b에있는면입니다.

28
00 : 01 : 45.130 -> 00 : 01 : 50.720
면의 높이가 한 지점에서 J (w, b)의 값을 보여줍니다.

29
00 : 01 : 50.720 -> 00 : 01 : 55.070
여기에서 우리가 원하는 것은

30
00 : 01 : 55.070 -> 00 : 01 : 59.730
J가 최소화되어있는 부분에 대응하는
w와 b의 값을 찾을 수 있냐는 것입니다.

31
00 : 02 : 00.830 -> 00 : 02 : 06.050
cost 함수 J는 볼록한 형태임을 알 수 있습니다.

32
00 : 02 : 06.050 -> 00 : 02 : 10.327
이건 그냥 큰 그릇 이라고 생각하시면 됩니다.

33
00 : 02 : 10.327 -> 00 : 02 : 13.717
이러한 볼록 함수는

34
00 : 02 : 13.717 -> 00 : 02 : 18.120
이와 같이 볼록하지 않고
여러 가지 극소점을 가지는 함수와는 반대되는 것입니다.

35
00 : 02 : 18.120 -> 00 : 02 : 22.240
여기에서 정의 된 비용 함수 J (w, b)가
볼록하다는 것은

36
00 : 02 : 22.240 -> 00 : 02 : 27.020
우리가이 비용 함수 J를
logistic regression에 사용할 수 있는

37
00 : 02 : 27.020 -> 00 : 02 : 29.610
가장 큰 이유 중 하나입니다.

38
00 : 02 : 29.610 -> 00 : 02 : 33.810
매개 변수로 좋은 값을 찾으려면

39
00 : 02 : 33.810 -> 00 : 02 : 39.160
우선 w와 b를 특정 값으로 initialize(초기화)합니다.

40
00 : 02 : 39.160 -> 00 : 02 : 43.360
이 그래프에서는 붉은 점으로 나타냅니다.

41
00 : 02 : 43.360 -> 00 : 02 : 47.562
로지스틱 회귀는
거의 모든 초기화 방법이 다 잘됩니다.

42
00 : 02 : 47.562 -> 00 : 02 : 50.690
그 방법들 대부분은 0으로 초기화하는 것입니다.

43
00 : 02 : 50.690 -> 00 : 02 : 52.910
무작위로 초기화하는 방법으로도 갈 수 있습니다.

44
00 : 02 : 52.910 -> 00 : 02 : 55.630
하지만 로지스틱 회귀에서는별로 사용되지 않습니다.

45
00 : 02 : 55.630 -> 00 : 02 : 59.310
그러나 이 함수가 볼록 하기 때문에
어떤 초기 값으로 설정했다고해도

46
00 : 02 : 59.310 -> 00 : 03 : 02.180
대체로 같은 점에 도착할 것입니다.

47
00 : 03 : 02.180 -> 00 : 03 : 06.450
경사 하강 법이 초기 값의 점에서 출발 하여

48
00 : 03 : 06.450 -> 00 : 03 : 10.310
가장 내려가는 경사가 가파른 방향으로 한 걸음 이동합니다.

49
00 : 03 : 10.310 -> 00 : 03 : 15.290
그래디언트 하강의 한 단계를 거친 후에는 
. 
50
00 : 03 : 15.290 -> 00 : 03 : 19.320
가장 가파른 하강 방향 또는 가능한 빠른 내리막 길로
 
51
00 : 03 : 19.320 -> 00 : 03 : 21.250
걸음을 시도하기 때문에 만약 그 지점이 가장 아래쪽 이라면
거기에서 끝날 수도 있습니다. 

52
00 : 03 : 21.250 -> 00 : 03 : 23.600
이것이 경사 하강 법의 한번의 반복 입니다.

53
00 : 03 : 23.600 -> 00 : 03 : 27.084
두 번 반복하면 여기 

54
00 : 03 : 27.084 -> 00 : 03 : 28.830
세 번째는 여기 라는 느낌입니다.

55
00 : 03 : 28.830 -> 00 : 03 : 32.640
아마 전체 극소점이 여기에서는 
그래프의 뒷면에 숨어 버리고 있습니다만

56
00 : 03 : 32.640 -> 00 : 03 : 38.880
당신이 전체 극소점을 상상 해보고 
가까워질 수 있기를 바랍니다.

57
00 : 03 : 38.880 -> 00 : 03 : 42.300
이 그림은 경사 하강 법의 그림입니다.

58
00 : 03 : 42.300 -> 00 : 03 : 44.310
좀 더 자세하게 그려 봅시다.

59
00 : 03 : 44.310 -> 00 : 03 : 47.750
그림을 그리기 위해,
J (w)라는 함수를 최소화한다고 가정합니다.

60
00 : 03 : 47.750 -> 00 : 03 : 51.700
그 함수는 이런 모양을 하고 있는 것입니다.

61
00 : 03 : 51.700 -> 00 : 03 : 54.650
그리기 쉽게 하기 위해 지금은 b를 무시하고 있습니다.

62
00 : 03 : 54.650 -> 00 : 03 : 59.210
이렇게하면,
고차원이 아니라 한 차원으로 나타낼 수 있습니다.

63
00 : 03 : 59.210 -> 00 : 04 : 01.240
경사 하강 법을 실시하는 것은,

64
00 : 04 : 01.240 -> 00 : 04 : 06.740
다음과 같은 업데이트를 반복 할 것입니다.

65
00 : 04 : 06.740 -> 00 : 04 : 09.467
w 값을 가지고 업데이트하고

66
00 : 04 : 09.467 -> 00 : 04 : 12.508
colon equals(:=)라는 기호를 사용하여
w를 업데이트 할 것을 나타냅시다.

67
00 : 04 : 12.508 -> 00 : 04 : 17.426
w를 w - α × dJ (w) / dw 로 나타냅시다.

68
00 : 04 : 17.426 -> 00 : 04 : 22.200
dJ (w) / dw라는 것은 미분입니다.

69
00 : 04 : 22.200 -> 00 : 04 : 26.230
이 알고리즘이 수렴 할 때까지 반복합니다.

70
00 : 04 : 26.230 -> 00 : 04 : 30.666
표기법에 대한 몇 가지 설명하면,
여기에서 α는 learning rate(학습 속도)이고.

71
00 : 04 : 30.666 -> 00 : 04 : 36.820
경사 하강 법의 반복마다
얼마나 걸음을 크게 할지를 결정할 것입니다.

72
00 : 04 : 36.820 -> 00 : 04 : 41.200
나중에 learning rate α를 결정하는 
몇 가지 방법을 이야기합시다.

73
00 : 04 : 41.200 -> 00 : 04 : 44.490
두 번째는, 이것은 미분 연산 입니다.

74
00 : 04 : 44.490 -> 00 : 04 : 48.010
기본적으로 매개 변수 w에 대해 
적정하게 변경 하는걸 기초로 합니다.

75
00 : 04 : 48.010 -> 00 : 04 : 52.700
경사 하강 법을 구현하는 코드를 쓰기 시작 때

76
00 : 04 : 52.700 -> 00 : 04 : 57.380
코드의 변수 이름 dw가
이 미분 항을 표현 한다는것을

77
00 : 04 : 58.620 -> 00 : 05 : 02.300
잊지 마세요.

78
00 : 05 : 02.300 -> 00 : 05 : 06.551
따라서 코드를 작성할 때,

79
00 : 05 : 06.551 -> 00 : 05 : 10.046
w : = w-α × dw라는 느낌으로 쓸 것입니다.

80
00 : 05 : 10.046 -> 00 : 05 : 14.750
즉, dw라는 변수 이름이 미분 항을 나타내는
것입니다.

81
00 : 05 : 14.750 -> 00 : 05 : 19.330
이제 이 경사 하강 법 업데이트의 의미를
이해할 수 있도록 설명 하겠습니다.

82
00 : 05 : 19.330 -> 00 : 05 : 21.880
w가 여기에 있다고 합시다.

83
00 : 05 : 21.880 -> 00 : 05 : 26.060
그러면 비용 함수 J (w)는
여기 있는 것입니다.

84
00 : 05 : 26.060 -> 00 : 05 : 29.270
미분의 정의는

85
00 : 05 : 29.270 -> 00 : 05 : 31.420
함수의 한 지점에서의 기울기를 의미한다는 걸
기억하십시오.

86
00 : 05 : 31.420 -> 00 : 05 : 36.190
함수의 기울기는

87
00 : 05 : 36.190 -> 00 : 05 : 40.290
이 점에서 J (w)의 접선이되는 삼각형의 높이를
폭으로 나눈 값입니다.

88
00 : 05 : 40.290 -> 00 : 05 : 43.900
여기에서 미분 한 결과는 양수입니다.

89
00 : 05 : 43.900 -> 00 : 05 : 48.830
w는 w - learning rate X dw 에의해 업데이트 됩니다.

90
00 : 05 : 48.830 -> 00 : 05 : 53.310
미분값이 양수인 것은 w에서 빼기가 되므로,

91
00 : 05 : 53.310 -> 00 : 05 : 55.260
업데이트된 w점은 왼쪽으로 이동하게됩니다.

92
00 : 05 : 55.260 -> 00 : 05 : 59.380
따라서, 만약 w를 큰 숫자로 시작하면

93
00 : 05 : 59.380 -> 00 : 06 : 04.450
경사 하강 법은 알고리즘이
천천히 파라미터를 작게하도록 작동합니다.

94
00 : 06 : 04.450 -> 00 : 06 : 08.545
또 다른 예로, 만약 w가 여기에 있으면

95
00 : 06 : 08.545 -> 00 : 06 : 15.050
이 점에서의 기울기 dJ / dw는 음수이기 때문에,

96
00 : 06 : 15.050 -> 00 : 06 : 22.771
경사 하강 법은 w를 w - α x 음수 값에 의해
업데이트합니다.

97
00 : 06 : 22.771 -> 00 : 06 : 27.122
그렇게 반복을 계속하게 되면

98
00 : 06 : 27.122 -> 00 : 06 : 31.530
w는 점점 커져갑니다.

99
00 : 06 : 31.530 -> 00 : 06 : 34.387
즉, 오른쪽에서 출발하던 왼쪽에서 출발하던

100
00 : 06 : 34.387 -> 00 : 06 : 39.000
경사 하강 법이 전체 최소 지점으로 
이끌어주는 것입니다.

101
00 : 06 : 39.000 -> 00 : 06 : 43.100
만약 당신이 미분과 미적분에 대해 잘 모르거나

102
00 : 06 : 43.100 -> 00 : 06 : 49.710
dJ (w) / dw라는 절의 의미를 몰라도,
걱정하지 마십시오.

103
00 : 06 : 49.710 -> 00 : 06 : 53.770
다음 비디오에서 미분에 대해 좀 더 이야기합니다.

104
00 : 06 : 53.770 -> 00 : 06 : 56.761
미적분에 대한 깊은 지식이 있으면,

105
00 : 06 : 56.761 -> 00 : 07 : 02.321
신경망의 기능에 대해
더 깊은 직관을 얻을 수있을 것입니다.

106
00 : 07 : 02.321 -> 00 : 07 : 05.471
하지만 만약 그렇게 미적분에 대해 잘 모르더라도,

107
00 : 07 : 05.471 -> 00 : 07 : 10.091
다음의 몇 가지 동영상들이 미분에 대해
충분한 직관을 전하므로

108
00 : 07 : 10.091 -> 00 : 07 : 14.980
당신도 신경망을
효과적으로 사용할 수있게 될 것입니다.

109
00 : 07 : 14.980 -> 00 : 07 : 16.410
그러나 지금의 시점에서는

110
00 : 07 : 16.410 -> 00 : 07 : 21.520
이 절은 함수의 기울기를 나타내고 있고,

111
00 : 07 : 21.520 -> 00 : 07 : 26.760
매개 변수의 현재 설정에서 함수의 기울기를 
알고 싶기 때문에 이러한 가파른 강하를 통해 

112
00 : 07 : 26.760 -> 00 : 07 : 31.140
비용 함수 J는 내리막 길을 걷는 방향으로 나아간다..

113
00 : 07 : 31.140 -> 00 : 07 : 35.450
라는 직감이 있으면 괜찮습니다.

114
00 : 07 : 36.660 -> 00 : 07 : 42.520
우리는 위에서 파라미터가 w밖에 없을 경우
J (w)의 경사 하강 법을 그려봤습니다.

115
00 : 07 : 42.520 -> 00 : 07 : 47.150
로지스틱 회귀는
함수는 w와 b가 모두 있습니다.

116
00 : 07 : 47.150 -> 00 : 07 : 50.894
그 경우,이 경사 하강 법의 내부 루프가

117
00 : 07 : 50.894 -> 00 : 07 : 53.302
다음과 같이 반복되어야합니다.

118
00 : 07 : 53.302 -> 00 : 07 : 57.970
w를 w - α × w에 대한 J (w, b) 미분값으로

119
00 : 07 : 57.970 -> 00 : 08 : 02.030
업데이트 할 수 있습니다.
(영상의 식을 참고 하세요)

120
00 : 08 : 02.030 -> 00 : 08 : 07.460
그리고 b는 b - α × b에 대한 
비용 함수의 미분값으로

121
00 : 08 : 07.460 -> 00 : 08 : 12.270
업데이트됩니다.

122
00 : 08 : 12.270 -> 00 : 08 : 17.300
아래에 쓴 두 식이 실제로 구현 되는 업데이트 입니다.

123
00 : 08 : 17.300 -> 00 : 08 : 22.320
여담입니다 만, 미적분학의 표기법에 관해

124
00 : 08 : 22.320 -> 00 : 08 : 24.560
조금 까다로운 부분이있어서 언급 해둡니다.

125
00 : 08 : 24.560 -> 00 : 08 : 28.387
미적분학을 이해하는 것이 그렇게 
중요하다고 생각하지 않지만,

126
00 : 08 : 28.387 -> 00 : 08 : 32.411
당신이 이것을 보았을 때 
고민하지 않게 하고 싶을 뿐 입니다.

127
00 : 08 : 32.411 -> 00 : 08 : 35.519
미적분을 할 때는

128
00 : 08 : 35.519 -> 00 : 08 : 40.730
실제로 이런 구불구불한 기호로 씁니다.

129
00 : 08 : 40.730 -> 00 : 08 : 46.160
이것은 특이한 글꼴로 날려 쓴
d 소문자와 비슷하지만,

130
00 : 08 : 46.160 -> 00 : 08 : 51.070
이는 절대 어려운 의미가 아니라

131
00 : 08 : 51.070 -> 00 : 08 : 56.145
함수 J (w, b)가 얼마나

132
00 : 08 : 56.145 -> 00 : 09 : 01.580
w의 방향으로 기울어 져 있는지를 
보여 주고 있을 뿐이라고 생각합니다.

133
00 : 09 : 01.580 -> 00 : 09 : 06.640
미적분학의 표기법은

134
00 : 09 : 06.640 -> 00 : 09 : 11.780
나는 논리적이라고 생각하지 않습니다

135
00 : 09 : 11.780 -> 00 : 09 : 16.940
그리고 이 표기는 일을 더 복잡하게 만든다고 생각합니다.

136
00 : 09 : 16.940 -> 00 : 09 : 21.550
만약 J에 두 개 이상의 변수가 있으면, 
소문자 d와 닮은 기호 대신에 더 이상한 기호를 사용합니다.

137
00 : 09 : 21.550 -> 00 : 09 : 24.380
이것은 편미분 기호라고합니다.

138
00 : 09 : 24.380 -> 00 : 09 : 26.120
하지만 너무 걱정하지 마십시오.

139
00 : 09 : 26.120 -> 00 : 09 : 31.090
만약 J가 하나의 변수에 대한 함수라면, d의 문자를 사용합니다.

140
00 : 09 : 31.090 -> 00 : 09 : 33.960
즉,이 이상한 편미분 기호를 사용하거나

141
00 : 09 : 33.960 -> 00 : 09 : 38.040
아니면 위와 같이 소문자 d를 사용하는지의 차이는

142
00 : 09 : 38.040 -> 00 : 09 : 41.570
J가 여러 변수의 함수인지 여부입니다.

143
00 : 09 : 41.570 -> 00 : 09 : 45.900
여러 변수가 있다면이 편미분 기호를 사용하고,

144
00 : 09 : 45.900 -> 00 : 09 : 51.480
변수가 하나 뿐이라면 소문자 d를 사용합니다.

145
00 : 09 : 51.480 -> 00 : 09 : 55.410
이것은 필요 이상으로 복잡하게 보이는

146
00 : 09 : 55.410 -> 00 : 09 : 58.540
미적분학의 이상한 표기 규칙 중 하나일 뿐입니다.

147
00 : 09 : 58.540 -> 00 : 10 : 03.300
만약 편미분 기호를 보게 되면

148
00 : 10 : 03.300 -> 00 : 10 : 07.290
여러 변수들 중 하나로
함수의 기울기를 측정 한 것이라고 생각하시면 됩니다.

149
00 : 10 : 07.290 -> 00 : 10 : 12.530
미적분학의 형식으로 올바른 양식을 따르자면

150
00 : 10 : 12.530 -> 00 : 10 : 18.070
여기 보이는 J는 하나가 아닌 두 개의 변수가 있기 때문에,

151
00 : 10 : 18.070 -> 00 : 10 : 22.540
아래에 쓰여진 이
편미분 기호로 표현 해야하네요.

152
00 : 10 : 22.540 -> 00 : 10 : 28.290
하지만 소문자 d와 크게 의미는 바뀌지 않습니다.

153
00 : 10 : 28.290 -> 00 : 10 : 31.360
마지막으로, 이것을 코드로 구현하는 경우에는

154
00 : 10 : 31.360 -> 00 : 10 : 36.220
w를 얼마나 업데이트 하는가하는이 양을

155
00 : 10 : 36.220 -> 00 : 10 : 41.980
변수 dw로 표현하는 방법을 사용합니다.

156
00 : 10 : 41.980 -> 00 : 10 : 44.220
b를 얼마나 업데이트 할 것인가 양도

157
00 : 10 : 44.220 -> 00 : 10 : 47.230
마찬가지로,

158
00 : 10 : 47.230 -> 00 : 10 : 50.740
코드는 db로 나타냅니다.

159
00 : 10 : 50.740 -> 00 : 10 : 55.580
여기까지가 경사 하강 법을 구현하는 방법입니다.

160
00 : 10 : 55.580 -> 00 : 10 : 59.830
만약 미적분을 최근 수 년 동안 본적도 없다면,

161
00 : 10 : 59.830 -> 00 : 11 : 03.770
지금까지 나온 미분이 너무 많다고 느낄지도 모릅니다.

162
00 : 11 : 03.770 -> 00 : 11 : 06.330
하지만 그렇게 느끼고 있어도 괜찮습니다.

163
00 : 11 : 06.330 -> 00 : 11 : 10.150
다음 동영상에서 미분에 대한 직관을 더 기를겁니다.

164
00 : 11 : 10.150 -> 00 : 11 : 13.560
미적분에 대해
수학적으로 너무 깊이 이해하지 않아도

165
00 : 11 : 13.560 -> 00 : 11 : 16.310
직관적인 이해만 있다면

166
00 : 11 : 16.310 -> 00 : 11 : 19.130
신경망을
효율적으로 사용할 수 있습니다.

167
00 : 11 : 19.130 -> 00 : 11 : 22.743
자, 다음 동영상으로 이동하여

168
00 : 11 : 22.743 -> 00 : 11 : 23.470
좀 더 미분에 대해 얘기해 봅시다.