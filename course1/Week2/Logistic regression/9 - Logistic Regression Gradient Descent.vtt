WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 02.250
어서 오세요. 이 비디오에서는

2
00 : 00 : 02.250 -> 00 : 00 : 04.980
미분을 계산하는 방법을 말씀 드리겠습니다.

3
00 : 00 : 04.980 -> 00 : 00 : 08.330
이는 logistic regression에서 gradient descent를
구현 할 수 있도록 하기 위해서입니다

4
00 : 00 : 08.330 -> 00 : 00 : 11.040
핵심은 여러분이 무엇을 구현하려고 하는지 입니다.

5
00 : 00 : 11.040 -> 00 : 00 : 13.230
즉, 이것은 여러분이 로지스틱 회귀에서 
경사 하강 법을 

6
00 : 00 : 13.230 -> 00 : 00 : 17.725
구현하기 위해 필요한 핵심 방정식이 될겁니다.

7
00 : 00 : 17.725 -> 00 : 00 : 22.185
이 비디오에서는 계산 그래프를 사용하여
이 계산을 하려고 생각합니다

8
00 : 00 : 22.185 -> 00 : 00 : 25.320
계산 그래프를 사용하여 로지스틱 회귀에 대한 
경사 강하 법을 유도 하는 것은 다소 과잉이라고 생각합니다. 

9
00 : 00 : 25.320 -> 00 : 00 : 29.342
그러나 이러한 방식에 익숙해지기 위해 

10
00 : 00 : 29.342 -> 00 : 00 : 31.183
이 방식으로 설명하도록 하겠습니다.

11
00 : 00 : 31.183 -> 00 : 00 : 33.975
이 방식에 익숙해 지는 것이 본격적으로 신경망에 
12
00 : 00 : 33.975 -> 00 : 00 : 38.370
대해서 이야기 할 때 더 의미가 있습니다.

13
00 : 00 : 38.370 -> 00 : 00 : 44.235
그러면 본격적으로 로지스틱 회귀 경사 강하 법에
대해서 알아봅시다.

14
00 : 00 : 44.235 -> 00 : 00 : 49.070
복습먼저 하겠습니다.
로지스틱 회귀는 다음과 같은 것이라고 설정했었습니다.

15
00 : 00 : 49.070 -> 00 : 00 : 53.220
예측 값인 y hat 이렇게 정의 되어 있구요.

16
00 : 00 : 53.220 -> 00 : 00 : 56.490
z는 이렇게 정의되어 있습니다.

17
00 : 00 : 56.490 -> 00 : 01 : 01.800
만약 현재 이 데이터의 경우에만 초점을 맞춘다면
현재 데이터에 관해서

18
00 : 01 : 01.800 -> 00 : 01 : 03.630
loss함수는 다음과 같이

19
00 : 01 : 03.630 -> 00 : 01 : 05.190
정의됩니다

20
00 : 01 : 05.190 -> 00 : 01 : 07.855
a는 로지스틱 회귀의 출력값

21
00 : 01 : 07.855 -> 00 : 01 : 10.535
y는 실제 출력값들의 레이블입니다

22
00 : 01 : 10.535 -> 00 : 01 : 15.735
이것을 계산 그래프로 작성해 보겠습니다.

23
00 : 01 : 15.735 -> 00 : 01 : 20.520
이 그래프는 입력 요소가 
X1 X2 두 가지 밖에 없다고 가정합시다.

24
00 : 01 : 20.520 -> 00 : 01 : 22.860
z를 계산하기 위해서는

25
00 : 01 : 22.860 -> 00 : 01 : 27.030
w1 w2의 입력이 필요합니다

26
00 : 01 : 27.030 -> 00 : 01 : 31.130
그리고 입력 요소인 x1 x2 이외에도
b가 필요합니다

27
00 : 01 : 31.130 -> 00 : 01 : 33.705
계산 그래프에서 이것 들이

28
00 : 01 : 33.705 -> 00 : 01 : 36.910
z를 계산하는 데 사용됩니다

29
00 : 01 : 36.910 -> 00 : 01 : 41.588
w1x1 + w2x2 + b 가 식이 될것이고,

30
00 : 01 : 41.588 -> 00 : 01 : 45.380
이 식을 사각형으로 묶습니다

31
00 : 01 : 45.380 -> 00 : 01 : 48.555
그리고 y hat을 계산합니다

32
00 : 01 : 48.555 -> 00 : 01 : 52.244
a = σ (z)에서

33
00 : 01 : 52.244 -> 00 : 01 : 55.740
이것이 계산 그래프의 다음 단계입니다

34
00 : 01 : 55.740 -> 00 : 01 : 58.725
그리고 마지막으로

35
00 : 01 : 58.725 -> 00 : 02 : 01.840
L (a, y)를 계산하지만
식은 여기에 옮겨 쓰지 않겠습니다.

36
00 : 02 : 01.840 -> 00 : 02 : 06.900
로지스틱 회귀에서 우리가 원하는 것은

37
00 : 02 : 06.900 -> 00 : 02 : 12.830
변수 w와 b를 변경해서
loss를 줄이는 것입니다.

38
00 : 02 : 12.830 -> 00 : 02 : 15.870
우리는 저번 시간에 
하나의 샘플에서 loss를 계산할 수 있는

39
00 : 02 : 15.870 -> 00 : 02 : 19.280
4단계의 propagation 계산 방법을
기술했습니다

40
00 : 02 : 19.280 -> 00 : 02 : 23.940
이번에는 미분을 역방향으로 계산 해 나가는 방법을
이야기합시다

41
00 : 02 : 23.940 -> 00 : 02 : 26.025
이것은 깨끗하게 고쳐 쓴 그림입니다

42
00 : 02 : 26.025 -> 00 : 02 : 30.690
이 loss에 관한 미분을 계산하는 것이
목적이므로

43
00 : 02 : 30.690 -> 00 : 02 : 33.570
역방향으로 계산을 시작할 때
먼저 할 것은

44
00 : 02 : 33.570 -> 00 : 02 : 38.010
a에 대한 loss의 미분을
(여기에 조금 추가 기입합니다)

45
00 : 02 : 38.010 -> 00 : 02 : 41.940
계산하는 것 입니다.

46
00 : 02 : 41.940 -> 00 : 02 : 43.570
코드를 입력한다면,

47
00 : 02 : 43.570 -> 00 : 02 : 49.000
da를 변수로 쓰면 될겁니다.

48
00 : 02 : 49.000 -> 00 : 02 : 52.725
당신이 미적분을 안다면

49
00 : 02 : 52.725 -> 00 : 03 : 02.004
이것이 -y / a + (1-y) / 1-a 로 
쓰여진다는 것을 알 겁니다.
(log함수에 대한 미분을 한 것, 모르셔도 됩니당!)

50
00 : 03 : 02.004 -> 00 : 03 : 06.185
만약 미적분을 안다면

51
00 : 03 : 06.185 -> 00 : 03 : 07.535
loss함수의 식을 사용해서

52
00 : 03 : 07.535 -> 00 : 03 : 10.515
소문자 a 변수에 대한 미분을
계산하고

53
00 : 03 : 10.515 -> 00 : 03 : 12.792
이와 같은 식을 만들 수 있습니다

54
00 : 03 : 12.792 -> 00 : 03 : 15.280
만약 미적분을 몰라도
걱정하지 마십시오

55
00 : 03 : 15.280 -> 00 : 03 : 17.960
이 과정을 진행하는 도중, 필요한 때에

56
00 : 03 : 17.960 -> 00 : 03 : 20.100
미분 방정식을 알려드립니다

57
00 : 03 : 20.100 -> 00 : 03 : 21.185
미적분학의 전문가라면

58
00 : 03 : 21.185 -> 00 : 03 : 24.590
미적분학을 사용하여
이전 슬라이드 loss함수의 식을 보고

59
00 : 03 : 24.590 -> 00 : 03 : 29.504
a에 관하여 미분을
한번 해보는 것도 좋습니다

60
00 : 03 : 29.504 -> 00 : 03 : 32.635
다시 말씀드리지만, 미적분을 몰라도
걱정하지 마십시오

61
00 : 03 : 32.635 -> 00 : 03 : 35.491
a에 대한 최종 출력 변수의 미분값인
da에 대한

62
00 : 03 : 35.491 -> 00 : 03 : 38.825
계산이 끝나면

63
00 : 03 : 38.825 -> 00 : 03 : 40.715
역방향으로 한 단계 더 갈 수 있습니다

64
00 : 03 : 40.715 -> 00 : 03 : 45.525
dz을 한번 구해 보겠습니다.

65
00 : 03 : 45.525 -> 00 : 03 : 47.648
dz는 Python 코드의 변수 이름이지만

66
00 : 03 : 47.648 -> 00 : 03 : 51.200
이 dz는 loss에 대한 미분인 dL와 함께

67
00 : 03 : 51.200 -> 00 : 03 : 53.618
dL / dz 라고 쓸 수 있을 겁니다.

68
00 : 03 : 53.618 -> 00 : 03 : 59.850
loss에 대해 L(a ,y)를
L 대신에 인수로 써도 상관 없습니다

69
00 : 03 : 59.850 -> 00 : 04 : 04.230
두 표기 방법 모두
당연히 허용됩니다

70
00 : 04 : 04.230 -> 00 : 04 : 09.605
이 식은 미분 계산을 하고 나면,
a-y로 표현할 수 있습니다

71
00 : 04 : 09.605 -> 00 : 04 : 14.685
이런 식들은 미적분을 잘 알고있는 사람들을 위해
조금 추가로 알려드리는 것일 뿐입니다.

72
00 : 04 : 14.685 -> 00 : 04 : 16.795
잘 몰라도
걱정하지 마십시오

73
00 : 04 : 16.795 -> 00 : 04 : 20.320
이 dL / dz는

74
00 : 04 : 20.320 -> 00 : 04 : 27.850
dL / da 곱하기 da / dz 로
표현할 수 있습니다

75
00 : 04 : 27.850 -> 00 : 04 : 29.940
또 da / dz를 미분한 값은

76
00 : 04 : 29.940 -> 00 : 04 : 33.755
a(1-a)가 됩니다.

77
00 : 04 : 33.755 -> 00 : 04 : 37.800
dL / da는 방금 전에 구해 놓은
이 식이 되겠군요

78
00 : 04 : 37.800 -> 00 : 04 : 41.530
이 두 가지 값을 사용하여

79
00 : 04 : 41.530 -> 00 : 04 : 43.846
dL / da 의 미분값 및
da / dz 의 미분값을 사용하여

80
00 : 04 : 43.846 -> 00 : 04 : 47.165
이 두 가지를 곱하면

81
00 : 04 : 47.165 -> 00 : 04 : 51.915
a-y 처럼 단순화시켜
표현 할 수 있습니다

82
00 : 04 : 51.915 -> 00 : 04 : 53.220
이렇게 a-y라는 미분 값을 산출했습니다

83
00 : 04 : 53.220 -> 00 : 04 : 57.390
이렇게 chain rule을 활용하면
간단하게 계산할 수 있습니다.

84
00 : 04 : 57.390 -> 00 : 05 : 02.770
미적분에 익숙 하신 경우
아무쪼록 전체 계산의 흐름을 복습해 보세요.

85
00 : 05 : 02.770 -> 00 : 05 : 05.345
하지만 익숙하지 않은 경우에는
dz = a - y 만 알아두면 됩니다.

86
00 : 05 : 05.345 -> 00 : 05 : 09.365
당신을 위해
제가 이 미적분 계산을 미리 해 둔 것입니다.

87
00 : 05 : 09.365 -> 00 : 05 : 13.010
backpropagation의 최종 단계는
역방향으로 계산을 진행 하면서

88
00 : 05 : 13.010 -> 00 : 05 : 17.480
w와 b를 얼마나 변화 시켜줘야 하는지를 
알아 내는 것 입니다.

89
00 : 05 : 17.480 -> 00 : 05 : 24.610
w1에 관한 미분은
"dw1"라고 표현할 수 있습니다

90
00 : 05 : 24.610 -> 00 : 05 : 31.810
"dw1"= x1 곱하기 dz입니다

91
00 : 05 : 31.810 -> 00 : 05 : 36.485
마찬가지로 w2 대한 dw2은

92
00 : 05 : 36.485 -> 00 : 05 : 39.455
x2 곱하기 dz입니다

93
00 : 05 : 39.455 -> 00 : 05 : 42.585
마지막으로 db = dz입니다

94
00 : 05 : 42.585 -> 00 : 05 : 47.375
하나의 데이터에 대해서만
경사 강하 법을 실시하고 싶은 경우에는

95
00 : 05 : 47.375 -> 00 : 05 : 49.280
이렇게 할 수 있습니다

96
00 : 05 : 49.280 -> 00 : 05 : 52.640
이 식을 사용하여 dz를 계산하고

97
00 : 05 : 52.640 -> 00 : 05 : 56.707
이러한 dw1 dw2 db 식을 사용하여

98
00 : 05 : 56.707 -> 00 : 06 : 01.170
이 업데이트를 실시합니다

99
00 : 06 : 01.170 -> 00 : 06 : 04.538
w1 := w1 - 알파 X dw1

100
00 : 06 : 04.538 -> 00 : 06 : 06.575
여기서 알파는 learning rate 입니다. 

101
00 : 06 : 06.575 -> 00 : 06 : 09.245
w2도 이와 같이 업데이트 할 수 있습니다

102
00 : 06 : 09.245 -> 00 : 06 : 14.170
b : = b-αdb입니다

103
00 : 06 : 14.170 -> 00 : 06 : 18.860
이것이 하나의 데이터에서의
경사 강하 법을 수행 하는 첫번째 단계입니다

104
00 : 06 : 18.860 -> 00 : 06 : 22.130
한 개의 training 데이터에서

105
00 : 06 : 22.130 -> 00 : 06 : 27.200
어떻게 미분을 계산하고, 로지스틱 회귀 
경사 강하 법을 구현하는지를 보고 왔습니다

106
00 : 06 : 27.200 -> 00 : 06 : 28.987
로지스틱 회귀 모델을 학습하려면

107
00 : 06 : 28.987 -> 00 : 06 : 34.700
한 개의 training 데이터가 아닌
m 개의 training 데이터를 사용해야 될 겁니다.

108
00 : 06 : 34.700 -> 00 : 06 : 36.120
다음 비디오 에서는,

109
00 : 06 : 36.120 -> 00 : 06 : 39.350
한 개의 training 데이터가 아니라

110
00 : 06 : 39.350 -> 00 : 06 : 40.760
전체 training 데이터에서
학습 시키도록

111
00 : 06 : 40.760 -> 00 : 06 : 42.400
이 아이디어들을 적용하는 방법을
알아 봅시다.