WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 03.475
이 동영상에서는 logistic regression에 대해
배울것 입니다.

2
00 : 00 : 03.475 -> 00 : 00 : 07.080
이것은 Supervised learning에서 출력 라벨 y가

3
00 : 00 : 07.080 -> 00 : 00 : 10.690
0 또는 1 일 때,
즉 binary classification 문제에서

4
00 : 00 : 10.690 -> 00 : 00 : 13.600
사용되는 알고리즘입니다.

5
00 : 00 : 13.600 -> 00 : 00 : 18.350
고양이인지 아닌지를 판단하고자
그 이미지에 대응하는

6
00 : 00 : 18.350 -> 00 : 00 : 23.150
특징 벡터 x의 입력이 있다고합시다.

7
00 : 00 : 23.150 -> 00 : 00 : 26.525
여러분은 예측 값을 출력 할 수
알고리즘이 갖고 싶을 겁니다.

8
00 : 00 : 26.525 -> 00 : 00 : 28.254
그 y의 예측을,

9
00 : 00 : 28.254 -> 00 : 00 : 31.130
y hat로 나타냅시다.

10
00 : 00 : 31.130 -> 00 : 00 : 35.896
보다 형식적으로 말하면,
특징 벡터 x가 주어진 경우

11
00 : 00 : 35.896 -> 00 : 00 : 40.630
당신은 Y hat 값이 1이 되기를 바랄겁니다.

12
00 : 00 : 40.630 -> 00 : 00 : 43.880
이전 동영상에서 본대로

13
00 : 00 : 43.880 -> 00 : 00 : 45.530
x가 이미지라고하면

14
00 : 00 : 45.530 -> 00 : 00 : 47.300
이미지가 고양이 일 확률을

15
00 : 00 : 47.300 -> 00 : 00 : 49.820
y hat이 가르쳐 주었으면 하는 것입니다.

16
00 : 00 : 49.820 -> 00 : 00 : 53.420
이전 동영상에서 말했듯이,

17
00 : 00 : 53.420 -> 00 : 00 : 56.960
x는 nx 차원 벡터입니다.

18
00 : 00 : 56.960 -> 00 : 01 : 02.000
그러자 logistic regression의 매개 변수는

19
00 : 01 : 02.000 -> 00 : 01 : 07.745
x처럼 nx 차원 벡터인 w와

20
00 : 01 : 07.745 -> 00 : 01 : 11.670
상수 인 b라는 것입니다.

21
00 : 01 : 11.670 -> 00 : 01 : 16.055
그래서 입력 x와 매개 변수 w, b가있을 때,

22
00 : 01 : 16.055 -> 00 : 01 : 20.595
어떻게 y hat을 출력할까요?

23
00 : 01 : 20.595 -> 00 : 01 : 22.970
작동이 안되긴 하겠지만
해 볼 수있는 것으로서,

24
00 : 01 : 22.970 -> 00 : 01 : 27.590
y hat을
W^t x X + b로 표현해 보는겁니다.

25
00 : 01 : 27.590 -> 00 : 01 : 33.045
즉 입력 x를 선형함수화 해 보는 겁니다.

26
00 : 01 : 33.045 -> 00 : 01 : 37.145
실제로 선형 회귀를 할 경우에는
이것을 사용합니다.

27
00 : 01 : 37.145 -> 00 : 01 : 41.345
그러나 binary classification에서는
이 알고리즘은 그다지 좋지 않습니다.

28
00 : 01 : 41.345 -> 00 : 01 : 45.575
왜냐하면
y hat은 y = 1 인 확률이 되야 하고,

29
00 : 01 : 45.575 -> 00 : 01 : 50.480
그것은 0과 1 사이(0<확률<1)에 있어야
되기 때문입니다.

30
00 : 01 : 50.480 -> 00 : 01 : 54.697
이 식에서는 0과 1사이로 나오기는 힘들어 보입니다.

31
00 : 01 : 54.697 -> 00 : 01 : 58.475
왜냐하면 이 식은 보시다시피 굉장히 커지거나
음수가 될 수도 있습니다.

32
00 : 01 : 58.475 -> 00 : 02 : 00.905
이러한 값이 0과 1 사이가 아니라면

33
00 : 02 : 00.905 -> 00 : 02 : 03.620
확률(0에서 1사이)에 맞지 않습니다.

34
00 : 02 : 03.620 -> 00 : 02 : 07.670
그래서 logistic regression은

35
00 : 02 : 07.670 -> 00 : 02 : 12.050
y hat값(확률과 동떨어진 값)에 
sigmoid 함수를 적용한 값입니다.

36
00 : 02 : 12.050 -> 00 : 02 : 14.850
sigmoid 함수는 이런 것입니다.

37
00 : 02 : 14.850 -> 00 : 02 : 24.000
수평 축을 z로하면
z에 대한 sigmoid함수는 이렇게됩니다.

38
00 : 02 : 24.000 -> 00 : 02 : 28.050
y값이 0에서 1까지 범위 안에서 부드럽게 이동합니다.

39
00 : 02 : 28.050 -> 00 : 02 : 30.120
y축에는 레이블을 쓰겠습니다.

40
00 : 02 : 30.120 -> 00 : 02 : 34.915
여기가 0이고, 보시다시피 0.5의 위치에서
y축과 교차합니다.

41
00 : 02 : 34.915 -> 00 : 02 : 41.305
이것이 z에 대한 sigmoid함수 이고,
우리는 wx + b 의 값을

42
00 : 02 : 41.305 -> 00 : 02 : 43.020
Z라고 표현할 것입니다.

43
00 : 02 : 43.020 -> 00 : 02 : 46.230
sigmoid함수의 공식은 이것입니다.

44
00 : 02 : 46.230 -> 00 : 02 : 49.380
실수인 z값에 대한 sigmoid 함수는

45
00 : 02 : 49.380 -> 00 : 02 : 52.510
1 + e ^ (- z) 분의 1입니다.

46
00 : 02 : 52.510 -> 00 : 02 : 54.695
여기에서 몇 가지를 알 수 있습니다.

47
00 : 02 : 54.695 -> 00 : 03 : 01.255
만약 z가 너무 크면,
e ^ (- z)는 0에 가깝습니다.

48
00 : 03 : 01.255 -> 00 : 03 : 03.420
그래서, 이 경우 z의 sigmoid값은 대략

49
00 : 03 : 03.420 -> 00 : 03 : 07.255
(1 + 0에 한없이 가까운) 분의 1입니다.

50
00 : 03 : 07.255 -> 00 : 03 : 11.280
왜냐하면 e의 (마이너스 큰 숫자)승 이라는 것은
0에 가까워지기 때문입니다.

51
00 : 03 : 11.280 -> 00 : 03 : 13.505
이것은 1에 가깝습니다.

52
00 : 03 : 13.505 -> 00 : 03 : 16.255
실제로 왼쪽의 그래프를 보면,

53
00 : 03 : 16.255 -> 00 : 03 : 20.475
z가 매우 클 때
z에 대한 sigmoid는 1에 가깝습니다.

54
00 : 03 : 20.475 -> 00 : 03 : 24.105
반대로, z가 너무 작거나

55
00 : 03 : 24.105 -> 00 : 03 : 28.970
또는 매우 큰 음수라면

56
00 : 03 : 29.180 -> 00 : 03 : 39.640
z에 대한 sigmoid는
1 + e ^ (- z) 분의 1이고,

57
00 : 03 : 39.640 -> 00 : 03 : 42.565
1 + e ^ (- z)는 거대한 숫자가 됩니다.

58
00 : 03 : 42.565 -> 00 : 03 : 47.944
(1+ 거대한 숫자) 분의 1로 생각하면,

59
00 : 03 : 47.944 -> 00 : 03 : 54.473
이것은

60
00 : 03 : 54.473 -> 00 : 03 : 56.570
0에 가깝습니다.

61
00 : 03 : 56.570 -> 00 : 04 : 00.325
사실, z가 매우 큰 음수가되면

62
00 : 04 : 00.325 -> 00 : 04 : 03.505
z에 대한 sigmoid는 0에 가깝습니다.

63
00 : 04 : 03.505 -> 00 : 04 : 06.070
logistic regression을 구현할 때

64
00 : 04 : 06.070 -> 00 : 04 : 10.350
당신이 하는 것은
y hat 값이 y가 1이될 확률을 잘 예측할 수 있도록

65
00 : 04 : 10.350 -> 00 : 04 : 15.220
매개변수 w와 b를 학습하는 것입니다.

66
00 : 04 : 15.220 -> 00 : 04 : 18.955
계속하기 전에 표기법에 대해 조금 더 설명합니다.

67
00 : 04 : 18.955 -> 00 : 04 : 20.830
우리가 신경망을
프로그램 할 때,

68
00 : 04 : 20.830 -> 00 : 04 : 26.855
주로 매개 변수 w와 매개 변수 b는
따로 둡니다.

69
00 : 04 : 26.855 -> 00 : 04 : 30.000
여기에서 b는 y절편 항에 해당합니다.

70
00 : 04 : 30.000 -> 00 : 04 : 31.295
다른 코스에서는

71
00 : 04 : 31.295 -> 00 : 04 : 35.110
당신은 아마 이들을 다르게 취급하고있는
표기를 본 적이 있을지도 모릅니다.

72
00 : 04 : 35.110 -> 00 : 04 : 42.205
관습적으로, x0라는 추가 변수를
정의하기도 하고 이는 1과 같습니다.

73
00 : 04 : 42.205 -> 00 : 04 : 47.250
이 경우 x는 nx + 1 차원입니다.

74
00 : 04 : 47.250 -> 00 : 04 : 53.865
그러면 y hat은 σ(θ의 전치)입니다.

75
00 : 04 : 53.865 -> 00 : 04 : 56.685
이 표기 관례에서

76
00 : 04 : 56.685 -> 00 : 05 : 00.510
θ는 벡터 매개 변수입니다.

77
00 : 05 : 00.510 -> 00 : 05 : 03.175
θ0, θ1, θ2 ... 그 뒤를이어서

78
00 : 05 : 03.175 -> 00 : 05 : 09.520
θnx까지 있습니다.

79
00 : 05 : 09.520 -> 00 : 05 : 11.723
θ0는 b의 것으로,

80
00 : 05 : 11.723 -> 00 : 05 : 13.663
이것은 단순한 실수(Real number)입니다.

81
00 : 05 : 13.663 -> 00 : 05 : 18.505
θ1에서 θnx가 w의 역할을합니다.

82
00 : 05 : 18.505 -> 00 : 05 : 20.350
당신이 신경망을
구현하는 경우에는

83
00 : 05 : 20.350 -> 00 : 05 : 26.145
b와 w를 별도의 매개 변수로
취급하는 것이 편합니다.

84
00 : 05 : 26.145 -> 00 : 05 : 27.430
그래서 이 클래스에서는

85
00 : 05 : 27.430 -> 00 : 05 : 32.087
아까 빨간색으로 쓴 표기법은
사용하지 않기로 합니다.

86
00 : 05 : 32.087 -> 00 : 05 : 36.330
이 표기법을 다른 클래스에서 본적이 없다면,
걱정하지 마십시오.

87
00 : 05 : 36.330 -> 00 : 05 : 39.610
이 표기법을 본 적이있는 사람들을
위한 이야기 였고,

88
00 : 05 : 39.610 -> 00 : 05 : 43.730
다만 이 코스에서 이 표기들을 사용하지
않는 다는 것을 말해주고 싶었습니다.

89
00 : 05 : 43.730 -> 00 : 05 : 45.235
본 적이 없다면,

90
00 : 05 : 45.235 -> 00 : 05 : 48.430
전혀 중요한 것이 아니기 때문에
걱정하지 마십시오.

91
00 : 05 : 48.430 -> 00 : 05 : 52.465
자, 이제 우리는 Logistic regression 모델이 
무엇 인지 보고 왔습니다.

92
00 : 05 : 52.465 -> 00 : 05 : 57.140
다음은 매개변수 w와 b를 바꾸기 위하여
cost 함수를 정의 해 보자.

93
00 : 05 : 57.140 -> 00 : 05 : 58.830
다음 비디오로 출바알~!