WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.440
이전 비디오는 경사 하강 법의 
수학적 정의를 이용해서

2
00 : 00 : 01.440 -> 00 : 00 : 05.700
예측 값을 계산하기 위해
벡터화를 사용하는 방법을 보고 왔습니다

3
00 : 00 : 05.700 -> 00 : 00 : 11.485
학습 데이터 전체에 대해
한 번에 소문자 a를 계산하는 방법입니다

4
00 : 00 : 11.485 -> 00 : 00 : 15.030
이 비디오에서는 학습 데이터 전체인 M개의 
training sample 에 대해

5
00 : 00 : 15.030 -> 00 : 00 : 19.205
기울기를 계산할 때
벡터화를 이용하는 방법을 봅니다

6
00 : 00 : 19.205 -> 00 : 00 : 21.380
이번에도 이것들 모두를 
한 번에 실행 하는 방법입니다

7
00 : 00 : 21.380 -> 00 : 00 : 22.890
이 비디오의 끝에서

8
00 : 00 : 22.890 -> 00 : 00 : 26.175
지금까지의 내용을 정리하고
로지스틱 회귀를

9
00 : 00 : 26.175 -> 00 : 00 : 29.730
매우 효율적인 수행 할 수 있도록
모델을 구현할 수 있습니다

10
00 : 00 : 29.730 -> 00 : 00 : 32.505
기울기 계산에서 첫번째로 할 것은
첫 번째 데이터에 대해

11
00 : 00 : 32.505 -> 00 : 00 : 36.910
dz (1)을 계산 했던 것을 기억하실 겁니다.

12
00 : 00 : 36.910 -> 00 : 00 : 43.870
a (1) -y (1)입니다
다음

13
00 : 00 : 43.870 -> 00 : 00 : 52.134
dz (2) = a (2) -y (2)입니다

14
00 : 00 : 52.134 -> 00 : 00 : 56.425
이 작업을 m 개의 학습 데이터 전체에
이어갑니다

15
00 : 00 : 56.425 -> 00 : 01 : 01.218
이것 대신에
새로운 변수 dZ를 정의합니다

16
00 : 01 : 01.218 -> 00 : 01 : 08.595
dZ는 [dz (1) dz (2) dz (m)]입니다

17
00 : 01 : 08.595 -> 00 : 01 : 13.910
여기서도 dz 변수는
수평으로 그려집니다

18
00 : 01 : 13.910 -> 00 : 01 : 21.200
따라서 이 것은 1 X m의 행렬입니다
다른 방법으로는 m 개 원소의 행벡터입니다.

19
00 : 01 : 21.200 -> 00 : 01 : 23.520
이전 슬라이드 내용을 기억 해보세요

20
00 : 01 : 23.520 -> 00 : 01 : 28.405
이미 본대로 대문자 A는 
a (1)에서 a (m)를 나열해서 정의했습니다.

21
00 : 01 : 28.405 -> 00 : 01 : 36.735
대문자 Y는
y (1)에서 y (m)으로 정의합니다

22
00 : 01 : 36.735 -> 00 : 01 : 39.200
그리고 이렇게 수평으로 나열했습니다.

23
00 : 01 : 39.200 -> 00 : 01 : 42.780
이러한 정의에 따라

24
00 : 01 : 42.780 -> 00 : 01 : 46.770
이미 알 수 있겠지만

25
00 : 01 : 46.770 -> 00 : 01 : 52.750
dZ = AY 이고
이것은 a (1) -y (1)을

26
00 : 01 : 52.750 -> 00 : 01 : 55.670
첫 번째 element로

27
00 : 01 : 55.670 -> 00 : 01 : 59.980
a (2) -y (2)가
두번 째로 이어집니다.

28
00 : 01 : 59.980 -> 00 : 02 : 06.115
보시다시피 첫 번째 a (1) -y (1)은
dz (1)의 정의와 동일하고

29
00 : 02 : 06.115 -> 00 : 02 : 11.670
두 번째 a (2) -y (2)는 dz (2)의 정의와 동일 하고, 
이후도 마찬가지입니다.

30
00 : 02 : 11.670 -> 00 : 02 : 13.965
이 한 줄의 코드를 이용해서

31
00 : 02 : 13.965 -> 00 : 02 : 20.095
이러한 모든
계산을 한번에 할 수 있습니다

32
00 : 02 : 20.095 -> 00 : 02 : 24.010
우리는 이전 슬라이드에서

33
00 : 02 : 24.010 -> 00 : 02 : 27.695
for loop를
하나 제거했는데

34
00 : 02 : 27.695 -> 00 : 02 : 31.600
training 데이터에 대한
두 번째 for loop가 아직 있습니다.

35
00 : 02 : 31.600 -> 00 : 02 : 35.440
dw 을 0 으로 초기화 합니다, 
값이 0인 벡터가 되겠네요

36
00 : 02 : 35.440 -> 00 : 02 : 38.905
그러나 아직 training 데이터에 대한 
loop가 남아 있습니다.

37
00 : 02 : 38.905 -> 00 : 02 : 43.015
dw + = x (1) 곱하기 dz (1)을

38
00 : 02 : 43.015 -> 00 : 02 : 50.440
첫 번째 training 데이터로 놓고,
dw + = x (2) 곱하기 dz (2)으로 계속하고,

39
00 : 02 : 50.440 -> 00 : 02 : 56.980
이것을 m 회 반복 한 후
dm / = m를 실행합니다

40
00 : 02 : 56.980 -> 00 : 03 : 03.370
db에 대해서도 마찬가지 0으로 초기화
db + = dz (1)을

41
00 : 03 : 03.370 -> 00 : 03 : 09.120
db + = dz (2)을
dz (m)까지 계속

42
00 : 03 : 09.120 -> 00 : 03 : 16.835
db / = m로 이어집니다
이것이 이전의 구현 방법입니다

43
00 : 03 : 16.835 -> 00 : 03 : 18.700
하나의 for loop는
이미 제거했기 때문에

44
00 : 03 : 18.700 -> 00 : 03 : 25.045
dw는 벡터되어 있지만
원래는 개별적으로 dw (1) dw (2)처럼

45
00 : 03 : 25.045 -> 00 : 03 : 26.850
업데이트했습니다

46
00 : 03 : 26.850 -> 00 : 03 : 29.860
이것 반복을 제거하긴 했지만,

47
00 : 03 : 29.860 -> 00 : 03 : 33.630
training set의 m 개의 데이터에 대하여
for loop가 남아 있습니다

48
00 : 03 : 33.630 -> 00 : 03 : 36.290
여기서 이러한 명령을
벡터화 해보겠습니다.

49
00 : 03 : 36.290 -> 00 : 03 : 38.380
이렇게 하겠습니다

50
00 : 03 : 38.380 -> 00 : 03 : 42.745
db의 벡터화 구현은
기본적으로 dz를

51
00 : 03 : 42.745 -> 00 : 03 : 47.940
우선 다 더한 후에
m으로 나눠 주는 겁니다.

52
00 : 03 : 47.940 -> 00 : 03 : 51.580
db는 1 / m 시그마 i = 1부터 m까지
dz (i)

53
00 : 03 : 51.580 -> 00 : 03 : 56.530
입니다.

54
00 : 03 : 56.530 -> 00 : 04 : 03.055
모든 dz값들이 행벡터화 되어 있기 때문에 
Python에서

55
00 : 04 : 03.055 -> 00 : 04 : 04.765
이렇게 구현할 수 있습니다

56
00 : 04 : 04.765 -> 00 : 04 : 08.155
1 / m 곱하기 np.sum (dz)

57
00 : 04 : 08.155 -> 00 : 04 : 12.210
로 말입니다.

58
00 : 04 : 12.210 -> 00 : 04 : 15.115
여러분은 그저 이 변수에 대해

59
00 : 04 : 15.115 -> 00 : 04 : 19.195
np.sum 함수를 호출하면 됩니다.
db도 마찬가지로 호출 합니다.

60
00 : 04 : 19.195 -> 00 : 04 : 22.330
dw는 어떨까요?

61
00 : 04 : 22.330 -> 00 : 04 : 26.375
dw에 대한 확실한 식을 써보겠습니다.

62
00 : 04 : 26.375 -> 00 : 04 : 28.164
dw는 1 / m 곱하기

63
00 : 04 : 28.164 -> 00 : 04 : 34.485
행렬 X 곱하기 dz tranpose 입니다.

64
00 : 04 : 34.485 -> 00 : 04 : 37.990
왜 이렇게 되는건지 한번 보겠습니다.

65
00 : 04 : 37.990 -> 00 : 04 : 41.806
이것은 1 / m 곱하기 행렬 X

66
00 : 04 : 41.806 -> 00 : 04 : 48.325
행렬 X는 x (1)부터 x (m)까지
열을 쌓아 놓은 것 이구요.

67
00 : 04 : 48.325 -> 00 : 04 : 56.040
dz transpose는 dz (1)에서 dz (m)입니다

68
00 : 04 : 56.040 -> 00 : 05 : 00.900
이 행렬과 벡터를 곱하면

69
00 : 05 : 00.900 -> 00 : 05 : 05.585
1 / m 곱하기

70
00 : 05 : 05.585 -> 00 : 05 : 12.523
x (1) dz (1) + .... + x (m) dz (m)
를 만들어낼 겁니다.

71
00 : 05 : 12.523 -> 00 : 05 : 21.405
보시다시피 dw는 n × 1 벡터가 될겁니다.
(여기서 행렬X 는 n개의 행을 가진 
행렬로 가정한겁니다)

72
00 : 05 : 21.405 -> 00 : 05 : 24.725
아시다시피 dw는

73
00 : 05 : 24.725 -> 00 : 05 : 27.710
x (i) dz (i)를 합계 한 것으로

74
00 : 05 : 27.710 -> 00 : 05 : 32.300
이 행렬 벡터의 계산 내용과 동일합니다

75
00 : 05 : 32.300 -> 00 : 05 : 35.655
당신은 한 줄의 코드로
dw를 계산할 수 있습니다

76
00 : 05 : 35.655 -> 00 : 05 : 40.010
미분 계산을 벡터화를 이용해서 구현하는건
바로 이것입니다.

77
00 : 05 : 40.010 -> 00 : 05 : 44.540
이 식으로 db를 구현하고,

78
00 : 05 : 44.540 -> 00 : 05 : 50.540
이 식에서 dw를 구현합니다.
training 세트에 대한 for loop가 없이도,

79
00 : 05 : 50.540 -> 00 : 05 : 55.265
파라미터를 업데이트할 수 있게된 겁니다. 

80
00 : 05 : 55.265 -> 00 : 06 : 01.185
지금까지의 전체 내용을 모아서
로지스틱 회귀의 구현 방법을 봅시다

81
00 : 06 : 01.185 -> 00 : 06 : 02.550
이것이 원래의 코드입니다.

82
00 : 06 : 02.550 -> 00 : 06 : 07.866
상당히 비효율적이고
벡터화없이 구현되 있습니다.

83
00 : 06 : 07.866 -> 00 : 06 : 11.775
이전 비디오에서 우리는
이 for loop을 제거 했습니다.

84
00 : 06 : 11.775 -> 00 : 06 : 14.400
dw1, dw2의 loop를

85
00 : 06 : 14.400 -> 00 : 06 : 15.755
사용하는것 대신에

86
00 : 06 : 15.755 -> 00 : 06 : 23.595
이 부분을 벡터 값 dw로 대체하여
dw + = x (i) 곱하기 dz (i)

87
00 : 06 : 23.595 -> 00 : 06 : 28.775
로 표현 했습니다

88
00 : 06 : 28.775 -> 00 : 06 : 32.000
앞으로 저희가 할 것은
아래 for loop을 제거 할뿐만 아니라

89
00 : 06 : 32.000 -> 00 : 06 : 36.670
위에 있는 for loop도 제거할 겁니다.

90
00 : 06 : 36.670 -> 00 : 06 : 38.654
다음과 같이 해보도록 하죠.

91
00 : 06 : 38.654 -> 00 : 06 : 42.925
이전 슬라이드에있는 것처럼

92
00 : 06 : 42.925 -> 00 : 06 : 46.085
대문자 Z가

93
00 : 06 : 46.085 -> 00 : 06 : 57.625
Z = w transpose 곱하기 X + b 였구요,
코드는 np.dot (wT, X) + b라고 씁니다.

94
00 : 06 : 57.625 -> 00 : 07 : 07.315
A는 대문자 Z의 sigmoid 함수입니다

95
00 : 07 : 07.315 -> 00 : 07 : 12.710
이제 여러분은 이 두 식의 모든 i에 대해
계산할 수 있을겁니다.

96
00 : 07 : 12.710 -> 00 : 07 : 14.715
또한 이전 슬라이드에서

97
00 : 07 : 14.715 -> 00 : 07 : 21.070
dz = A - Y를 계산했습니다

98
00 : 07 : 21.070 -> 00 : 07 : 24.460
역시, 이 줄의 모든 i에 대한
계산을 할 수 있을겁니다.

99
00 : 07 : 24.460 -> 00 : 07 : 31.495
또한 dw는 1 / m 곱하기 X 곱하기

100
00 : 07 : 31.495 -> 00 : 07 : 39.700
dz의 transpose이고,
db = 1 / m 곱하기 np.sum (dz)가

101
00 : 07 : 39.700 -> 00 : 07 : 43.328
됩니다

102
00 : 07 : 43.328 -> 00 : 07 : 49.120
이제 여러분은 foward propagation과 
back propagation을 한 것입니다

103
00 : 07 : 49.120 -> 00 : 07 : 53.030
for loop없이
모든 훈련 데이터 m에 대해서

104
00 : 07 : 53.030 -> 00 : 07 : 57.340
예측 값과 미분 값을
계산 한 것입니다.

105
00 : 07 : 57.340 -> 00 : 08 : 00.835
경사 강하 법의 업데이트는 이렇게됩니다

106
00 : 08 : 00.835 -> 00 : 08 : 04.462
w = w - learning rate 곱하기 위에서 계산 한 dw
가 될것입니다.

107
00 : 08 : 04.462 -> 00 : 08 : 12.020
b는 b - learning rate 곱하기 db가 됩니다

108
00 : 08 : 12.020 -> 00 : 08 : 17.341
콜론을 넣는 것으로
대입 임을 나타냅니다

109
00 : 08 : 17.341 -> 00 : 08 : 21.675
하지만 저는 표기 방법에
신경을 많이 쓰지는 않습니다.

110
00 : 08 : 21.675 -> 00 : 08 : 25.450
여기까지의 작업에 의해

111
00 : 08 : 25.450 -> 00 : 08 : 29.635
로지스틱 회귀의 1 회 분의 gradient descent의 
업데이트를 구현할 수 있습니다.

112
00 : 08 : 29.635 -> 00 : 08 : 32.308
지금까지 제가
최대한 for loop를

113
00 : 08 : 32.308 -> 00 : 08 : 35.260
제거하도록 말해 왔지만

114
00 : 08 : 35.260 -> 00 : 08 : 38.230
여러 번의 gradient descent를

115
00 : 08 : 38.230 -> 00 : 08 : 42.880
실행하려면
for loop를 그 횟수만큼 반복해야합니다

116
00 : 08 : 42.880 -> 00 : 08 : 47.860
천번의 gradient descent를
실행하고 싶은 경우에는

117
00 : 08 : 47.860 -> 00 : 08 : 53.675
반복의 수 만큼의
for loop가 필요합니다

118
00 : 08 : 53.675 -> 00 : 08 : 55.870
여기처럼 바깥 쪽에 있는
for loop처럼 말이죠.

119
00 : 08 : 55.870 -> 00 : 08 : 59.210
이 for loop를 제거하는 방법은
없는 것 같습니다

120
00 : 08 : 59.210 -> 00 : 09 : 02.390
단지 for loop를 사용하지 않고

121
00 : 09 : 02.390 -> 00 : 09 : 07.117
최소 1 회분의 gradient descent를
실행 하는건 엄청나게 좋은 일이라고 생각합니다

122
00 : 09 : 07.117 -> 00 : 09 : 09.880
네, 이게 다 입니다.
이제 매우 벡터화되고

123
00 : 09 : 09.880 -> 00 : 09 : 14.745
매우 효율적인 로즈스틱 회귀의 
gradient descent를 구현할 수 있습니다

124
00 : 09 : 14.745 -> 00 : 09 : 18.850
다음 비디오에서는
하나 더 얘기해 드릴겁니다.

125
00 : 09 : 18.850 -> 00 : 09 : 24.155
여기의 설명에서 약간 암시했지만
이 방법은 broadcasting 이라고 불립니다.

126
00 : 09 : 24.155 -> 00 : 09 : 28.240
broadcasting은
Python과 numpy에서 사용하면

127
00 : 09 : 28.240 -> 00 : 09 : 32.915
코드의 일부를
훨씬 더 효율적으로 할 수있는 기술입니다

128
00 : 09 : 32.915 -> 00 : 09 : 37.090
다음 동영상에서
broadcasting의 내용을 봅시다