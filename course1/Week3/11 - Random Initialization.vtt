WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.619
신경망을 훈련 할 때

2
00 : 00 : 01.619 -> 00 : 00 : 03.955
weight를 random하게 초기화 하는 것이
중요합니다

3
00 : 00 : 03.955 -> 00 : 00 : 08.426
로지스틱 회귀는
weight를 0으로 초기화해도 괜찮지만

4
00 : 00 : 08.426 -> 00 : 00 : 12.258
신경망에서
weight를 무조건 0으로 초기화하면

5
00 : 00 : 12.258 -> 00 : 00 : 14.706
경사 하강 법을 적용해도
잘되지 않습니다

6
00 : 00 : 14.706 -> 00 : 00 : 15.289
그 이유를 살펴 보겠습니다.

7
00 : 00 : 15.289 -> 00 : 00 : 20.173
2 개의 feature 입력 값이 있습니다

8
00 : 00 : 20.173 -> 00 : 00 : 25.953
그래서 n [0] = 2 이고
두 숨겨진 레이어가 있으므로 n [1] = 2 네요

9
00 : 00 : 25.953 -> 00 : 00 : 31.547
그러자 숨겨진 레이어에 연관된 행렬 W [1]은

10
00 : 00 : 31.547 -> 00 : 00 : 35.373
2 × 2가됩니다

11
00 : 00 : 35.373 -> 00 : 00 : 41.230
모두 0으로 초기화해서
[0 0 0 0]인 2 × 2 행렬이라고 합시다.

12
00 : 00 : 41.230 -> 00 : 00 : 45.531
그리고 b[1]도 [0 0]이라 합니다

13
00 : 00 : 45.531 -> 00 : 00 : 50.788
b를 0으로 초기화 하는 것은
사실 문제 없습니다

14
00 : 00 : 50.788 -> 00 : 00 : 54.240
그러나 W를 모두 0으로 하는 것은 문제입니다

15
00 : 00 : 54.240 -> 00 : 00 : 59.625
문제가 되는 이유는 다음과 같습니다.

16
00 : 00 : 59.625 -> 00 : 01 : 05.522
모든 샘플에 대해

17
00 : 01 : 05.522 -> 00 : 01 : 09.253
a1 [1]과 a2 [1]는 같을겁니다.

18
00 : 01 : 09.253 -> 00 : 01 : 12.613
이 활성화 값과 이 활성화 값이
같다는 것입니다

19
00 : 01 : 12.613 -> 00 : 01 : 17.170
왜냐하면 숨겨진 레이어의 두 유닛은
똑같은 함수를 계산하고 있기 때문입니다

20
00 : 01 : 17.170 -> 00 : 01 : 21.810
그리고 back propagation 에서는

21
00 : 01 : 21.810 -> 00 : 01 : 24.478
dz1 [1] dz2 [1]도 동일해서

22
00 : 01 : 24.478 -> 00 : 01 : 30.165
마치 대칭 처럼 되어 버립니다.

23
00 : 01 : 30.165 -> 00 : 01 : 33.720
숨겨진 레이어의 unit 두 개가 모두
마찬가지로 초기화되는 것입니다

24
00 : 01 : 33.720 -> 00 : 01 : 36.080
여기에 출력 될 때의 weight도

25
00 : 01 : 36.080 -> 00 : 01 : 39.851
동일하다고 가정하고 있습니다

26
00 : 01 : 39.851 -> 00 : 01 : 45.122
W [2]도 [0 0]이라는 것입니다

27
00 : 01 : 45.122 -> 00 : 01 : 48.691
이와 같은 방법으로 신경망을
초기화하면

28
00 : 01 : 48.691 -> 00 : 01 : 53.590
숨겨진 레이어의 이 장치와 이 장치가
완전히 동일합니다

29
00 : 01 : 53.590 -> 00 : 01 : 57.011
완벽하게 대칭이다라고 할 수 있습니다

30
00 : 01 : 57.011 -> 00 : 02 : 01.687
똑같은 함수를 계산하고있다
라는 의미입니다

31
00 : 02 : 01.687 -> 00 : 02 : 03.765
귀납법적인 관점에서

32
00 : 02 : 03.765 -> 00 : 02 : 08.064
어떤 훈련의 반복 후에도

33
00 : 02 : 08.064 -> 00 : 02 : 11.272
두 장치는 동일한 함수를
계산하고 있다고 말할 수 있습니다

34
00 : 02 : 11.272 -> 00 : 02 : 17.521
그래서 결과적으로 dW는 이렇게

35
00 : 02 : 17.521 -> 00 : 02 : 20.681
모든 행이 동일한 값인
행렬입니다

36
00 : 02 : 20.681 -> 00 : 02 : 23.318
weight를 업데이트하면

37
00 : 02 : 23.318 -> 00 : 02 : 30.163
W [1] =  W [1] -α × dW로
업데이트됩니다

38
00 : 02 : 30.163 -> 00 : 02 : 33.740
그러면 모든 반복 후에

39
00 : 02 : 33.740 -> 00 : 02 : 37.616
첫째 줄이 둘째 줄과 같은 것을
알 수 있습니다

40
00 : 02 : 37.616 -> 00 : 02 : 41.487
귀납법에서 다음과 같은 것을 알 수 있습니다

41
00 : 02 : 41.487 -> 00 : 02 : 44.688
W의 값을 모두 0으로 초기화하면

42
00 : 02 : 44.688 -> 00 : 02 : 49.164
숨겨진 레이어의 두 unit이
같은 함수의 계산에서 시작되므로

43
00 : 02 : 49.164 -> 00 : 02 : 53.541
그리고 두 unit 모두가 출력 장치에
같은 영향을 주기 때문에

44
00 : 02 : 53.541 -> 00 : 02 : 57.542
1 회 반복 후에도

45
00 : 02 : 57.542 -> 00 : 03 : 00.273
두 숨겨진 유닛의 대칭성은 남아있습니다.

46
00 : 03 : 00.273 -> 00 : 03 : 04.507
그래서 귀납법을 이용하면
2 번째 3 번째 반복 후

47
00 : 03 : 04.507 -> 00 : 03 : 07.013
아무리 많이 신경망을
훈련 하더라도

48
00 : 03 : 07.013 -> 00 : 03 : 10.373
두 Hidden 유닛은 똑같은 함수로
계산을 계속 하고 있는 것을 알 수 있습니다

49
00 : 03 : 10.373 -> 00 : 03 : 15.212
이 경우 숨겨진 유닛을
두개 이상 가져갈 의미가 없어져 버립니다

50
00 : 03 : 15.212 -> 00 : 03 : 17.692
모두가 같은 것을 계산하고 있기 때문입니다.

51
00 : 03 : 17.692 -> 00 : 03 : 22.378
물론 더 큰 신경망에서

52
00 : 03 : 22.378 -> 00 : 03 : 24.972
3 개의 feature가 있고,
많은 숨겨진 유닛이 있다고 해도,

53
00 : 03 : 24.972 -> 00 : 03 : 29.239
이 신경망과 같은 결과가 나올 것입니다.

54
00 : 03 : 29.239 -> 00 : 03 : 34.107
모든 weight를 0으로 초기화하면

55
00 : 03 : 34.107 -> 00 : 03 : 37.103
모든 숨겨진 유닛이
대칭이 되어 버리는 것입니다

56
00 : 03 : 37.103 -> 00 : 03 : 40.603
그리고 아무리 많이 경사 하강 법을 실행해도

57
00 : 03 : 40.603 -> 00 : 03 : 44.037
똑같은 함수를 계산 하게 됩니다.

58
00 : 03 : 44.037 -> 00 : 03 : 48.785
각각의 숨겨진 유닛들이
다른 함수로 계산하는게 좋기 때문에

59
00 : 03 : 48.785 -> 00 : 03 : 52.835
이것은 바람직하지는 않습니다.

60
00 : 03 : 52.835 -> 00 : 03 : 57.748
해결 방법은 매개 변수를 임의로
초기화하는 것입니다

61
00 : 03 : 57.748 -> 00 : 03 : 58.677
실제로는 다음과 같은 것을합니다

62
00 : 03 : 58.677 -> 00 : 04 : 04.053
W1 = np.random.randn 이라고
설정합니다

63
00 : 04 : 04.053 -> 00 : 04 : 07.037
이제 (2,2)크기의 가우스 분포의 
랜덤한 행렬이 생성됩니다

64
00 : 04 : 07.037 -> 00 : 04 : 12.358
그리고 대개는 매우 작은 숫자
0.01 등을 곱합니다

65
00 : 04 : 12.358 -> 00 : 04 : 14.951
즉 아주 작은 값의 랜덤한 행렬로
초기화 하는 것입니다

66
00 : 04 : 14.951 -> 00 : 04 : 20.590
b는 대칭이 되어도
문제가 없습니다.

67
00 : 04 : 20.590 -> 00 : 04 : 24.735
같은 의미로 대칭성 파괴의 
문제가 없습니다.

68
00 : 04 : 24.735 -> 00 : 04 : 29.370
따라서 b는 그냥 0으로 초기화해도
괜찮을 겁니다.

69
00 : 04 : 29.370 -> 00 : 04 : 32.166
W가 무작위로 초기화되는 한

70
00 : 04 : 32.166 -> 00 : 04 : 36.769
각각의 Hidden 유닛이 다른 계산을 
하고 있는 상태에서 시작되기 때문입니다

71
00 : 04 : 36.769 -> 00 : 04 : 40.912
그래서 다른 대칭성의 파괴의 문제에
고생하지 않습니다

72
00 : 04 : 40.912 -> 00 : 04 : 43.795
마찬가지로 W [2]도 무작위로 초기화합시다

73
00 : 04 : 43.795 -> 00 : 04 : 48.858
b [2]는 0으로 초기화 될 수 있습니다

74
00 : 04 : 48.858 -> 00 : 04 : 55.321
그런데 이 상수는 어디서 왔고 왜 0.01인가
라고 생각하실지도 모릅니다.

75
00 : 04 : 55.321 -> 00 : 04 : 58.478
왜 100이나 1000은 안되는 것일까?

76
00 : 04 : 58.478 -> 00 : 05 : 02.313
그것은 보통 weight는
아주 작은 임의의 값으로

77
00 : 05 : 02.313 -> 00 : 05 : 05.763
초기화 하는 것이 선호되기 때문입니다

78
00 : 05 : 05.763 -> 00 : 05 : 10.443
만약 tanh와 sigmoid 함수 나
다른 sigmoid를

79
00 : 05 : 10.443 -> 00 : 05 : 14.047
설령 그것을 출력층에만 
적용한다고 하더라도

80
00 : 05 : 14.047 -> 00 : 05 : 17.922
만약 weight가 너무 클 경우에

81
00 : 05 : 17.922 -> 00 : 05 : 23.967
활성화의 값을 계산했을 때

82
00 : 05 : 23.967 -> 00 : 05 : 28.621
z [1] = W1x + b 이고,

83
00 : 05 : 28.621 -> 00 : 05 : 34.094
a [1]은 z [1]이 대입된 
활성화 함수라는걸 기억합시다.

84
00 : 05 : 34.094 -> 00 : 05 : 39.097
그래서 만약 w가 너무 크면

85
00 : 05 : 39.097 -> 00 : 05 : 44.235
z값이 너무 크거나 
작은 값이 되버립니다.

86
00 : 05 : 44.235 -> 00 : 05 : 49.789
그렇게되면 tanh와 sigmoid 함수의
평평한 부분에 와버려서

87
00 : 05 : 49.789 -> 00 : 05 : 55.699
경사가 작아지기 때문에

88
00 : 05 : 55.699 -> 00 : 05 : 58.302
경사 하강 법의 연산 속도가 늦어져 버립니다

89
00 : 05 : 58.302 -> 00 : 05 : 59.730
즉 학습 속도가 느려질 것입니다

90
00 : 05 : 59.730 -> 00 : 06 : 04.133
복습해보면, w가 너무 크면

91
00 : 06 : 04.133 -> 00 : 06 : 08.633
훈련의 처음에도
z가 매우 커지므로

92
00 : 06 : 08.633 -> 00 : 06 : 13.525
tanh와 sigmoid 활성화 함수가
포화 된것 같은 상태가 되기 때문에

93
00 : 06 : 13.525 -> 00 : 06 : 15.418
학습이 늦어 져 버립니다

94
00 : 06 : 15.418 -> 00 : 06 : 17.231
신경망에서 sigmoid와 tanh 활성화 함수를


95
00 : 06 : 17.231 -> 00 : 06 : 22.149
전혀 사용하지 않는 경우
그다지 문제가 없지만

96
00 : 06 : 22.149 -> 00 : 06 : 26.506
binary classification을 한다면
출력 장치는 sigmoid 함수이므로

97
00 : 06 : 26.506 -> 00 : 06 : 30.806
매개 변수의 초기 상태는
크지 않은 것이 좋습니다

98
00 : 06 : 30.806 -> 00 : 06 : 35.435
이런 이유로 0.01 나
다른 작은 숫자를

99
00 : 06 : 35.435 -> 00 : 06 : 36.872
곱하는 것이
합리적 이라고 할 수 있습니다

100
00 : 06 : 36.872 -> 00 : 06 : 38.536
그리고 W [2]에 대해서도 같습니다.

101
00 : 06 : 38.536 -> 00 : 06 : 44.295
이것이 random.randn이 되고

102
00 : 06 : 44.295 -> 00 : 06 : 49.545
이 예에서는 1 × 2 행렬이네요
그것에 0.01을 곱합니다.

103
00 : 06 : 49.545 -> 00 : 06 : 51.404
여기에 's'를 잊었습니다

104
00 : 06 : 51.404 -> 00 : 07 : 00.085
마지막으로 0.01보다 좋은 상수가
있을 수 있습니다

105
00 : 07 : 00.085 -> 00 : 07 : 04.304
숨겨진 레이어가 하나 만 있는
신경망을 훈련 할 때

106
00 : 07 : 04.304 -> 00 : 07 : 09.129
숨겨진 레이어가 많지 않은 비교적 얕은
신경망이므로

107
00 : 07 : 09.129 -> 00 : 07 : 12.392
0.01로 설정해도 괜찮을 겁니다.

108
00 : 07 : 12.392 -> 00 : 07 : 15.705
그러나 더 깊은 신경망을
훈련하는 경우

109
00 : 07 : 15.705 -> 00 : 07 : 19.294
0.01이 아닌 상수를 선택하고 
싶을 수 도 있습니다.

110
00 : 07 : 19.294 -> 00 : 07 : 23.642
다음 주에는 0.01이 아닌 수를
언제, 어떻게 적용하는지에

111
00 : 07 : 23.642 -> 00 : 07 : 27.925
대해서 이야기하고 싶습니다

112
00 : 07 : 27.925 -> 00 : 07 : 32.008
어느 쪽이든 어느 정도
작은 값이 됩니다

113
00 : 07 : 32.008 -> 00 : 07 : 34.584
이번 주의 동영상은 여기까지 입니다.

114
00 : 07 : 34.584 -> 00 : 07 : 38.348
숨겨진 레이어가 하나인
신경망을 만드는 방법이나

115
00 : 07 : 38.348 -> 00 : 07 : 42.430
파라미터의 초기화 방법
Forward prop을 사용한 예측 방법

116
00 : 07 : 42.430 -> 00 : 07 : 45.445
그리고 back prop을 사용해서
미분을 구하는 방법이나

117
00 : 07 : 45.445 -> 00 : 07 : 46.275
경사 하강 법의 구현 방법에 대해
알게 되었습니다

118
00 : 07 : 46.275 -> 00 : 07 : 48.654
그래서 이번 주 퀴즈와

119
00 : 07 : 48.654 -> 00 : 07 : 51.166
프로그래밍 과제를 할 수 있게 되었다고 생각합니다

120
00 : 07 : 51.166 -> 00 : 07 : 52.143
행운을 빌겠습니다.

121
00 : 07 : 52.143 -> 00 : 07 : 54.802
프로그래밍 과제를 즐겨 줄 수 있으면
좋겠군요 ㅎㅎ

122
00 : 07 : 54.802 -> 00 : 07 : 57.728
그럼 네 번째 주에서 만나기를
기대하고 있습니다