WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 03.116
이전 비디오에서 우리는 신경망의 그림을
여러 번 그렸습니다

2
00 : 00 : 03.116 -> 00 : 00 : 05.712
이 비디오에서는
이들이 도대체 무엇을 의미 하는가를 이야기합니다

3
00 : 00 : 05.712 -> 00 : 00 : 06.728
다시 말하면,

4
00 : 00 : 06.728 -> 00 : 00 : 11.235
전에 그렸던 신경망이
무엇을 표현하고 있는지 입니다

5
00 : 00 : 11.235 -> 00 : 00 : 15.014
우선 single hidden layer라고 불리는
신경망의 부분에

6
00 : 00 : 15.014 -> 00 : 00 : 17.290
초점을 맞추고
시작하고 싶습니다

7
00 : 00 : 17.290 -> 00 : 00 : 19.667
신경망의 그림입니다

8
00 : 00 : 19.667 -> 00 : 00 : 22.986
각각의 부분에
이름을 지정합니다

9
00 : 00 : 22.986 -> 00 : 00 : 27.447
세로로 쌓여 있는 x1, x2, x3가 있습니다.

10
00 : 00 : 27.447 -> 00 : 00 : 30.694
이것을 input layer라고 부릅니다.

11
00 : 00 : 30.694 -> 00 : 00 : 35.764
당연하겠지만 이 input layer에는
신경망의 입력값이 포함 되어 있습니다

12
00 : 00 : 35.764 -> 00 : 00 : 37.990
이 원형의 레이어가 있습니다

13
00 : 00 : 37.990 -> 00 : 00 : 41.663
이것은 신경망의 Hidden layer라고 부릅니다

14
00 : 00 : 41.663 -> 00 : 00 : 45.414
왜 숨겨져 있다고 하는지는 곧 설명해 드릴겁니다.

15
00 : 00 : 45.414 -> 00 : 00 : 49.509
이 하나의 노드에서 형성되는 마지막 층은

16
00 : 00 : 49.509 -> 00 : 00 : 53.894
Output layer라고 하고,

17
00 : 00 : 53.894 -> 00 : 00 : 56.059
예측 값인 y hat을 생성합니다

18
00 : 00 : 56.059 -> 00 : 00 : 59.932
Supervised learning에서 학습하는 신경망은

19
00 : 00 : 59.932 -> 00 : 01 : 05.237
training set의
입력 값 x와 목표 출력 값 y가 포함되어 있습니다

20
00 : 01 : 05.237 -> 00 : 01 : 09.239
Hidden layer가 의미하는 것은
training set 내에서

21
00 : 01 : 09.239 -> 00 : 01 : 12.702
이 중심부 노드의 진정한 역할
관측 되고 있지 않습니다

22
00 : 01 : 12.702 -> 00 : 01 : 15.185
즉 training set에서
어떤 값이 들어갈 지 알 수 없습니다

23
00 : 01 : 15.185 -> 00 : 01 : 16.640
입력 값도 눈에 보이고

24
00 : 01 : 16.640 -> 00 : 01 : 18.094
출력 값도 눈에 보이지만,

25
00 : 01 : 18.094 -> 00 : 01 : 20.992
training set 내에서
Hidden layer의 것은 모릅니다

26
00 : 01 : 20.992 -> 00 : 01 : 25.542
training set 내에서
보이지 않기 때문에 숨겨진 레이어라는

27
00 : 01 : 25.542 -> 00 : 01 : 28.088
이름으로 불리는것 같습니다.

28
00 : 01 : 28.088 -> 00 : 01 : 30.262
표기 방법을 소개하겠습니다

29
00 : 01 : 30.262 -> 00 : 01 : 35.542
벡터 x로 입력 feature를
표현 했었지만

30
00 : 01 : 35.542 -> 00 : 01 : 37.226
다른 방법으로는

31
00 : 01 : 37.226 -> 00 : 01 : 41.987
입력 feature 값은
a [0]으로 표기합니다

32
00 : 01 : 41.987 -> 00 : 01 : 44.934
a는 activation의 약자 입니다.

33
00 : 01 : 44.934 -> 00 : 01 : 47.733
그리고 이것은 신경망

34
00 : 01 : 47.733 -> 00 : 01 : 51.651
각 계층이 다음 계층에 값을 전달하는 것을
나타냅니다

35
00 : 01 : 51.651 -> 00 : 01 : 55.998
입력 층은 x 값을
숨겨진 레이어에 전달합니다

36
00 : 01 : 55.998 -> 00 : 02 : 01.110
따라서 입력 층의 activation을
a (0)라고 하겠습니다.

37
00 : 02 : 01.110 -> 00 : 02 : 05.990
다음 층의 숨겨진 레이어에서
이에 따라 활성화 값을 생성합니다

38
00 : 02 : 05.990 -> 00 : 02 : 09.601
이것을 a [1]라고 씁니다

39
00 : 02 : 09.601 -> 00 : 02 : 13.306
이 첫 번째 장치 노드는

40
00 : 02 : 13.306 -> 00 : 02 : 17.824
a [1] 1의 값을 생성합니다

41
00 : 02 : 17.824 -> 00 : 02 : 20.735
두 번째 노드는

42
00 : 02 : 20.735 -> 00 : 02 : 23.311
a [1] 2 값을 생성하고
계속될 겁니다

43
00 : 02 : 23.311 -> 00 : 02 : 26.488
a [1]가 의미하는 것은

44
00 : 02 : 26.488 -> 00 : 02 : 30.120
4 차원 벡터이고

45
00 : 02 : 30.120 -> 00 : 02 : 34.707
Python에서는 4x1 열 벡터와 같은 의미 입니다.

46
00 : 02 : 34.707 -> 00 : 02 : 39.205
4 차원인 이유는
4 개의 숨겨진 노드 장치가

47
00 : 02 : 39.205 -> 00 : 02 : 42.684
숨겨진 레이어에 있기 때문입니다

48
00 : 02 : 42.684 -> 00 : 02 : 46.302
그리고 마지막으로
출력 계층이 값을 생성합니다

49
00 : 02 : 46.302 -> 00 : 02 : 47.948
a [2]인 실수(Real numeber)값 입니다

50
00 : 02 : 47.948 -> 00 : 02 : 51.658
y hat은 a [2]의 값을 그대로 취합니다

51
00 : 02 : 51.658 -> 00 : 02 : 55.885
로지스틱 회귀에서
y hat = a 라고 쓰는 이유입니다

52
00 : 02 : 55.885 -> 00 : 03 : 00.349
로지스틱 회귀 에서는 출력 층이 하나 밖에
없기 때문에

53
00 : 03 : 00.349 -> 00 : 03 : 03.583
[]으로 표기하지 않은 겁니다.

54
00 : 03 : 03.583 -> 00 : 03 : 07.916
신경망의 어느 층에서 왔는지

55
00 : 03 : 07.916 -> 00 : 03 : 11.653
명시 적으로 표현하기 위해
[]를 사용하는 것입니다.

56
00 : 03 : 11.653 -> 00 : 03 : 15.468
표기 방법에 있어서
재밌는 점은

57
00 : 03 : 15.468 -> 00 : 03 : 20.194
지금 보고 있는 네트워크를
2 layer network 라고합니다

58
00 : 03 : 20.194 -> 00 : 03 : 23.541
그 이유는 신경망의 층을 셀 때

59
00 : 03 : 23.541 -> 00 : 03 : 25.321
입력layer는 층 수에 넣지 않기 때문입니다

60
00 : 03 : 25.321 -> 00 : 03 : 28.858
즉 숨겨진 레이어가 첫 번째 레이어가 되고
출력 층은 2번째 레이어가 됩니다.

61
00 : 03 : 28.858 -> 00 : 03 : 32.661
이 표기 방법에서는
입력 층을 0 번째 레이어라고합니다

62
00 : 03 : 32.661 -> 00 : 03 : 35.887
엄밀하게는 이 신경망은
3층 이라고 봐야 할지도 모릅니다

63
00 : 03 : 35.887 -> 00 : 03 : 39.649
입력 층, 숨겨진 레이어 출력 층 3개가
있기 때문입니다

64
00 : 03 : 39.649 -> 00 : 03 : 43.357
연구 논문 및 기타 자료를 읽을 때도 그렇지만
이 과정에서는 관습적으로

65
00 : 03 : 43.357 -> 00 : 03 : 47.489
이 예제와 같은 신경망은
2층 신경망 이라고합니다.

66
00 : 03 : 47.489 -> 00 : 03 : 51.602
공식적으로는 입력 층은
숫자에 넣지 않기 때문입니다

67
00 : 03 : 51.602 -> 00 : 03 : 55.912
마지막으로 나중에 나오는 것은

68
00 : 03 : 55.912 -> 00 : 03 : 59.670
숨겨진 레이어 및 출력 계층에
관련된 매개 변수가 있습니다

69
00 : 03 : 59.670 -> 00 : 04 : 03.447
숨겨진 레이어는 w, b라는 변수가
연결되어 있습니다

70
00 : 04 : 03.447 -> 00 : 04 : 08.218
여기에는 [1]이라고 써서 이 매개 변수가

71
00 : 04 : 08.218 -> 00 : 04 : 12.395
숨겨진 레이어의 첫 번째 레이어와 
연관이 있음을 나타냅니다

72
00 : 04 : 12.395 -> 00 : 04 : 15.416
나중에 보겠지만
이 경우 w는 4x3의 행렬이고

73
00 : 04 : 15.416 -> 00 : 04 : 18.016
b는 4x1 벡터 입니다. 

74
00 : 04 : 18.016 -> 00 : 04 : 21.754
첫 번째 4는
숨겨진 레이어에 4 개의 노드가

75
00 : 04 : 21.754 -> 00 : 04 : 24.503
존재하기 때문이고

76
00 : 04 : 24.503 -> 00 : 04 : 28.120
3은 3 개의 입력 feature가
있는 것에서 나온겁니다.

77
00 : 04 : 28.120 -> 00 : 04 : 31.980
나중에 이 행렬의 차원에 대해서 좀 더
자세하게 얘기 하겠습니다.

78
00 : 04 : 31.980 -> 00 : 04 : 33.844
그때가 되면 더 이해할 수 
있을지도 모릅니다

79
00 : 04 : 33.844 -> 00 : 04 : 37.813
그러나 출력 계층도 또한 연관된 매개 변수가 있습니다

80
00 : 04 : 37.813 -> 00 : 04 : 41.663
매개 변수 w [2], b [2] 입니다.

81
00 : 04 : 41.663 -> 00 : 04 : 45.747
이들의 배열은 1x4와 1x1입니다

82
00 : 04 : 45.747 -> 00 : 04 : 49.297
1x4 라는 것은
숨겨진 레이어에 4 개의 유닛이 있고

83
00 : 04 : 49.297 -> 00 : 04 : 51.177
출력 층이 1 단위 밖에 없기 때문입니다

84
00 : 04 : 51.177 -> 00 : 04 : 56.378
다시 말하지만 이러한 행렬과 벡터의 배열을
향후 비디오에서 살펴 보겠습니다

85
00 : 04 : 56.378 -> 00 : 04 : 59.839
우리는 이곳 에서 2층의 신경망이
어떤 것인지 보고 왔습니다

86
00 : 04 : 59.839 -> 00 : 05 : 03.108
하나의 숨겨진 레이어가 있는 신경망 이었습니다.

87
00 : 05 : 03.108 -> 00 : 05 : 04.260
다음 비디오 에서는

88
00 : 05 : 04.260 -> 00 : 05 : 08.513
이 신경망이
도대체 무엇을 계산 하고 있는지 살펴 보겠습니다.

89
00 : 05 : 08.513 -> 00 : 05 : 11.223
신경망이 x의 입력 값을 이용해서

90
00 : 05 : 11.223 -> 00 : 05 : 14.169
y hat을 계산하는 데까지
어떻게 도착하는지 봅시다