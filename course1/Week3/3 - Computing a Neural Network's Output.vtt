WEBVTT

1
00 : 00 : 00.006 -> 00 : 00 : 04.743
이전 동영상에서 한개의 숨겨진 레이어가 있는
신경망이 무엇인지 보았습니다.

2
00 : 00 : 04.743 -> 00 : 00 : 08.175
이번에는 실제로 그 신경망이

3
00 : 00 : 08.175 -> 00 : 00 : 10.361
어떻게 출력을 계산하고 있는지
자세히 살펴 봅시다.

4
00 : 00 : 10.361 -> 00 : 00 : 15.533
로지스틱 회귀와 비슷한 것이
여러 번 반복되고 있다는 느낌 입니다

5
00 : 00 : 15.533 -> 00 : 00 : 16.423
다음의 예를 살펴 보자.

6
00 : 00 : 16.423 -> 00 : 00 : 19.364
이건 2개의 계층을 가진 신경망입니다

7
00 : 00 : 19.364 -> 00 : 00 : 23.973
이것이 정확히 무엇을 계산할 것인지
자세히 배워 봅시다.

8
00 : 00 : 23.973 -> 00 : 00 : 26.653
이전에 로지스틱 회귀의 원형은

9
00 : 00 : 26.653 -> 00 : 00 : 31.035
두 계산 단계를 나타내고 있다고
이야기했습니다

10
00 : 00 : 31.035 -> 00 : 00 : 34.498
우선 이렇게 z를 계산하고

11
00 : 00 : 34.498 -> 00 : 00 : 37.754
다음 z의 sigmoid 함수인
activation을 계산합니다

12
00 : 00 : 37.754 -> 00 : 00 : 40.536
신경망 에서는
이것을 여러 번 반복할 뿐입니다

13
00 : 00 : 40.536 -> 00 : 00 : 43.953
숨겨진 레이어 안의 노드 하나만
자세히 보겠습니다.

14
00 : 00 : 43.953 -> 00 : 00 : 46.320
숨겨진 레이어의 첫 번째 노드를 보십시오.

15
00 : 00 : 46.320 -> 00 : 00 : 48.079
우선 다른 노드는
흐리게 표시 해 둡니다.

16
00 : 00 : 48.079 -> 00 : 00 : 50.820
왼쪽의 로지스틱 회귀처럼

17
00 : 00 : 50.820 -> 00 : 00 : 54.391
이 숨겨진 레이어의 노드도
두 계산의 단계를 따를겁니다.

18
00 : 00 : 54.391 -> 00 : 00 : 58.418
첫 번째 단계가
이 왼쪽 절반에 해당한다고 생각 해보면

19
00 : 00 : 58.418 -> 00 : 01 : 02.754
이 부분은 z = wT 곱하기 x + b를 계산합니다

20
00 : 01 : 02.754 -> 00 : 01 : 08.253
표기법 내용이지만 이 값은
모두 첫 번째의 숨겨진 레이어에 대한 것이므로

21
00 : 01 : 08.253 -> 00 : 01 : 13.458
이렇게 대괄호를 씁니다

22
00 : 01 : 13.458 -> 00 : 01 : 16.597
그리고 숨겨진 레이어의 첫 번째 노드이므로
아래 첨자 1을 씁니다

23
00 : 01 : 16.597 -> 00 : 01 : 18.424
첫째는 이 계산을 수행하고

24
00 : 01 : 18.424 -> 00 : 01 : 24.419
두 번째 단계에서는
a [1] 1 = σ (z [1] 1)을 계산합니다

25
00 : 01 : 24.419 -> 00 : 01 : 29.013
z와 a 모두에 대해
표기법의 관례로는

26
00 : 01 : 29.013 -> 00 : 01 : 33.770
a에 [l]와 i를 붙이면 대괄호로 작성된
l는 레이어의 번호를 나타내고

27
00 : 01 : 33.770 -> 00 : 01 : 37.720
아래에 적힌 i는 그 레이어 에서의
노드의 번호를 나타냅니다

28
00 : 01 : 37.720 -> 00 : 01 : 42.344
지금 보고 있는 노드는 첫번째 층
즉 Hidden layer 층에 있고

29
00 : 01 : 42.344 -> 00 : 01 : 45.878
그 중 첫 번째 노드이므로
두 숫자가 모두 1입니다

30
00 : 01 : 45.878 -> 00 : 01 : 49.965
이 신경망의
첫 번째 노드 인 작은 원은

31
00 : 01 : 49.965 -> 00 : 01 : 52.579
이 두 단계의 계산을
할 것을 나타냅니다

32
00 : 01 : 52.579 -> 00 : 01 : 58.399
다음 숨겨진 레이어의 두 번째 노드를
보도록 합시다.

33
00 : 01 : 58.399 -> 00 : 02 : 01.482
이 역시 왼쪽의 로지스틱 회귀에 따라서

34
00 : 02 : 01.482 -> 00 : 02 : 04.781
이 작은 원은
두개의 계산 단계를 나타냅니다

35
00 : 02 : 04.781 -> 00 : 02 : 08.733
우선 z를 계산합니다
아직 층은 첫 번째 이고

36
00 : 02 : 08.733 -> 00 : 02 : 12.996
이번에는 두 번째 노드 네요
이것이 w [1] 2 T 곱하기 x + b [1] 2입니다

37
00 : 02 : 12.996 -> 00 : 02 : 17.880
다음 a [1] 2 = σ (z [1] 2)입니다

38
00 : 02 : 17.880 -> 00 : 02 : 23.071
필요하다면 동영상을 세워두고

39
00 : 02 : 23.071 -> 00 : 02 : 28.453
여기에 보라색으로 쓴 표기법이
잘 맞는지 확인 해보세요

40
00 : 02 : 28.453 -> 00 : 02 : 32.831
지금까지 신경망에서
숨겨진 층에 있는 처음 두개의 노드를 보고 왔습니다

41
00 : 02 : 32.831 -> 00 : 02 : 36.940
3,4 번째 숨겨진 노드도 비슷하게 계산 됩니다.

42
00 : 02 : 36.940 -> 00 : 02 : 39.778
저는 이 두 등식을

43
00 : 02 : 39.778 -> 00 : 02 : 44.169
다음 슬라이드에 복사 하겠습니다.

44
00 : 02 : 44.169 -> 00 : 02 : 48.921
이것이 신경망이고,
이것이 첫 번째 그리고 이것이 두 번째 식입니다

45
00 : 02 : 48.921 -> 00 : 02 : 54.050
첫 번째와 두 번째의 숨겨진 레이어에 대해
아까 쓴 식 입니다.

46
00 : 02 : 54.050 -> 00 : 02 : 59.022
더 나아가 세 번째와 네 번째 숨겨진 레이어에
해당 표현식을 쓰면

47
00 : 02 : 59.022 -> 00 : 03 : 02.093
이렇게 됩니다

48
00 : 03 : 02.093 -> 00 : 03 : 06.550
표기법에 대해 다시 확인하면
이것은 벡터 w [1] 1 을

49
00 : 03 : 06.550 -> 00 : 03 : 09.430
transpose 한 것에 x를 곱하고 있습니다
맞죠?

50
00 : 03 : 09.430 -> 00 : 03 : 13.460
여기에 위 첨자 T는
벡터의 transpose를 나타내고있는 것입니다

51
00 : 03 : 13.460 -> 00 : 03 : 17.585
여러분도 생각 하시겠지만,
실제로 신경망을 구현할 때

52
00 : 03 : 17.585 -> 00 : 03 : 20.209
이 구문들을 for 루프로 사용하는 것은
매우 비효율적입니다

53
00 : 03 : 20.209 -> 00 : 03 : 25.174
그래서 이 네 개의 식을 벡터화 하려고 합니다.

54
00 : 03 : 25.174 -> 00 : 03 : 29.348
우선 z를 벡터로 계산하면
어떻게 될까를 생각합시다

55
00 : 03 : 29.348 -> 00 : 03 : 30.859
이것은 다음과 같이 할 수 있습니다

56
00 : 03 : 30.859 -> 00 : 03 : 34.843
먼저 이러한 w들을 행렬에 쌓겠습니다.

57
00 : 03 : 34.843 -> 00 : 03 : 38.767
그러자 w [1] 1의 transpose 니까
행벡터 들이 될겁니다.

58
00 : 03 : 38.767 -> 00 : 03 : 42.231
원래 열벡터 이었던 w를
전치했기 때문에 행벡터가 되는 것입니다

59
00 : 03 : 42.231 -> 00 : 03 : 48.494
그리고 w [1] 2 전치 w [1] 3 전치
w [1] 4 전치 계속 쌓습니다.

60
00 : 03 : 48.494 -> 00 : 03 : 54.499
이러한 네 개의 벡터를 쌓는 것으로
이러한 행렬을 얻을 수 있습니다

61
00 : 03 : 54.499 -> 00 : 03 : 59.204
또 다른 아이디어는
로지스틱 회귀 단위가 네 개있어

62
00 : 03 : 59.204 -> 00 : 04 : 03.913
각각의 유닛이
해당 매개 변수 w1,w2,w3의 벡터를 가지고

63
00 : 04 : 03.913 -> 00 : 04 : 06.535
그 네 개의 벡터를 정리하여

64
00 : 04 : 06.535 -> 00 : 04 : 08.842
4 × 3 행렬이 완성된다
라고 생각할 수도 있습니다

65
00 : 04 : 08.842 -> 00 : 04 : 14.281
이 행렬에 입력된 feature들
x1, x2, x3을 곱해서

66
00 : 04 : 14.281 -> 00 : 04 : 19.806
행렬 곱셈의 방식으로 계산하면

67
00 : 04 : 19.806 -> 00 : 04 : 24.546
w [1] 1T x, w [1] 2T x,
w [1] 3T x, w [1] 4T x 가 될겁니다

68
00 : 04 : 24.546 -> 00 : 04 : 30.995
b도 잊어서는 안 되겠네요

69
00 : 04 : 30.995 -> 00 : 04 : 35.997
그래서 여기에 벡터 b [1] 1, b [1] 2

70
00 : 04 : 35.997 -> 00 : 04 : 40.811
b [1] 3, b [1] 4를 더합니다
이 부분에 해당 하는군요

71
00 : 04 : 40.811 -> 00 : 04 : 45.654
그리고 이쪽도 b [1] 1, b [1] 2
b [1] 3, b [1] 4입니다

72
00 : 04 : 45.654 -> 00 : 04 : 50.579
여기서 얻을 수있는 네 줄의 식은

73
00 : 04 : 50.579 -> 00 : 04 : 55.772
위에서 계산 한 네 개의 값에
정확하게 일치 하고 있는 것을 알 수 있습니다

74
00 : 04 : 55.772 -> 00 : 05 : 00.899
바로 이것이 z [1] 1이되고

75
00 : 05 : 00.899 -> 00 : 05 : 05.303
z [1] 2, z [1] 3, z [1] 4 될 것입니다
여기에서 정의 된대로군요

76
00 : 05 : 05.303 -> 00 : 05 : 10.289
그리고 당연하게 보일지 모르지만
이 전체를 z [1]라고 부르기로 합니다

77
00 : 05 : 10.289 -> 00 : 05 : 15.097
개별 z를 쌓아올린
열이기 때문이죠.

78
00 : 05 : 15.097 -> 00 : 05 : 19.524
벡터화 할 때의 규칙 하나는,

79
00 : 05 : 19.524 -> 00 : 05 : 23.966
하나의 레이어에 다른 노드가있을 때
세로로 나열하는 것입니다

80
00 : 05 : 23.966 -> 00 : 05 : 27.656
따라서 z [1] 1 부터 z [1] 4까지가 있고

81
00 : 05 : 27.656 -> 00 : 05 : 31.852
그들이 숨겨진 레이어의 네 가지 노드에
일치 하는 경우

82
00 : 05 : 31.852 -> 00 : 05 : 36.481
그들을 수직으로 세워서
벡터 z [1]을 만든겁니다.

83
00 : 05 : 36.481 -> 00 : 05 : 40.457
또 다른 표기법을 사용해 봅니다

84
00 : 05 : 40.457 -> 00 : 05 : 45.233
아까 소문자 w [1] 1, w [1] 2 등을
쌓아서 만든 이 행렬은 4 × 3 행렬입니다

85
00 : 05 : 45.233 -> 00 : 05 : 49.860
이를 대문자 W [1]라고 부르기로합니다

86
00 : 05 : 49.860 -> 00 : 05 : 54.623
마찬가지로 이것은 b [1]이라고합니다
4 × 1 벡터 네요

87
00 : 05 : 54.623 -> 00 : 05 : 59.584
이제 이 벡터 나 행렬 표기법을 사용하여
z를 계산할 수있었습니다

88
00 : 05 : 59.584 -> 00 : 06 : 03.535
마지막으로 하고 싶은 것은
이 a의 값도 계산하는 것입니다

89
00 : 06 : 03.535 -> 00 : 06 : 08.195
당연히 a [1]은
a [1] 1에서 a [1] 4 까지가

90
00 : 06 : 08.195 -> 00 : 06 : 13.019
수직으로 쌓여있는 것으로 정의합니다

91
00 : 06 : 13.019 -> 00 : 06 : 18.202
이 네 개의 값을 가지고 와서
a [1]라는 벡터에 담은것 뿐입니다

92
00 : 06 : 18.202 -> 00 : 06 : 21.122
그리고 이것은 σ (z [1])입니다

93
00 : 06 : 21.122 -> 00 : 06 : 25.794
이 sigmoid 함수는
네 개의 z 요소를 가져와서

94
00 : 06 : 25.794 -> 00 : 06 : 30.761
각 요소마다 sigmoid 함수를
적용하는 것입니다

95
00 : 06 : 30.761 -> 00 : 06 : 36.750
복습한번 해보죠, z [1] = W [1] x + b [1]

96
00 : 06 : 36.750 -> 00 : 06 : 41.883
a [1] = σ (z [1])임을 알 수 있었습니다

97
00 : 06 : 41.883 -> 00 : 06 : 47.321
다음 슬라이드로 이들을 복사합니다

98
00 : 06 : 47.321 -> 00 : 06 : 52.156
입력값 x가 있을 때
신경망의 첫번째 층을 보겠습니다.

99
00 : 06 : 52.156 -> 00 : 06 : 56.286
z [1] = W [1] x + b [1]

100
00 : 06 : 56.286 -> 00 : 07 : 01.526
a [1] = σ (z [1])가 있습니다

101
00 : 07 : 01.526 -> 00 : 07 : 06.563
그 차원은 (4,1) = (4,3) × (3,1) + (4,1)
가 될겁니다.

102
00 : 07 : 06.563 -> 00 : 07 : 11.297
a는 z와 같은 차수이므로 (4,1) 이군요

103
00 : 07 : 11.297 -> 00 : 07 : 16.793
그리고 x = a [0] 였고, 
y hat은 a [2] 였다는걸 기억합시다.

104
00 : 07 : 16.793 -> 00 : 07 : 21.560
그래서 실제로 x를 a[0]로 
바꿀 수있는 것입니다

105
00 : 07 : 21.560 -> 00 : 07 : 25.417
a [0]는 입력 x의 단순한 치환 값이기 때문입니다

106
00 : 07 : 25.417 -> 00 : 07 : 30.968
비슷한 방법으로
다음 층의 계산도 이끌 수 있습니다

107
00 : 07 : 30.968 -> 00 : 07 : 35.972
출력 계층은 매개 변수 W [2]와 b [2] 와

108
00 : 07 : 35.972 -> 00 : 07 : 40.770
연관 지을 수 있습니다

109
00 : 07 : 40.770 -> 00 : 07 : 44.549
이 경우 W [2]는 1 × 4 행렬이고

110
00 : 07 : 44.549 -> 00 : 07 : 47.529
b [2]는 1 × 1의 단순한 실수입니다

111
00 : 07 : 47.529 -> 00 : 07 : 51.982
연산 결과 z [2]은 실수가되므로
1 × 1 행렬로 쓸 수 있습니다

112
00 : 07 : 51.982 -> 00 : 07 : 57.267
이것은 (1,4) a는 (4,1)에서
b [2]은 (1,1)입니다

113
00 : 07 : 57.267 -> 00 : 08 : 02.397
그래서 연산 결과로 실수를 얻을 수 있는 겁니다.

114
00 : 08 : 02.397 -> 00 : 08 : 07.787
마지막 출력 장치를
로지스틱 회귀처럼 파악하면

115
00 : 08 : 07.787 -> 00 : 08 : 12.517
로지스틱 회귀에서 w transpose는
W [2] 의 역할을하고

116
00 : 08 : 12.517 -> 00 : 08 : 16.675
그리고 b는 b [2]와 동일합니다

117
00 : 08 : 16.675 -> 00 : 08 : 21.665
우선 네트워크
왼쪽을 무시하면

118
00 : 08 : 21.665 -> 00 : 08 : 26.434
이 마지막 출력 단위는
로지스틱 회귀와 비슷합니다

119
00 : 08 : 26.434 -> 00 : 08 : 30.010
다른 점은
매개 변수를 w와 b라고 쓰는 대신

120
00 : 08 : 30.010 -> 00 : 08 : 35.784
이번에는 (1,4) 차원의 W [2]와
(1,1) 차원의 b [2]라고 쓰는것 입니다.

121
00 : 08 : 35.784 -> 00 : 08 : 39.765
복습하면
로지스틱 회귀에서

122
00 : 08 : 39.765 -> 00 : 08 : 44.620
출력값 즉 예측 값을 구현하려면

123
00 : 08 : 44.620 -> 00 : 08 : 51.143
z = wT 곱하기 x + b와
y hat = a = σ (z)를 계산합니다

124
00 : 08 : 51.143 -> 00 : 08 : 55.499
숨겨진 레이어가 하나인 신경망을
구현하기 위해서는

125
00 : 08 : 55.499 -> 00 : 09 : 00.131
이 네 개의 식을 구현하면 됩니다.

126
00 : 09 : 00.131 -> 00 : 09 : 04.902
숨겨진 레이어의 네 개의 로지스틱 회귀 
유닛의 출력을

127
00 : 09 : 04.902 -> 00 : 09 : 09.329
벡터 형태로 구현 한 것이
이 부분이 하고 있는 일 입니다.

128
00 : 09 : 09.329 -> 00 : 09 : 13.867
출력 계층의 로지스틱 회귀 계산은
이 부분이 담당하고 있습니다

129
00 : 09 : 13.867 -> 00 : 09 : 18.401
이 설명에서 모든걸 알 수 있으면 좋겠지 만
여기서 꼭 배우면 좋겠다는 부분은 

130
00 : 09 : 18.401 -> 00 : 09 : 22.001
이 신경망의 계산에는
이 네 줄의 코드가 필요하다는 것입니다

131
00 : 09 : 22.001 -> 00 : 09 : 25.706
여기까지
입력 feature 벡터 x가 하나 있을 때

132
00 : 09 : 25.706 -> 00 : 09 : 30.278
이 신경망의 출력을 네 줄의 코드로 
계산할 수 있는 방법을 배웠습니다.

133
00 : 09 : 30.278 -> 00 : 09 : 34.575
저는 이 계산도 로지스틱 회귀와 
비슷한 느낌으로

134
00 : 09 : 34.575 -> 00 : 09 : 39.002
여러 training 샘플에 걸쳐
벡터화하고 싶습니다

135
00 : 09 : 39.002 -> 00 : 09 : 43.653
training 샘플을 행렬의 열로 나열하면
조금 수정하는 것만으로 가능합니다

136
00 : 09 : 43.653 -> 00 : 09 : 47.396
또한 로지스틱 회귀처럼

137
00 : 09 : 47.396 -> 00 : 09 : 50.514
신경망의 출력을
한 번에 하나의 training 샘플이 아니라

138
00 : 09 : 50.514 -> 00 : 09 : 55.114
모든 샘플에 대해
단번에 계산 할 수 있습니다

139
00 : 09 : 55.114 -> 00 : 09 : 57.939
다음 동영상에서
그 내용을 살펴 봅시다.