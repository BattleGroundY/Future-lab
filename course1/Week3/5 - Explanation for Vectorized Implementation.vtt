WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.530
이전 비디오에서는 행렬 x 형태로 가로로 쌓인 

2
00 : 00 : 01.530 -> 00 : 00 : 06.885
트레이닝 예제를 통해 foraward propagation의

3
00 : 00 : 06.885 -> 00 : 00 : 11.158
벡터화 된 구현을 도출 할 수있었습니다. 

4
00 : 00 : 11.158 -> 00 : 00 : 14.760
여러 예제를 통해 우리가 적어 둔 방정식이

5
00 : 00 : 14.760 -> 00 : 00 : 19.775
올바른 벡터화 구현 인 이유에 대해 정당화 해 봅시다.

6
00 : 00 : 19.775 -> 00 : 00 : 25.590
몇개의 예제를 통해 forward propagation 계산의 
일부 과정을 살펴 보겠습니다.

7
00 : 00 : 25.590 -> 00 : 00 : 27.645
첫 번째 training 샘플 에서는

8
00 : 00 : 27.645 -> 00 : 00 : 35.130
(w1) x1 + b1를 계산하고,

9
00 : 00 : 35.130 -> 00 : 00 : 38.970
두 번째 training 샘플 에서는

10
00 : 00 : 38.970 -> 00 : 00 : 49.310
(w1) x2 + b1

11
00 : 00 : 49.310 -> 00 : 00 : 50.900
세 번째 training 샘플은

12
00 : 00 : 50.900 -> 00 : 00 : 56.064
(w1) x3 + b1를 계산하게 될 것입니다.

13
00 : 00 : 56.064 -> 00 : 01 : 00.930
문제를 간략하게 설명하기 위해 b값은 생략합니다.

14
00 : 01 : 00.930 -> 00 : 01 : 08.395
즉, b를 0으로 만듦으로써 단순화 합니다.

15
00 : 01 : 08.395 -> 00 : 01 : 11.140
그러나 여기서 b를 0으로 만드는 것은

16
00 : 01 : 11.140 -> 00 : 01 : 14.320
0이 아닐때 보다 그저 약간의 
변화를 더 가져올 뿐입니다.

17
00 : 01 : 14.320 -> 00 : 01 : 17.610
이러한 설정은 설명을 조금 더 
단순화 하기 위한 조그만 변경사항일 뿐입니다.

18
00 : 01 : 17.610 -> 00 : 01 : 21.110
그러면 w1이 행렬이됩니다.

19
00 : 01 : 21.110 -> 00 : 01 : 25.625
따라서 이 행렬에는 일정 수의 행이 있습니다.

20
00 : 01 : 25.625 -> 00 : 01 : 28.296
x1에 대한 연산을 한번 봅시다.

21
00 : 01 : 28.296 -> 00 : 01 : 35.070
w1에 x1을 곱하여 얻은 결과는

22
00 : 01 : 35.070 -> 00 : 01 : 40.021
이러한 열벡터 입니다.

23
00 : 01 : 40.021 -> 00 : 01 : 47.420
마찬가지로 x2를 보면

24
00 : 01 : 47.420 -> 00 : 01 : 54.730
w1 곱하기 x2를 하게 되면

25
00 : 01 : 54.730 -> 00 : 02 : 00.460
또 다른 열 벡터를 얻습니다.

26
00 : 02 : 00.460 -> 00 : 02 : 03.250
그것이 z12이 될 것입니다.

27
00 : 02 : 03.250 -> 00 : 02 : 06.730
마지막으로 x3을 보면

28
00 : 02 : 06.730 -> 00 : 02 : 12.315
w1에 x3을 곱하면됩니다.

29
00 : 02 : 12.315 -> 00 : 02 : 19.530
세 번째 열 벡터인 z13를 가져옵니다.

30
00 : 02 : 19.530 -> 00 : 02 : 25.250
이제 당신이 대문자 X로 설정된 
training set을 기억한다면

31
00 : 02 : 25.250 -> 00 : 02 : 31.475
이것은 모든 training 샘플을 쌓아서 얻습니다.

32
00 : 02 : 31.475 -> 00 : 02 : 37.010
행렬 X의 성분은 벡터 x1으로 출발해서

33
00 : 02 : 37.010 -> 00 : 02 : 43.430
수평으로 x2를 쌓은 다음 수평으로 x3 쌓습니다.

34
00 : 02 : 43.430 -> 00 : 02 : 46.250
이건 3개의 training 샘플만 있는 경우입니다.

35
00 : 02 : 46.250 -> 00 : 02 : 50.371
더 많은 샘플이 있다면 이처럼 수평으로 쌓일 것입니다.

36
00 : 02 : 50.371 -> 00 : 02 : 57.790
이 행렬에 w를 곱합니다.

37
00 : 02 : 57.790 -> 00 : 03 : 00.190
행렬 곱셈이 어떻게 작동하는지 생각하면

38
00 : 03 : 00.190 -> 00 : 03 : 02.680
첫 번째 열은

39
00 : 03 : 02.680 -> 00 : 03 : 06.313
제가 여기에 칠한 이 보라색 값들.

40
00 : 03 : 06.313 -> 00 : 03 : 10.930
두 번째 열은 연두색 값입니다.

41
00 : 03 : 10.930 -> 00 : 03 : 16.612
세 번째 열은 이 주황색 값입니다.

42
00 : 03 : 16.612 -> 00 : 03 : 19.480
그들은 결국 이렇게 될 것입니다.

43
00 : 03 : 19.480 -> 00 : 03 : 27.740
물론 이것은 열 벡터로 표현 된 z11과

44
00 : 03 : 27.740 -> 00 : 03 : 37.185
열 벡터로 표시되는 z12가 뒤에 오고

45
00 : 03 : 37.185 -> 00 : 03 : 39.273
그 뒤에는 열 벡터 z13으로 표시됩니다.

46
00 : 03 : 39.273 -> 00 : 03 : 41.100
이건 세 가지 training 샘플이 있는 경우이고

47
00 : 03 : 41.100 -> 00 : 03 : 44.255
더 많은 샘플이 있을 때는 매트릭스에 
더 많은 열이 있게 될겁니다.

48
00 : 03 : 44.255 -> 00 : 03 : 51.220
그리고 이것은 대문자 Z[1]입니다.

49
00 : 03 : 51.220 -> 00 : 03 : 55.230
이것으로 왜 우리가 이전에

50
00 : 03 : 55.230 -> 00 : 04 : 02.830
W1 곱하기 xi = z1i 라고 설정 했는지에 대한 
justification이 됬으리라 믿습니다.

51
00 : 04 : 02.830 -> 00 : 04 : 08.310
이것은 우리가 오직 하나의 
training 샘플 만을 고려할 때입니다.

52
00 : 04 : 08.310 -> 00 : 04 : 12.565
다른 트레이닝 샘플들을 
컬럼에 스태킹 할 때는

53
00 : 04 : 12.565 -> 00 : 04 : 15.250
그 결과들도

54
00 : 04 : 15.250 -> 00 : 04 : 18.725
보시다시피 벡터Z에 누적되게 되는겁니다.

55
00 : 04 : 18.725 -> 00 : 04 : 24.565
제가 직접 보여드리지는 않을거지만, 
Python broadcasting 메커니즘을 사용하여 직접 확인해 볼 수 있습니다.

56
00 : 04 : 24.565 -> 00 : 04 : 26.245
뒤로 돌아가서 더하기를 하면,

57
00 : 04 : 26.245 -> 00 : 04 : 30.534
이 b 값은 여전히 살아 있습니다.

58
00 : 04 : 30.534 -> 00 : 04 : 34.540
(물론 실제로 일어날 일은 
파이썬의 broadcasting 메커니즘을 통해서 일겁니다.)

59
00 : 04 : 34.540 -> 00 : 04 : 41.790
이 행렬의 각 열에 Bi가 별도로 추가됩니다.

60
00 : 04 : 41.790 -> 00 : 04 : 48.220
그래서 이 슬라이드에서 저는

61
00 : 04 : 48.220 -> 00 : 04 : 51.980
Z1 = w1 곱하기 X + b1 라는 식이

62
00 : 04 : 51.980 -> 00 : 04 : 54.020
이전 슬라이드의 네 단계 중 첫 번째 단계

63
00 : 04 : 54.020 -> 00 : 04 : 57.493
의 올바른 벡터화 작업이라는걸 증명했습니다.

64
00 : 04 : 57.493 -> 00 : 04 : 59.990
그러나 실제로 유사한 분석 방법을 사용하면

65
00 : 04 : 59.990 -> 00 : 05 : 02.660
다른 단계들도 증명할 수 있을 겁니다.

66
00 : 05 : 02.660 -> 00 : 05 : 08.105
계산 후에 열단위로 입력 값을 쌓으면

67
00 : 05 : 08.105 -> 00 : 05 : 11.510
컬럼별로 같은 스태킹 결과를 얻습니다.

68
00 : 05 : 11.510 -> 00 : 05 : 14.970
마지막으로,이 비디오에서 우리가 이야기 한 내용을 요약합니다.

69
00 : 05 : 14.970 -> 00 : 05 : 16.520
이것이 당신의 신경 네트워크라면

70
00 : 05 : 16.520 -> 00 : 05 : 21.693
Forward propagation을 하기 위해서 정의 해야될 것들입니다.

71
00 : 05 : 21.693 -> 00 : 05 : 27.693
먼저 for문을 통해 training 샘플은 m개가 
존재한다는걸 정의하고

72
00 : 05 : 27.693 -> 00 : 05 : 34.100
그런 다음 우리는 이러한 값에 대해 training 샘플을 
이와 같은 열에 쌓습니다.

73
00 : 05 : 34.100 -> 00 : 05 : 38.265
Z1 a1 z2 a2 값은 다음과 같이 해당 열에 쌓습니다.

74
00 : 05 : 38.265 -> 00 : 05 : 43.820
이것은 A1의 경우지만 Z1도 동일합니다.

75
00 : 05 : 43.820 -> 00 : 05 : 46.975
Z2와 A2도 동일합니다.

76
00 : 05 : 46.975 -> 00 : 05 : 51.090
전에 슬라이드에서 보여드렸던 것중 하나가

77
00 : 05 : 51.090 -> 00 : 05 : 58.785
이 코드를 이용해 벡터화를 사용하면 
m개의 샘플을 동시에 처리 할 수 있다는 것입니다.

78
00 : 05 : 58.785 -> 00 : 06 : 00.555
비슷한 방식으로

79
00 : 06 : 00.555 -> 00 : 06 : 03.880
다른 모든 벡터화 코드들이 
여기 있는 4 줄의 코드에 대해 

80
00 : 06 : 03.880 -> 00 : 06 : 08.811
올바르게 벡터화 한 것인지에 대한 답을 찾을 수 있을 겁니다.

81
00 : 06 : 08.811 -> 00 : 06 : 10.675
다시 상기해 보자면,

82
00 : 06 : 10.675 -> 00 : 06 : 18.960
X 또한 A0과 같기 때문에

83
00 : 06 : 18.960 -> 00 : 06 : 27.980
입력 feature 벡터 x가 a0와 같기 때문에, 
xi는 a0i와 동일합니다.

84
00 : 06 : 27.980 -> 00 : 06 : 30.870
이 수식에는 실제로 대칭성이 있습니다.

85
00 : 06 : 30.870 -> 00 : 06 : 34.110
첫 번째 수식은 다음과 같이 쓸 수도 있습니다.

86
00 : 06 : 34.110 -> 00 : 06 : 41.790
Z1은 w1A0 + b1과 같습니다.

87
00 : 06 : 41.790 -> 00 : 06 : 45.680
이 두개의 수식과 이 두개의 수식은

88
00 : 06 : 45.680 -> 00 : 06 : 51.805
A 위의 첨자를 1로 표현하니까
매우 유사하게 보입니다.

89
00 : 06 : 51.805 -> 00 : 06 : 55.880
이것은 실제로 신경 네트워크의 여러 레이어가

90
00 : 06 : 55.880 -> 00 : 07 : 00.585
모두 똑같은 일을 하고 있고,
또는 같은 계산을 반복한다는 것을 보여줍니다.

91
00 : 07 : 00.585 -> 00 : 07 : 04.220
여기서 우리는 두 개의 레이어를 가진 
신경망에 대해 배웠습니다.

92
00 : 07 : 04.220 -> 00 : 07 : 08.475
다음 주에 우리는 신경 네트워크에 
더 깊이 노출 될 것입니다.

93
00 : 07 : 08.475 -> 00 : 07 : 11.670
더 깊은 신경 네트워크도
본질적으로는 이 두 단계 만 수행하면 됩니다.

94
00 : 07 : 11.670 -> 00 : 07 : 16.215
여기 보이는 것보다 단지 여러 번 
반복되는 것일 뿐입니다.

95
00 : 07 : 16.215 -> 00 : 07 : 21.255
여기까지가 여러 training 샘플을 기반으로 
신경망을 벡터화하는 방법입니다

96
00 : 07 : 21.255 -> 00 : 07 : 25.590
지금까지 우리는 신경망에서 
sigmoid 함수를 사용해 왔습니다

97
00 : 07 : 25.590 -> 00 : 07 : 27.925
이것은 실제로 최선의 선택이 아닙니다.

98
00 : 07 : 27.925 -> 00 : 07 : 29.675
다음 동영상에서 더 공부 해봅시다.

99
00 : 07 : 29.675 -> 00 : 07 : 32.450
sigmoid가 아닌 다른 activation function을 
어떻게 사용는지 말이죠.

100
00 : 07 : 32.450 -> 00 : 07 : 37.190
sigmoid 함수는 가능한 선택지 중 하나 일뿐입니다.