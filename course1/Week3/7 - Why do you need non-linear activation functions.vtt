WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 04.360
왜 신경망에
비선형 활성화 함수가 필요할까요?

2
00 : 00 : 04.360 -> 00 : 00 : 07.425
신경망에 뭔가 흥미로운 함수를
계산하려고하면

3
00 : 00 : 07.425 -> 00 : 00 : 10.335
비선형 활성화 함수가 필요하다고 알려져 있습니다.

4
00 : 00 : 10.335 -> 00 : 00 : 15.830
이것은 신경망의
Forward propagation 식입니다

5
00 : 00 : 15.830 -> 00 : 00 : 17.770
이 식을 제거 해서

6
00 : 00 : 17.770 -> 00 : 00 : 22.315
함수 g를 제거하고
a [1] = z [1] 라고 쓰겠습니다.

7
00 : 00 : 22.315 -> 00 : 00 : 27.690
또는 g (z) = z 라고 합시다.

8
00 : 00 : 27.690 -> 00 : 00 : 31.813
이것은 어떠한 경우 선형 활성화 함수 라고합니다

9
00 : 00 : 31.813 -> 00 : 00 : 33.205
더 좋은 이름은

10
00 : 00 : 33.205 -> 00 : 00 : 37.800
항등 활성화 함수가 있습니다
입력 값을 그대로 출력하고 있기 때문입니다

11
00 : 00 : 37.800 -> 00 : 00 : 39.600
이를 위해서는
a [2] = z [2] 라고 또 둬야 겠군요,

12
00 : 00 : 39.600 -> 00 : 00 : 43.310
그렇다면 어떻게 해야할까요?

13
00 : 00 : 43.310 -> 00 : 00 : 45.183
이 경우의 모델은

14
00 : 00 : 45.183 -> 00 : 00 : 53.620
y hat을 입력 feature x의 선형 함수로 
계산 하고 있을뿐입니다

15
00 : 00 : 53.620 -> 00 : 00 : 55.940
첫 번째 두 식을 생각합니다

16
00 : 00 : 55.940 -> 00 : 01 : 04.547
a [1] = z [1] = w [1] x + b

17
00 : 01 : 04.547 -> 00 : 01 : 15.965
a [2] = z [2] = w [2] a [1] + b [2]라고합니다

18
00 : 01 : 15.965 -> 00 : 01 : 25.370
그리고 a [1]의 정의를
아래의 식에 대입하면

19
00 : 01 : 25.370 -> 00 : 01 : 32.585
a [2] = w [2] (w [1] x + b [1])

20
00 : 01 : 32.585 -> 00 : 01 : 35.695
이 되는군요

21
00 : 01 : 35.695 -> 00 : 01 : 40.985
이 부분은 a [1]입니다

22
00 : 01 : 40.985 -> 00 : 01 : 47.460
그리고 + b [2]입니다
단순화하면

23
00 : 01 : 47.460 -> 00 : 01 : 53.120
(w [2] w [1]) x + (w [2] + b [1] + b [2])

24
00 : 01 : 53.120 -> 00 : 01 : 57.966
(w [2] w [1]) x + (w [2] + b [1] + b [2])
됩니다

25
00 : 01 : 57.966 -> 00 : 02 : 01.930
그 결과에서

26
00 : 02 : 01.930 -> 00 : 02 : 06.726
이 부분을 w 프라임, b 프라임으로 놓겠습니다.

27
00 : 02 : 06.726 -> 00 : 02 : 10.935
결과 w 프라임 x와 b 프라임이 됩니다.

28
00 : 02 : 10.935 -> 00 : 02 : 13.720
보시다시피 선형 활성화 함수를 사용하는 경우 또는

29
00 : 02 : 13.720 -> 00 : 02 : 17.095
항등 활성화 함수라고 부르는 것을 사용하는 경우에는

30
00 : 02 : 17.095 -> 00 : 02 : 23.335
신경망은 단순히 입력 값을 
선형 함수로 출력 할뿐입니다

31
00 : 02 : 23.335 -> 00 : 02 : 26.260
신경망에 많은 숨겨진 레이어가 있는

32
00 : 02 : 26.260 -> 00 : 02 : 27.460
Deep Network에 대해

33
00 : 02 : 27.460 -> 00 : 02 : 29.167
추후에 말하겠지만

34
00 : 02 : 29.167 -> 00 : 02 : 34.460
만약 선형 활성화 함수를 사용하거나

35
00 : 02 : 34.460 -> 00 : 02 : 36.760
활성화 함수를 사용하지 않는 경우에는

36
00 : 02 : 36.760 -> 00 : 02 : 39.250
신경망에 많은 층이 있어도

37
00 : 02 : 39.250 -> 00 : 02 : 42.970
선형 활성화 함수의 계산을
하고 있을뿐입니다

38
00 : 02 : 42.970 -> 00 : 02 : 45.905
숨겨진 레이어가 전혀 없다고 해도 똑같습니다.

39
00 : 02 : 45.905 -> 00 : 02 : 49.335
다음과 같은 경우도 있습니다

40
00 : 02 : 49.335 -> 00 : 02 : 50.880
선형 활성화 함수를

41
00 : 02 : 50.880 -> 00 : 02 : 55.170
여기에 사용하고
시그 모이 드 함수를 여기에 사용하면

42
00 : 02 : 55.170 -> 00 : 02 : 58.275
이 모델은 숨겨진 레이어없는

43
00 : 02 : 58.275 -> 00 : 03 : 02.505
표준 로지스틱 회귀와 동일합니다

44
00 : 03 : 02.505 -> 00 : 03 : 05.910
일부러 증명하지 않지만
원하신다면 직접 해보시길 바랍니다.

45
00 : 03 : 05.910 -> 00 : 03 : 07.465
여기에서 처럼

46
00 : 03 : 07.465 -> 00 : 03 : 11.265
숨겨진 레이어에 선형함수를 사용 하는 것은
사실 큰 의미가 없습니다

47
00 : 03 : 11.265 -> 00 : 03 : 17.130
2 개의 선형 함수를 합쳐도
그 자체가 선형 함수가 되기 때문입니다

48
00 : 03 : 17.130 -> 00 : 03 : 19.950
비선형 함수를 하나라도 넣지 않으면

49
00 : 03 : 19.950 -> 00 : 03 : 21.235
Deep network 에서 사용될 
좀 더 특별하고 흥미로운 

50
00 : 03 : 21.235 -> 00 : 03 : 25.350
함수를 계산할 수 없게 됩니다.

51
00 : 03 : 25.350 -> 00 : 03 : 29.820
선형 활성화 함수를
쓸수도 있는 방면이 하나 있습니다

52
00 : 03 : 29.820 -> 00 : 03 : 36.810
g (z) = z, 이것은 회귀 문제
기계 학습을 사용할 때입니다

53
00 : 03 : 36.810 -> 00 : 03 : 39.420
y가 실수일 때입니다

54
00 : 03 : 39.420 -> 00 : 03 : 42.675
예를 들어 주택 가격을 예측한다면

55
00 : 03 : 42.675 -> 00 : 03 : 46.935
y는 0이나 1이 아닌
모든 실수입니다

56
00 : 03 : 46.935 -> 00 : 03 : 54.660
주택 가격은 0 달러부터
매우 높은 가격까지가 될겁니다.

57
00 : 03 : 54.660 -> 00 : 03 : 58.640
수백만 달러의 경우도 있을 수 있지요

58
00 : 03 : 58.640 -> 00 : 04 : 04.580
하지만 데이터 세트 값이
아무리 높아지더라도

59
00 : 04 : 04.580 -> 00 : 04 : 09.705
y가 실수를 취하기 때문에

60
00 : 04 : 09.705 -> 00 : 04 : 14.700
이 경우에는 선형 활성화 함수를
사용해도 좋을지도 모릅니다

61
00 : 04 : 14.700 -> 00 : 04 : 17.805
y hat도

62
00 : 04 : 17.805 -> 00 : 04 : 24.215
실수 값으로 마이너스 무한대 플러스 무한대
의 범위를 가지게 될겁니다.

63
00 : 04 : 24.215 -> 00 : 04 : 28.700
그러나 Hidden unit에는
선형 활성화 함수를 사용해서는 안됩니다

64
00 : 04 : 28.700 -> 00 : 04 : 34.380
ReLU, Tanh, Leaky ReLU 나
다른 것을 사용합니다

65
00 : 04 : 34.380 -> 00 : 04 : 39.995
선형 활성화 함수를 사용할 가능성이 있는 곳은
일반적으로 출력 계층입니다

66
00 : 04 : 39.995 -> 00 : 04 : 41.595
그 이외에

67
00 : 04 : 41.595 -> 00 : 04 : 44.730
선형 활성화 함수를

68
00 : 04 : 44.730 -> 00 : 04 : 50.135
사용하는 것은, 지금은 얘기하진 않겠지만 
compression(압축)과 같이

69
00 : 04 : 50.135 -> 00 : 04 : 52.320
아주 특별한 경우를 제외하고는

70
00 : 04 : 52.320 -> 00 : 04 : 56.250
선형 활성화 함수를 사용하는 것은
매우 드문 일입니다

71
00 : 04 : 56.250 -> 00 : 04 : 59.130
1주차의 비디오에서 본
주택 가격의 예측 모델의

72
00 : 04 : 59.130 -> 00 : 05 : 03.795
주택 가격은 마이너스 값이 없었습니다

73
00 : 05 : 03.795 -> 00 : 05 : 07.060
이 경우에는 ReLU 활성화 함수를 사용하여

74
00 : 05 : 07.060 -> 00 : 05 : 11.580
y hat이
항상 0 이상이 되도록 했습니다.

75
00 : 05 : 11.580 -> 00 : 05 : 13.980
이제 비선형 활성화 함수가

76
00 : 05 : 13.980 -> 00 : 05 : 19.290
왜 신경망에 필수적인지
알게 되셨다고 생각합니다

77
00 : 05 : 19.290 -> 00 : 05 : 23.945
다음부터는 경사 강하 법에 대해
얘기해 보겠습니다.

78
00 : 05 : 23.945 -> 00 : 05 : 27.440
다음 비디오에서 경사 강하 법을 말하기 위하여

79
00 : 05 : 27.440 -> 00 : 05 : 29.230
개별 활성화 함수들의

80
00 : 05 : 29.230 -> 00 : 05 : 34.105
기울기와 미분의 추정 방법을
보여주고 싶습니다

81
00 : 05 : 34.105 -> 00 : 05 : 35.600
다음 비디오로 갑시다