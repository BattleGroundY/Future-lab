WEBVTT

1
00 : 00 : 00.360 -> 00 : 00 : 04.530
이전 비디오에서는 신경망에서
1 개의 training 샘플에서

2
00 : 00 : 04.530 -> 00 : 00 : 06.610
예측 값을 계산하는 방법을
보았습니다.

3
00 : 00 : 06.610 -> 00 : 00 : 11.520
이 비디오에서는 여러개의 training 샘플을
벡터화하는 방법을 소개합니다

4
00 : 00 : 11.520 -> 00 : 00 : 15.350
결과는 로지스틱 회귀 때 본 것과
거의 동일합니다

5
00 : 00 : 15.350 -> 00 : 00 : 19.050
각각의 training 샘플을
각각의 열에 거듭해가는 것으로

6
00 : 00 : 19.050 -> 00 : 00 : 23.630
방금 비디오에서의 식을
조금 바꾸면 사용할 수 있습니다

7
00 : 00 : 23.630 -> 00 : 00 : 27.860
신경망에서
거의 동시에

8
00 : 00 : 27.860 -> 00 : 00 : 32.340
모든 샘플에 대해서 출력 값을
계산할 수 있습니다

9
00 : 00 : 32.340 -> 00 : 00 : 35.080
어떻게 할 것인지
한번 보도록 합시다.

10
00 : 00 : 35.080 -> 00 : 00 : 40.192
이 4 개의 식은 이전 비디오에서 사용한 식으로
z [1] a [1] z [2] a [2]의 계산법을

11
00 : 00 : 40.192 -> 00 : 00 : 41.348
나타내고 있습니다

12
00 : 00 : 41.348 -> 00 : 00 : 46.867
이것을 사용하면
벡터 x를 입력 할 때

13
00 : 00 : 46.867 -> 00 : 00 : 53.810
단일 training 샘플의 a [2] = y hat을
생성 할 수 있습니다

14
00 : 00 : 54.920 -> 00 : 01 : 00.050
m 개의 샘플이있을 때에는
이 과정을 반복합니다

15
00 : 01 : 00.050 -> 00 : 01 : 01.870
즉 첫 번째 샘플

16
00 : 01 : 01.870 -> 00 : 01 : 06.600
x (1)을 사용하여

17
00 : 01 : 06.600 -> 00 : 01 : 11.062
첫 번째 샘플의 예측 값인 y hat (1)을
계산합니다

18
00 : 01 : 11.062 -> 00 : 01 : 16.537
x (2) 도 같은 식을 사용해서
y hat (2)을 생성합니다

19
00 : 01 : 16.537 -> 00 : 01 : 23.050
x (m) 까지 계속 반복해서
y hat (m)을 생성합니다

20
00 : 01 : 23.050 -> 00 : 01 : 28.349
모든 활성화 함수 a의 표기를 하기 위해

21
00 : 01 : 28.349 -> 00 : 01 : 31.669
a [2] (1)로 쓰고

22
00 : 01 : 31.669 -> 00 : 01 : 36.676
a [2] (2)로 쓰고

23
00 : 01 : 36.676 -> 00 : 01 : 40.640
a [2] (m)으로 씁니다

24
00 : 01 : 40.640 -> 00 : 01 : 46.830
이 표기 a [2] (i)에서

25
00 : 01 : 46.830 -> 00 : 01 : 52.520
(i)가 i 개의 샘플을 의미하고

26
00 : 01 : 52.520 -> 00 : 01 : 57.220
[2]는 2층 째를 말합니다

27
00 : 01 : 58.530 -> 00 : 02 : 02.460
[]와 ()의 첨자는 이렇게
사용됩니다

28
00 : 02 : 04.170 -> 00 : 02 : 07.920
벡터화되어 있지 않는 구현에서

29
00 : 02 : 07.920 -> 00 : 02 : 11.000
모든 training 샘플에 대해
예측 값을 계산하는 경우에는

30
00 : 02 : 11.000 -> 00 : 02 : 15.630
for i = 1 to m을

31
00 : 02 : 15.630 -> 00 : 02 : 18.260
이러한 4 개의 식에 대해 각각
구현해야합니다

32
00 : 02 : 18.260 -> 00 : 02 : 24.162
z [1] (i)

33
00 : 02 : 24.162 -> 00 : 02 : 30.064
= W [1] x (i) + b [1]

34
00 : 02 : 30.064 -> 00 : 02 : 38.253
a [1] (i)는 sigmoid z [1] (i)에서

35
00 : 02 : 38.253 -> 00 : 02 : 43.683
z [2] (i) = W [2] a [1] (i) + b [2]입니다

36
00 : 02 : 43.683 -> 00 : 02 : 50.099
그리고

37
00 : 02 : 50.099 -> 00 : 02 : 56.686
a [2] (i)는 sigmoid z [2] (i)입니다

38
00 : 02 : 56.686 -> 00 : 03 : 03.172
이러한 4 개의 식에 대해서
training 샘플에

39
00 : 03 : 03.172 -> 00 : 03 : 08.788
의존하고있는 모든 변수에
(i)를 넣습니다

40
00 : 03 : 08.788 -> 00 : 03 : 12.612
즉 (i)를
x, Z, a 에 넣습니다

41
00 : 03 : 12.612 -> 00 : 03 : 18.570
이렇게하면 m 개의 training 샘플의 출력을
계산할 수 있습니다

42
00 : 03 : 18.570 -> 00 : 03 : 23.930
여기서 여러분이 원하는 것은
이 계산을 벡터화하여 for 루프를 제거하는 겁니다.

43
00 : 03 : 23.930 -> 00 : 03 : 27.680
그런데 제가 너무 많은 선형 대수의 기초를
중요시하는 느낌도 있지만,

44
00 : 03 : 27.680 -> 00 : 03 : 31.170
딥러닝 시대에는
이를 제대로 구현할 수 있는 것이

45
00 : 03 : 31.170 -> 00 : 03 : 34.580
중요한 일이라고 생각합니다.

46
00 : 03 : 34.580 -> 00 : 03 : 38.160
우리는 이 코스 에서의 표기 방법에
매우 주의를 기울여 선택했습니다

47
00 : 03 : 38.160 -> 00 : 03 : 41.460
벡터화 과정을 최대한 간단하게
표현 하는 것을 목표로 했습니다

48
00 : 03 : 41.460 -> 00 : 03 : 46.140
이 핵심 기초 내용을 보고
이 알고리즘의 올바른 구현과

49
00 : 03 : 46.140 -> 00 : 03 : 49.750
더 빨리 작동시킬 수 있게 도움을
줄 수 있기를 바랍니다.

50
00 : 03 : 51.060 -> 00 : 03 : 56.210
그럼 이 코드 전체를
복사하여 다음 슬라이드에 넣어

51
00 : 03 : 56.210 -> 00 : 03 : 57.880
어떻게 벡터화할지
보겠습니다.

52
00 : 03 : 59.130 -> 00 : 04 : 02.154
여기에 이전 슬라이드에서
m 개의 훈련 샘플을

53
00 : 04 : 02.154 -> 00 : 04 : 04.324
순차적으로 처리하는 for 루프가 있습니다

54
00 : 04 : 04.324 -> 00 : 04 : 09.769
정의를 기억하시기 바랍니다.

55
00 : 04 : 09.769 -> 00 : 04 : 16.860
행렬 X는 training 샘플
열에 쌓은 것 이었습니다

56
00 : 04 : 16.860 -> 00 : 04 : 20.180
training 샘플을
열에 나열한 겁니다.

57
00 : 04 : 20.180 -> 00 : 04 : 23.220
nx와 m 차원 배열

58
00 : 04 : 23.220 -> 00 : 04 : 27.860
행렬입니다

59
00 : 04 : 29.198 -> 00 : 04 : 32.630
이 for 루프를 제거하고
벡터화 구현을 위해

60
00 : 04 : 32.630 -> 00 : 04 : 35.760
필요한 중요한 부분을
알아보겠습니다.

61
00 : 04 : 35.760 -> 00 : 04 : 41.394
여기서 해야 할 것은

62
00 : 04 : 41.394 -> 00 : 04 : 46.035
Z [1] = w [1] X + b [1]을 계산하는 것입니다

63
00 : 04 : 46.035 -> 00 : 04 : 50.692
A [1] = sigmoid z [1]

64
00 : 04 : 50.692 -> 00 : 04 : 56.157
Z [2] = w [2] A [1] + b [2]로 계산

65
00 : 04 : 56.157 -> 00 : 05 : 01.348
마지막으로

66
00 : 05 : 01.348 -> 00 : 05 : 10.100
A [2] = sigmoid z [2]

67
00 : 05 : 10.100 -> 00 : 05 : 16.440
유사점을 보자면
문자의 벡터 x들을

68
00 : 05 : 16.440 -> 00 : 05 : 23.480
각 열에 거듭해가는 것으로
큰 행렬 X까지 만들었습니다.

69
00 : 05 : 23.480 -> 00 : 05 : 28.494
이것을 z에 대해서도 적용하면

70
00 : 05 : 28.494 -> 00 : 05 : 33.509
z [1] (1), z [1] (2)와 같이 계속

71
00 : 05 : 33.509 -> 00 : 05 : 40.290
이것을 모든 열 벡터
z [1] (m)까지 쌓아갑니다.

72
00 : 05 : 40.290 -> 00 : 05 : 46.270
이 z [1] (i)에도 m의 모든 것을
열에 쌓고나면

73
00 : 05 : 46.270 -> 00 : 05 : 50.045
Z [1] 행렬이 나옵니다.

74
00 : 05 : 50.045 -> 00 : 05 : 55.299
마찬가지로 이 a [1] (i)은

75
00 : 05 : 55.299 -> 00 : 06 : 00.957
a [1] (1) a [1] (2)

76
00 : 06 : 00.957 -> 00 : 06 : 06.980
그리고 a [1] (m)까지
열에 쌓으면

77
00 : 06 : 06.980 -> 00 : 06 : 11.610
소문자 x에서 대문자 X를 만들고
소문자 z에서 대문자 Z를

78
00 : 06 : 11.610 -> 00 : 06 : 13.280
만든것 처럼

79
00 : 06 : 13.280 -> 00 : 06 : 20.920
벡터 소문자 a에서
A [1]를 산출합니다

80
00 : 06 : 20.920 -> 00 : 06 : 26.685
마찬가지로 Z [2]와 A [2]도
계산할 수 있습니다

81
00 : 06 : 26.685 -> 00 : 06 : 30.141
이들도 이 벡터들을 사용하여

82
00 : 06 : 30.141 -> 00 : 06 : 32.016
수평으로 쌓아가면서
얻을 수 있습니다

83
00 : 06 : 32.016 -> 00 : 06 : 37.326
이와 같은 방법으로

84
00 : 06 : 37.326 -> 00 : 06 : 40.840
Z [2]와 A [2]를 얻을 수 있습니다

85
00 : 06 : 40.840 -> 00 : 06 : 44.042
이 표기가 있는 것으로
도움이 되는 부분은

86
00 : 06 : 44.042 -> 00 : 06 : 47.391
이 Z와 A 등의 행렬은

87
00 : 06 : 47.391 -> 00 : 06 : 51.420
training 샘플에 대해
번호를 매기면서 수평으로 나열합니다.

88
00 : 06 : 51.420 -> 00 : 06 : 55.631
이것이 옆의 번호 매기기가
training 샘플에 대응하는 이유입니다.

89
00 : 06 : 55.631 -> 00 : 06 : 59.730
왼쪽에서 오른쪽으로 가면서
training 데이터를 순차적으로 나열하는 겁니다.

90
00 : 06 : 59.730 -> 00 : 07 : 04.617
세로의 이 수직 번호는
신경망의 각 노드에

91
00 : 07 : 04.617 -> 00 : 07 : 06.130
대한 정보를 제공합니다.

92
00 : 07 : 06.130 -> 00 : 07 : 11.077
예를 들어 이 노드는

93
00 : 07 : 11.077 -> 00 : 07 : 16.554
행렬의 상단 모서리의 값은
첫 번째 훈련 샘플의

94
00 : 07 : 16.554 -> 00 : 07 : 21.633
숨겨진 레이어의 첫 번째 유닛에 대응합니다

95
00 : 07 : 21.633 -> 00 : 07 : 25.812
하나 아래의 값은 첫 번째 훈련 샘플의
숨겨진 레이어의 두 번째 유닛

96
00 : 07 : 25.812 -> 00 : 07 : 27.525
에 대응합니다.

97
00 : 07 : 27.525 -> 00 : 07 : 31.505
그리고 첫 번째 훈련 샘플의
숨겨진 레이어의 3번째 유닛 으로 이어집니다

98
00 : 07 : 31.505 -> 00 : 07 : 37.540
아래로 살펴 보자면 숨겨진 레이어의 유닛 
번호 매기기를 색인하고 있습니다.

99
00 : 07 : 39.670 -> 00 : 07 : 42.564
한편 옆으로 살펴 보자면
숨겨진 레이어의 첫 번째 유닛의 첫 훈련 샘플에서

100
00 : 07 : 42.564 -> 00 : 07 : 45.450
2 번째,

101
00 : 07 : 45.450 -> 00 : 07 : 48.240
3 번째 그리고
차례 차례로 이어갑니다

102
00 : 07 : 48.240 -> 00 : 07 : 53.718
여기 노드가 대응하는 것이
첫번째 training샘플의 숨겨진 레이어의 첫 번째 유닛

103
00 : 07 : 53.718 -> 00 : 07 : 59.030
마지막으로 m 개의 training 샘플의
activation 까지 계속됩니다

104
00 : 08 : 00.760 -> 00 : 08 : 07.663
정리하면 행렬 A의 수평 측은
training 샘플의 번호에 대응하고

105
00 : 08 : 10.150 -> 00 : 08 : 14.195
수직 측은 행렬 A의

106
00 : 08 : 14.195 -> 00 : 08 : 17.589
각각의 숨겨진 레이어 단위에
대응하고 있습니다

107
00 : 08 : 22.342 -> 00 : 08 : 26.870
같은 개념이 행렬 Z, 행렬 X에서도
적용됩니다

108
00 : 08 : 26.870 -> 00 : 08 : 31.840
가로가 각각의 training 샘플에 대응

109
00 : 08 : 31.840 -> 00 : 08 : 36.227
세로는 신경망의 layer 유닛에 대응 하면서

110
00 : 08 : 36.227 -> 00 : 08 : 41.180
각각의 입력 feature에
대응하고 있습니다

111
00 : 08 : 42.750 -> 00 : 08 : 46.600
지금까지 이러한 식들을 벡터화하여 신경망을
구현하는 방법을 배웠습니다.

112
00 : 08 : 46.600 -> 00 : 08 : 51.320
여러 샘플에 대한
벡터화입니다

113
00 : 08 : 51.320 -> 00 : 08 : 55.130
다음 비디오에서는
이런 종류의 벡터화 방법에 대해

114
00 : 08 : 55.130 -> 00 : 08 : 59.070
올바른 구현인지
판단할 수 있는 justification 방법을 소개합니다

115
00 : 08 : 59.070 -> 00 : 09 : 03.468
정당화 방법은 로지스틱 회귀에서 본 것과
유사할 거라고 생각합니다.

116
00 : 09 : 03.468 -> 00 : 09 : 05.300
다음 비디오로 넘어갑니다.