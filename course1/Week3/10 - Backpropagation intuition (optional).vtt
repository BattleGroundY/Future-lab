WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.230
이전 동영상에서

2
00 : 00 : 01.230 -> 00 : 00 : 03.720
Back propagation의 식을 보고 왔습니다

3
00 : 00 : 03.720 -> 00 : 00 : 06.900
이번 동영상에서는
계산 그래프를 사용하여

4
00 : 00 : 06.900 -> 00 : 00 : 10.515
그 식이 어떻게 유도 됬는지에 대한
직감을 잡으시기 바랍니다.

5
00 : 00 : 10.515 -> 00 : 00 : 12.385
이 동영상은 옵션 비디오 이기 때문에

6
00 : 00 : 12.385 -> 00 : 00 : 14.106
보시던 안보시던
자유롭게 해주세요

7
00 : 00 : 14.106 -> 00 : 00 : 16.360
어느쪽으로도 작업은
잘 할 수 있을 것입니다

8
00 : 00 : 16.360 -> 00 : 00 : 19.410
복습해보면 로지스틱 회귀에 대해 이야기 할 때

9
00 : 00 : 19.410 -> 00 : 00 : 23.685
z 및 a 그리고 손실을 계산하기 위해

10
00 : 00 : 23.685 -> 00 : 00 : 26.145
Forward prop을 진행했습니다.

11
00 : 00 : 26.145 -> 00 : 00 : 27.445
그리고 미분 값을 구하기 위해

12
00 : 00 : 27.445 -> 00 : 00 : 32.520
이러한 뒤쪽을 향한 연산을 했죠,
우선 da를 계산하고

13
00 : 00 : 32.520 -> 00 : 00 : 35.400
그리고 dz를 계산하고

14
00 : 00 : 35.400 -> 00 : 00 : 40.720
또한 dw와 db를 계산 했습니다

15
00 : 00 : 40.720 -> 00 : 00 : 46.970
그래서 손실함수 L (a, y)의 정의는

16
00 : 00 : 46.970 -> 00 : 00 : 52.655
-yloga- (1-y) log (1-a)

17
00 : 00 : 52.655 -> 00 : 00 : 57.440
입니다.

18
00 : 00 : 57.440 -> 00 : 00 : 59.750
만약 미적분에 대한 자세한 내용을 잘 아신다면

19
00 : 00 : 59.750 -> 00 : 01 : 03.600
이것의 a에 대한 미분을 구하면

20
00 : 01 : 03.600 -> 00 : 01 : 06.156
이것은 da에 대한 공식입니다

21
00 : 01 : 06.156 -> 00 : 01 : 09.060
즉, da는 이것과 같습니다.

22
00 : 01 : 09.060 -> 00 : 01 : 12.750
그리고 정말 미분을 해보면

23
00 : 01 : 12.750 -> 00 : 01 : 18.808
이것은 -y / a + (1-y / 1-a)라고 볼 수 있습니다

24
00 : 01 : 18.808 -> 00 : 01 : 23.040
이것이 미분을 취하면 이렇게 되는 것은
미적분학에서 알 수 있습니다

25
00 : 01 : 23.040 -> 00 : 01 : 26.680
뒤쪽으로 한걸음 더 나아가 dz을 계산하면

26
00 : 01 : 26.680 -> 00 : 01 : 32.430
dz는 a - y이라고 볼 수 있습니다

27
00 : 01 : 32.430 -> 00 : 01 : 37.920
이유는 앞에서 설명했지만

28
00 : 01 : 37.920 -> 00 : 01 : 45.425
체인룰에 따르면 dz = da × g '(z)입니다

29
00 : 01 : 45.425 -> 00 : 01 : 50.535
여기서 g (z)는 z에
시그 모이 드 함수를 적용한 것입니다

30
00 : 01 : 50.535 -> 00 : 01 : 56.245
로지스틱 회귀에서
출력 장치의 활성화 함수 입니다.

31
00 : 01 : 56.245 -> 00 : 02 : 00.570
이것도 하나의 로지스틱 회귀 라는 것을
기억하십시오

32
00 : 02 : 00.570 -> 00 : 02 : 05.757
x1, x2, x3 및
하나의 시그 모이 드 유닛이있어

33
00 : 02 : 05.757 -> 00 : 02 : 07.400
거기에서 나온 a 는
y hat을 도출합니다.

34
00 : 02 : 07.400 -> 00 : 02 : 11.400
따라서 여기의 활성화 함수는 시그 모이 드 함수입니다

35
00 : 02 : 11.400 -> 00 : 02 : 12.960
여담입니다만

36
00 : 02 : 12.960 -> 00 : 02 : 17.205
미적분학의 체인룰에 대해
익숙한 사람들을 위해서 말해둡니다.

37
00 : 02 : 17.205 -> 00 : 02 : 22.520
이것의 이유는 a는 σ (z)이고

38
00 : 02 : 22.520 -> 00 : 02 : 29.310
그래서 z에 대한 L의 미분은

39
00 : 02 : 29.310 -> 00 : 02 : 36.800
(a에 대한 L의 미분) × (da / dz)입니다

40
00 : 02 : 36.800 -> 00 : 02 : 39.611
이 a는 σ(z)와 동일하고

41
00 : 02 : 39.611 -> 00 : 02 : 42.970
이것은 d / dz × g (z)와 동일합니다

42
00 : 02 : 42.970 -> 00 : 02 : 49.080
마지막으로 이것은 g '(z)와 동일합니다.

43
00 : 02 : 49.080 -> 00 : 02 : 54.060
따라서 코드에서는
dz라고 써있는 이 식은

44
00 : 02 : 54.060 -> 00 : 02 : 59.484
코드에서는 da로 표시 되어 있는 이 식
× g '(z)입니다

45
00 : 02 : 59.484 -> 00 : 03 : 05.860
이것은 단지 이 식을 나타내고있을뿐입니다

46
00 : 03 : 05.860 -> 00 : 03 : 09.172
마지막 식의 도출은 미적분학
특히 체인룰에 익숙하지 않은 경우

47
00 : 03 : 09.172 -> 00 : 03 : 13.510
의미를 모르실 수도 있습니다.

48
00 : 03 : 13.510 -> 00 : 03 : 15.325
몰라도 걱정하실 필요 없습니다

49
00 : 03 : 15.325 -> 00 : 03 : 18.853
필요할 때 직관적인 설명을
하도록 하겠습니다.

50
00 : 03 : 18.853 -> 00 : 03 : 22.315
로지스틱 회귀를 위해
dz를 계산했기 때문에

51
00 : 03 : 22.315 -> 00 : 03 : 26.335
dw와 db를 계산합니다
dw = dz 곱하기 x 이고

52
00 : 03 : 26.335 -> 00 : 03 : 31.470
db는 훈련 샘플이 하나의 경우
그냥 dz입니다

53
00 : 03 : 31.470 -> 00 : 03 : 33.822
이것이 로지스틱 회귀 였습니다.

54
00 : 03 : 33.822 -> 00 : 03 : 36.700
신경망에서 Back prop를 계산 할 때

55
00 : 03 : 36.700 -> 00 : 03 : 40.090
이와 비슷한 계산을 하지만
계산을 두 번 수행한다는 점에서 다릅니다

56
00 : 03 : 40.090 -> 00 : 03 : 46.995
x는 직접 출력 장치에 갈 것이 아니라

57
00 : 03 : 46.995 -> 00 : 03 : 50.930
먼저 숨겨진 레이어로 옮겨진 후에
출력 장치로 이동 때문입니다

58
00 : 03 : 50.930 -> 00 : 03 : 58.405
여기에서한 계산은
한 단계의 계산과 같은 것으로

59
00 : 03 : 58.405 -> 00 : 04 : 04.483
이러한 두 층의 신경망 에서는
이 계산을 두 번 합니다

60
00 : 04 : 04.483 -> 00 : 04 : 08.586
이것은 입력 레이어, 숨겨진 레이어, 및 출력 레이어가 있는

61
00 : 04 : 08.586 -> 00 : 04 : 10.138
2 레이어 신경망 이라고 생각합니다

62
00 : 04 : 10.138 -> 00 : 04 : 12.070
계산의 진행 방식을 기억하십시오.

63
00 : 04 : 12.070 -> 00 : 04 : 17.210
우선 이 수식을 사용하여 z [1]을 계산

64
00 : 04 : 17.210 -> 00 : 04 : 22.177
그리고 a [1] 과 z [2]를 계산합니다

65
00 : 04 : 22.177 -> 00 : 04 : 25.505
z [2]는 매개 변수 W [2]와 b [2]의 영향도받습니다

66
00 : 04 : 25.505 -> 00 : 04 : 27.530
다음은 z [2]를 바탕으로

67
00 : 04 : 27.530 -> 00 : 04 : 32.815
a [2]를 구하고, 마지막으로 손실을 계산할 수 있습니다

68
00 : 04 : 32.815 -> 00 : 04 : 41.560
Back prop에서는 반대 방향으로 이동합니다
우선 da [2]를 계산하고 그리고 dz [2]

69
00 : 04 : 41.560 -> 00 : 04 : 48.805
또 다시 dW [2] db [2]를 계산합니다

70
00 : 04 : 48.805 -> 00 : 04 : 53.232
또 다시 da [1]

71
00 : 04 : 53.232 -> 00 : 04 : 57.278
dz [1] ...이런 식으로 계속됩니다

72
00 : 04 : 57.278 -> 00 : 05 : 00.290
입력 x에 대한 미분을 계산할 필요는 없습니다

73
00 : 05 : 00.290 -> 00 : 05 : 03.745
Supervised learning에서 x는 고정 되어 있고

74
00 : 05 : 03.745 -> 00 : 05 : 07.845
최적화 할 필요가 없기 때문이죠

75
00 : 05 : 07.845 -> 00 : 05 : 09.655
따라서 적어도 Supervised learning에서
x에 대해 미분은 계산하지 않습니다

76
00 : 05 : 09.655 -> 00 : 05 : 15.605
da [2]의 계산과정은 생략 하겠습니다.

77
00 : 05 : 15.605 -> 00 : 05 : 18.110
실제로 da [2]를 계산하고

78
00 : 05 : 18.110 -> 00 : 05 : 20.750
그것을 사용하여
dz [2]을 구할 수 있지만

79
00 : 05 : 20.750 -> 00 : 05 : 25.760
실제로 여러분은 이 두 가지를
하나의 단계로 정리할 수 있습니다.

80
00 : 05 : 25.760 -> 00 : 05 : 31.715
그러면 이전의 계산을 적용해보면, dz [2] = a [2] -y

81
00 : 05 : 31.715 -> 00 : 05 : 33.620
물론 dW [2]와 db [2]도 있습니다

82
00 : 05 : 33.620 -> 00 : 05 : 38.615
이들은 아래에 쓰겠습니다.

83
00 : 05 : 38.615 -> 00 : 05 : 46.700
dW [2] = dz [2] a [1]T

84
00 : 05 : 46.700 -> 00 : 05 : 52.040
db [2] = dz [2]입니다

85
00 : 05 : 52.040 -> 00 : 05 : 55.990
이 연산은 로지스틱 회귀와
비슷하네요

86
00 : 05 : 55.990 -> 00 : 06 : 03.550
로지스틱 회귀는
dw = dz 곱하기 x라는 것 이었습니다

87
00 : 06 : 03.550 -> 00 : 06 : 08.770
이번에는 a [1]이 x의 역할을 담당하고 있습니다

88
00 : 06 : 08.770 -> 00 : 06 : 14.125
대문자 행렬 W 및 개별 파라미터 w의
관계를 생각하면

89
00 : 06 : 14.125 -> 00 : 06 : 16.660
여기에 transpose가 쓰이는게 맞겠죠?

90
00 : 06 : 16.660 -> 00 : 06 : 24.370
출력이 하나인 로지스틱 회귀에서
w는 하나의 행 벡터이기 때문입니다

91
00 : 06 : 24.370 -> 00 : 06 : 26.980
dW [2]도 비슷한 것이지만

92
00 : 06 : 26.980 -> 00 : 06 : 32.440
여기있는 dW는 열 벡터이므로

93
00 : 06 : 32.440 -> 00 : 06 : 36.980
로지스틱 회귀의 x와는 달리
a [1]을 transpose 합니다.

94
00 : 06 : 36.980 -> 00 : 06 : 40.335
이제 Back prop의 절반이 끝났습니다

95
00 : 06 : 40.335 -> 00 : 06 : 44.045
여기서도 da [1]을 계산해도 상관은 없지만

96
00 : 06 : 44.045 -> 00 : 06 : 49.440
실제로 da [1]과 dz [1]의 계산은

97
00 : 06 : 49.440 -> 00 : 06 : 52.330
하나로 묶어 버리는 일이 많습니다

98
00 : 06 : 52.330 -> 00 : 06 : 57.130
따라서 구현 할 때 dz [1] =

99
00 : 06 : 57.130 -> 00 : 07 : 03.480
(W [2] T × dz [2]) * (g [1] '(z [1]))

100
00 : 07 : 03.480 -> 00 : 07 : 10.383
* 는 elementwise 곱셈입니다.

101
00 : 07 : 10.383 -> 00 : 07 : 13.960
여기서 차원을 한번 확인 해봅시다.

102
00 : 07 : 13.960 -> 00 : 07 : 19.510
이러한 신경망이 있다고 해봅시다.

103
00 : 07 : 19.510 -> 00 : 07 : 23.000
출력값 y hat이 여기에 있네요

104
00 : 07 : 23.000 -> 00 : 07 : 28.265
n[0] = nx와 같습니다.
즉 n[0] 개의 입력 feature가 있다고 합시다.

105
00 : 07 : 28.265 -> 00 : 07 : 30.230
n [1] 개의 숨겨진 유닛

106
00 : 07 : 30.230 -> 00 : 07 : 34.275
그리고 n [2]가 있습니다

107
00 : 07 : 34.275 -> 00 : 07 : 36.740
이 경우 n [2] 출력 장치는 하나이기 때문에

108
00 : 07 : 36.740 -> 00 : 07 : 38.565
n [2]는 1입니다

109
00 : 07 : 38.565 -> 00 : 07 : 48.795
행렬 W[2]는 (n [2], n [1]) 차원이므로

110
00 : 07 : 48.795 -> 00 : 07 : 57.490
z [2] 그리고 dz [2]는 (n [2], 1) 차원입니다

111
00 : 07 : 57.490 -> 00 : 07 : 59.850
binary classification을 할 때는
(1,1)가 되네요

112
00 : 07 : 59.850 -> 00 : 08 : 04.750
z [1]과 dz [1]은

113
00 : 08 : 04.750 -> 00 : 08 : 10.045
(n 1, 1) 차원입니다

114
00 : 08 : 10.045 -> 00 : 08 : 16.115
모든 변수 foo와 dfoo은
언제든지 동일한 차원을 갖습니다

115
00 : 08 : 16.115 -> 00 : 08 : 20.850
그래서 W와 dW와 b와 db, z와 dz도

116
00 : 08 : 20.850 -> 00 : 08 : 23.680
항상 같은 차원을 가지는 것입니다

117
00 : 08 : 23.680 -> 00 : 08 : 26.895
이러한 모든 차원이 맞는지를
확인하시기 바랍니다.

118
00 : 08 : 26.895 -> 00 : 08 : 35.430
dz [1] = W [2] T × dz [2]가 되네요

119
00 : 08 : 35.430 -> 00 : 08 : 44.490
이것은 elementwise 곱셈으로
* g [1] '(z [1])가 추가 될겁니다.

120
00 : 08 : 44.490 -> 00 : 08 : 47.040
위에서 dz [1] 차원은

121
00 : 08 : 47.040 -> 00 : 08 : 52.575
(n 1, 1) 차원입니다

122
00 : 08 : 52.575 -> 00 : 08 : 57.945
W [2]가 전치되어 있기 때문에
이것은 (n 1, n [2]) 차원입니다

123
00 : 08 : 57.945 -> 00 : 09 : 05.790
dz [2] (n [2], 1) 차원이고

124
00 : 09 : 05.790 -> 00 : 09 : 07.230
이것은 z [1]과 같은 차원입니다

125
00 : 09 : 07.230 -> 00 : 09 : 11.820
그래서 이것도 (n 1, 1)이되고
elementwise 곱셈이 되네요

126
00 : 09 : 11.820 -> 00 : 09 : 14.350
이제 차원은 정확히 알게 되었죠?

127
00 : 09 : 14.350 -> 00 : 09 : 18.330
(n 1, 1) 차원 벡터는

128
00 : 09 : 18.330 -> 00 : 09 : 23.520
(n 1, n [2]) 차원의 행렬
× (n [2], 1)에서 얻을 수 있습니다

129
00 : 09 : 23.520 -> 00 : 09 : 28.890
왜냐하면 이 두 곱으로
(n 1, 1) 차원의 행렬이 나오고

130
00 : 09 : 28.890 -> 00 : 09 : 34.618
두 (n 1, 1) 차원 벡터끼리의
elementwise 곱셈이기 때문입니다

131
00 : 09 : 34.618 -> 00 : 09 : 36.060
따라서 차원은 딱 어울립니다

132
00 : 09 : 36.060 -> 00 : 09 : 40.620
Back prop를 구현할 때의 조언을 하나 드리자면

133
00 : 09 : 40.620 -> 00 : 09 : 44.790
행렬의 차원이 일치하는 것을
확인한 후

134
00 : 09 : 44.790 -> 00 : 09 : 47.190
즉 W [1]과 W [2], Z [1]

135
00 : 09 : 47.190 -> 00 : 09 : 50.430
Z [2], A [1], A [2] 등의 행렬의 차수를

136
00 : 09 : 50.430 -> 00 : 09 : 54.180
확실히 생각해서

137
00 : 09 : 54.180 -> 00 : 09 : 58.642
그 차수가 맞는지 확인하면

138
00 : 09 : 58.642 -> 00 : 10 : 03.390
그 결과 Back prop 버그를
많이 없앨 수 있습니다

139
00 : 10 : 03.390 -> 00 : 10 : 06.960
좋네요
이제 dz [1]을 구했습니다

140
00 : 10 : 06.960 -> 00 : 10 : 12.160
마지막으로 dW [1] db [1]입니다

141
00 : 10 : 12.160 -> 00 : 10 : 13.965
여기에 쓰는 것이 좋지만

142
00 : 10 : 13.965 -> 00 : 10 : 17.200
공간이 부족하기 때문에
오른쪽에 쓰겠습니다.

143
00 : 10 : 17.200 -> 00 : 10 : 21.965
dW [1] db [1]은 다음 식으로 구할 수 있습니다

144
00 : 10 : 21.965 -> 00 : 10 : 25.950
이것은 dz [1] 곱하기 xT 이고

145
00 : 10 : 25.950 -> 00 : 10 : 28.905
이것은 dz [1]입니다

146
00 : 10 : 28.905 -> 00 : 10 : 34.045
이 식과 표현은 비슷하네요

147
00 : 10 : 34.045 -> 00 : 10 : 37.095
이것은 단순한 우연이 아닙니다

148
00 : 10 : 37.095 -> 00 : 10 : 41.660
x가 a [0]의 역할을 담당하고 있어
x의 전치는 a [0]의 전치랑 같아져

149
00 : 10 : 41.660 -> 00 : 10 : 45.484
실제로 이러한 표현은 매우 유사한 것입니다

150
00 : 10 : 45.484 -> 00 : 10 : 50.260
이제 Back prop가 어떻게
증명되는지 이해 하셨나요?

151
00 : 10 : 50.260 -> 00 : 10 : 54.530
여기에는 dz [2] dW [2] db [2] dz [1] dW [1]
그리고 db [1] 이라는

152
00 : 10 : 54.530 -> 00 : 11 : 00.190
6 개의 중요한 식이 있습니다

153
00 : 11 : 00.190 -> 00 : 11 : 05.767
이 6 개의 식을 취하고
다음 슬라이드로 복사 하겠습니다.

154
00 : 11 : 05.767 -> 00 : 11 : 08.950
여기까지 하나의 훈련 샘플을 훈련 시킬 때의

155
00 : 11 : 08.950 -> 00 : 11 : 13.959
Back prop을 찾아 왔습니다

156
00 : 11 : 13.959 -> 00 : 11 : 21.530
물론 한 번에 하나의
훈련 샘플을 처리하는 것보다

157
00 : 11 : 21.530 -> 00 : 11 : 27.810
모든 훈련 샘플을 벡터화하여
처리하는게 더 좋습니다.

158
00 : 11 : 27.810 -> 00 : 11 : 30.971
Forward prop의 경우에는

159
00 : 11 : 30.971 -> 00 : 11 : 33.545
한 개의 훈련 샘플을 처리 할 때

160
00 : 11 : 33.545 -> 00 : 11 : 41.665
이런 식이 있었습니다

161
00 : 11 : 41.665 -> 00 : 11 : 43.655
벡터화하기 위해서는

162
00 : 11 : 43.655 -> 00 : 11 : 51.260
모든 z를 쌓아서
이렇게 열로 정렬합니다.

163
00 : 11 : 51.260 -> 00 : 12 : 00.775
z [1] (m)까지 있군요
이를 대문자 Z라고 불렀습니다.

164
00 : 12 : 00.775 -> 00 : 12 : 04.960
이처럼 열로 늘어 놓은 것을

165
00 : 12 : 04.960 -> 00 : 12 : 10.240
대문자 버전으로 놓습니다.

166
00 : 12 : 10.240 -> 00 : 12 : 17.093
그럼 Z [1] = W [1] X + b [1]

167
00 : 12 : 17.093 -> 00 : 12 : 24.700
A [1] = g [1] (Z [1]) 인 것이 됩니다.

168
00 : 12 : 24.700 -> 00 : 12 : 28.645
이 코스에서 우리는 표기법을
주의 깊게 정의했기 때문에

169
00 : 12 : 28.645 -> 00 : 12 : 35.550
샘플을 행렬의 열로 나란히 쌓는것도 문제가 없습니다.

170
00 : 12 : 35.550 -> 00 : 12 : 40.105
신중하게 계산해 나간다면
Back prop도 마찬가지로 있을 것입니다

171
00 : 12 : 40.105 -> 00 : 12 : 46.645
벡터화한 식은 다음과 같습니다

172
00 : 12 : 46.645 -> 00 : 12 : 52.250
우선 모든 훈련 샘플에서
dz를 구해

173
00 : 12 : 52.250 -> 00 : 12 : 58.339
행렬의 각각의 열로 정렬합니다
a[2], y 이들도 똑같이하면

174
00 : 12 : 58.339 -> 00 : 13 : 03.070
이것이 벡터화한 구현이 됩니다

175
00 : 13 : 03.070 -> 00 : 13 : 05.569
이것이 dW [2]의 계산 방법이구요

176
00 : 13 : 05.569 -> 00 : 13 : 11.130
여기에는 1 / m를 붙였습니다
왜냐하면 비용 함수 J는

177
00 : 13 : 11.130 -> 00 : 13 : 18.410
i가 1부터 m까지의 손실을 더해
m으로 나눈 것이기 때문입니다

178
00 : 13 : 18.410 -> 00 : 13 : 20.615
따라서 미분을 계산 할 때

179
00 : 13 : 20.615 -> 00 : 13 : 23.885
로지스틱 회귀에 weight를
업데이트 할 때처럼

180
00 : 13 : 23.885 -> 00 : 13 : 27.982
1 / m를 추가해주는 겁니다.

181
00 : 13 : 27.982 -> 00 : 13 : 31.790
db [2]도 이렇게 있게 됩니다

182
00 : 13 : 31.790 -> 00 : 13 : 40.640
이것도 dZ의 총합계를 1 / m 한 것입니다
dZ [1]는 다음과 같이 계산할 수 있습니다

183
00 : 13 : 40.640 -> 00 : 13 : 49.109
다시 말하지만 이것은 elementwise 곱셈입니다

184
00 : 13 : 49.109 -> 00 : 13 : 56.595
이전 슬라이드에서 보았을 때는
이것은 (n 1, 1) 차원이지만

185
00 : 13 : 56.595 -> 00 : 14 : 03.185
이번에는 (n 1, m)로 되어 있습니다

186
00 : 14 : 03.185 -> 00 : 14 : 09.045
이들은 모두 (n 1, m) 차원입니다

187
00 : 14 : 09.045 -> 00 : 14 : 19.310
따라서 이 별표는
elementwise 곱셈이 되는 것입니다

188
00 : 14 : 19.310 -> 00 : 14 : 21.454
나머지 둘은 이렇게됩니다

189
00 : 14 : 21.454 -> 00 : 14 : 25.836
이들은 그렇게
충격적인 변화는 없지요

190
00 : 14 : 25.836 -> 00 : 14 : 29.510
이제 Back prop 알고리즘의 구현 방법을
직관적으로 이해하실 수 있으면 좋겠군요

191
00 : 14 : 29.510 -> 00 : 14 : 32.205
머신러닝의 모든 부분중

192
00 : 14 : 32.205 -> 00 : 14 : 34.820
Backprop 알고리즘의 미분 방법이

193
00 : 14 : 34.820 -> 00 : 14 : 38.465
내가 본 것 중 가장 복잡한
수학의 하나라고 생각합니다

194
00 : 14 : 38.465 -> 00 : 14 : 42.470
이 과정에는 선형 대수학과

195
00 : 14 : 42.470 -> 00 : 14 : 46.830
행렬의 미분에 관한 지식이 필요합니다.

196
00 : 14 : 46.830 -> 00 : 14 : 50.165
행렬의 미적분학 전문가 분이라면

197
00 : 14 : 50.165 -> 00 : 14 : 54.255
이 과정을 통해 스스로 미분 알고리즘을
입증 할 수 있을지도 모르지만

198
00 : 14 : 54.255 -> 00 : 14 : 57.500
딥러닝을 실천 하고 있는 사람들 중에는

199
00 : 14 : 57.500 -> 00 : 15 : 01.060
이 동영상과 동일한 수준의
식의 도출 방법을보고

200
00 : 15 : 01.060 -> 00 : 15 : 04.100
직감을 가지고 이 알고리즘을

201
00 : 15 : 04.100 -> 00 : 15 : 08.580
효과적으로 구현할 수 있는 사람이
많이 있다고 생각합니다

202
00 : 15 : 08.580 -> 00 : 15 : 10.070
미적분학의 전문가라면

203
00 : 15 : 10.070 -> 00 : 15 : 13.395
처음부터 모든 부분을
증명해 보는 것도 좋다고 생각합니다

204
00 : 15 : 13.395 -> 00 : 15 : 15.665
이것은 머신 러닝에서 내가 본 중

205
00 : 15 : 15.665 -> 00 : 15 : 20.010
가장 어려운 도출 중 하나입니다

206
00 : 15 : 20.010 -> 00 : 15 : 22.861
어느 쪽이든 이것을 구현하기만 하면
작동이 잘 될것이고

207
00 : 15 : 22.861 -> 00 : 15 : 27.260
여러분이 이것을 조정하는데
충분한 직관은 가지고 있다고 생각합니다

208
00 : 15 : 27.260 -> 00 : 15 : 30.830
신경망을 구현하기 전에

209
00 : 15 : 30.830 -> 00 : 15 : 34.190
마지막으로 한가지
세세한 것을 가르쳐 드리려고 합니다

210
00 : 15 : 34.190 -> 00 : 15 : 37.720
그것은 신경망의 weight를
초기화하는 방법입니다.

211
00 : 15 : 37.720 -> 00 : 15 : 40.600
매개 변수를 제로가 아닌

212
00 : 15 : 40.600 -> 00 : 15 : 42.560
무작위로 초기화하는 것은

213
00 : 15 : 42.560 -> 00 : 15 : 45.515
신경망을 훈련하는 데 매우 중요합니다.

214
00 : 15 : 45.515 -> 00 : 15 : 48.000
다음 비디오에서 그 이유를 살펴 보겠습니다.