WEBVTT

1
00 : 00 : 00.390 -> 00 : 00 : 04.350
신경망을 만들때

2
00 : 00 : 02.580 -> 00 : 00 : 06.720
필요한 많은 선택 사항 중 하나는

3
00 : 00 : 04.350 -> 00 : 00 : 09.599
숨겨진 레이어나 출력 레이어에 적용되는

4
00 : 00 : 06.720 -> 00 : 00 : 11.490
activation function을 무엇으로 할지 정하는 겁니다.

5
00 : 00 : 09.599 -> 00 : 00 : 13.139
지금까지 우리 모두

6
00 : 00 : 11.490 -> 00 : 00 : 16.080
Sigmiod 함수를 사용해 왔지만

7
00 : 00 : 13.139 -> 00 : 00 : 18.720
다른 활성화 함수를 사용하는 것이 더 나을 때도 있습니다.

8
00 : 00 : 16.080 -> 00 : 00 : 20.939
이러한 옵션 들을 한번

9
00 : 00 : 18.720 -> 00 : 00 : 23.279
살펴 보겠습니다.

10
00 : 00 : 20.939 -> 00 : 00 : 26.099
신경망의 forward propagation의

11
00 : 00 : 23.279 -> 00 : 00 : 28.710
이 두 단계에서 Sigmiod 함수를 사용합니다.

12
00 : 00 : 26.099 -> 00 : 00 : 32.610
따라서 Sigmiod 함수는 이 곳의 
활성화 함수라고도합니다.

13
00 : 00 : 28.710 -> 00 : 00 : 37.590
이것은 Sigmiod 함수 곡선입니다.

14
00 : 00 : 32.610 -> 00 : 00 : 40.680
식은 이렇게 되겠군요

15
00 : 00 : 37.590 -> 00 : 00 : 42.600
일반적인 상황에서

16
00 : 00 : 40.680 -> 00 : 00 : 49.739
우리는 Z에 대한 다른 함수를 사용할 수 있습니다.

17
00 : 00 : 42.600 -> 00 : 00 : 53.309
우리는 그것을 여기에 G라고 씁니다.

18
00 : 00 : 49.739 -> 00 : 00 : 56.010
g는 비선형 함수이고

19
00 : 00 : 53.309 -> 00 : 00 : 59.250
반드시 Sigmiod 함수인건 아닙니다.

20
00 : 00 : 56.010 -> 00 : 01 : 01.879
예를 들어 sigmoid 함수 값의 범위는 0과 1 사이입니다

21
00 : 00 : 59.250 -> 00 : 01 : 04.290
Sigmoid 함수보다 일반적으로

22
00 : 01 : 01.879 -> 00 : 01 : 06.900
더 나은 activation 기능을 하는

23
00 : 01 : 04.290 -> 00 : 01 : 10.320
함수 로는

24
00 : 01 : 06.900 -> 00 : 01 : 14.189
Hyperbolic tangent(쌍곡선) 함수가 있습니다.

25
00 : 01 : 10.320 -> 00 : 01 : 19.979
이것은 z, a 및 z에 관한 tanh 함수이고,

26
00 : 01 : 14.189 -> 00 : 01 : 25.710
함수 값의 범위는 1과 -1 사이입니다.

27
00 : 01 : 19.979 -> 00 : 01 : 31.079
tanh 함수의 수식은 다음과 같습니다.

28
00 : 01 : 25.710 -> 00 : 01 : 37.799


29
00 : 01 : 31.079 -> 00 : 01 : 40.140
tanh 함수의 수식은 다음과 같습니다.

30
00 : 01 : 37.799 -> 00 : 01 : 43.890
이것은 실제로 수학적으로 보면

31
00 : 01 : 40.140 -> 00 : 01 : 46.350
이동 된 sigmoid 함수 입니다.

32
00 : 01 : 43.890 -> 00 : 01 : 49.860
이것은 보통의 sigmoid 함수가

33
00 : 01 : 46.350 -> 00 : 01 : 52.079
이동 후에

34
00 : 01 : 49.860 -> 00 : 01 : 54.570
원점보다 왼쪽의 출력범위를 조정합니다.

35
00 : 01 : 52.079 -> 00 : 01 : 58.530
보시다시피 함수 출력 범위를 
-1에서 1 사이로 만듭니다.

36
00 : 01 : 54.570 -> 00 : 02 : 05.340
Hidden layer의 셀에 이를 적용해서
함수 g를 z에 관해서 표현하면

37
00 : 01 : 58.530 -> 00 : 02 : 09.910
tanh (z)와 같아지고,

38
00 : 02 : 05.340 -> 00 : 02 : 12.490
이 함수의 효과는 Sigmoid 함수를 사용한 결과보다

39
00 : 02 : 09.910 -> 00 : 02 : 14.020
기본적으로 더 좋을 것입니다.
이것은 현재 함수의 값 때문입니다.

40
00 : 02 : 12.490 -> 00 : 02 : 16.930
함수 값이 1에서 -1 사이 이기 때문에,

41
00 : 02 : 14.020 -> 00 : 02 : 19.000
Hidden layer이 활성화 함수 출력의 평균 값이

42
00 : 02 : 16.930 -> 00 : 02 : 21.550
0에 가까워 집니다.

43
00 : 02 : 19.000 -> 00 : 02 : 23.020
이는 당신이 학습 알고리즘을

44
00 : 02 : 21.550 -> 00 : 02 : 23.590
훈련 시킬때

45
00 : 02 : 23.020 -> 00 : 02 : 25.690
데이터를 centering(중앙 집중화)

46
00 : 02 : 23.590 -> 00 : 02 : 29.709
할 수 있다는 의미 입니다.

47
00 : 02 : 25.690 -> 00 : 02 : 31.510
그러니까 Sigmiod 함수 대신 tanh를 사용하십시오.

48
00 : 02 : 29.709 -> 00 : 02 : 34.750
이는 데이터 centering의 효과를 달성하여

49
00 : 02 : 31.510 -> 00 : 02 : 36.880
데이터 중심의 평균을

50
00 : 02 : 34.750 -> 00 : 02 : 39.610
0.5(sigmoid의 평균)가 아니라 
0에 가깝게 만들어 줍니다.

51
00 : 02 : 36.880 -> 00 : 02 : 41.410
이것은 다음 단계의 학습을 

52
00 : 02 : 39.610 -> 00 : 02 : 43.510
더욱 쉽게 만들어줄 것입니다.

53
00 : 02 : 41.410 -> 00 : 02 : 45.820
우리는 두 번째 과정에서 
최적화 알고리즘을 논의 할 때

54
00 : 02 : 43.510 -> 00 : 02 : 47.380
구체적으로 토론 할 것입니다.

55
00 : 02 : 45.820 -> 00 : 02 : 50.739
당신이 알아야 할 것 중 하나는

56
00 : 02 : 47.380 -> 00 : 02 : 52.480
나는 Sigmoid 함수를 거의 
사용하지 않는 다는 겁니다.

57
00 : 02 : 50.739 -> 00 : 02 : 54.250
activation 함수로서

58
00 : 02 : 52.480 -> 00 : 02 : 56.410
sigmoid는 가능하면 사용하지 마십시오.

59
00 : 02 : 54.250 -> 00 : 02 : 59.560
tanh 함수에 거의 대부분의 기능이 있으면서도
sigmoid보다 확실히 뛰어납니다.

60
00 : 02 : 56.410 -> 00 : 03 : 03.550
한 가지 예외는

61
00 : 02 : 59.560 -> 00 : 03 : 07.420
y 값이 0 또는 1만 나오는 경우에는

62
00 : 03 : 03.550 -> 00 : 03 : 10.570
출력 레이어 에서의 y hat 값도

63
00 : 03 : 07.420 -> 00 : 03 : 13.989
0과 1 사이로 출력되는 것이

64
00 : 03 : 10.570 -> 00 : 03 : 16.570
바람직합니다.

65
00 : 03 : 13.989 -> 00 : 03 : 19.360
-1과 1 사이가 아니라 말이죠.

66
00 : 03 : 16.570 -> 00 : 03 : 21.430
이처럼 binary classification

67
00 : 03 : 19.360 -> 00 : 03 : 24.670
에서는 활성화 함수로서

68
00 : 03 : 21.430 -> 00 : 03 : 26.350
Sigmoid 활성화 함수를

69
00 : 03 : 24.670 -> 00 : 03 : 29.709
사용할 수 있습니다.

70
00 : 03 : 26.350 -> 00 : 03 : 35.170
sigmoid를 사용한다면 출력 레이어

71
00 : 03 : 29.709 -> 00 : 03 : 37.180
z2에 대한 함수 g는 sigmoid (z2)가 될겁니다.

72
00 : 03 : 35.170 -> 00 : 03 : 40.299
여기에 표시되는 예제에서는

73
00 : 03 : 37.180 -> 00 : 03 : 43.920
Hidden layer 에서는

74
00 : 03 : 40.299 -> 00 : 03 : 47.769
활성화 함수로서 tanh를 사용했고,

75
00 : 03 : 43.920 -> 00 : 03 : 49.299
출력 레이어의 활성화 함수로는 
sigmoid를 사용했습니다.

76
00 : 03 : 47.769 -> 00 : 03 : 51.670
따라서 서로 다른 계층은 서로 다른 
활성화 함수를 사용했습니다..

77
00 : 03 : 49.299 -> 00 : 03 : 53.709
때로는 각 레이어에서 사용되는 활성화 함수가

78
00 : 03 : 51.670 -> 00 : 03 : 55.690
다르다는 것을

79
00 : 03 : 53.709 -> 00 : 03 : 58.510
표시해야 합니다.

80
00 : 03 : 55.690 -> 00 : 04 : 02.230
이를 표시할 때, []를 사용할 수 있습니다.

81
00 : 03 : 58.510 -> 00 : 04 : 04.540
G [1]과

82
00 : 04 : 02.230 -> 00 : 04 : 06.940
G [2]는 같은 함수가 
아닐 수도 있다는걸 나타냅니다.

83
00 : 04 : 04.540 -> 00 : 04 : 09.340
이 예제에서는
84
00 : 04 : 06.940 -> 00 : 04 : 11.470
[1]은 Hidden layer를 의미하고,

85
00 : 04 : 09.340 -> 00 : 04 : 12.879
[2]는 출력 레이어를

86
00 : 04 : 11.470 -> 00 : 04 : 15.680
의미합니다.

87
00 : 04 : 12.879 -> 00 : 04 : 18.109
sigmoid 와 tanh 함수의

88
00 : 04 : 15.680 -> 00 : 04 : 20.780
단점 중 하나는

89
00 : 04 : 18.109 -> 00 : 04 : 22.910
Z 값이 매우 크거나 매우 작은 경우

90
00 : 04 : 20.780 -> 00 : 04 : 24.460
함수의 미분의 기울기 또는

91
00 : 04 : 22.910 -> 00 : 04 : 27.560
경사가 매우 작아집니다.

92
00 : 04 : 24.460 -> 00 : 04 : 30.139
Z가 매우 크거나 작을 때

93
00 : 04 : 27.560 -> 00 : 04 : 33.169
함수의 기울기는

94
00 : 04 : 30.139 -> 00 : 04 : 35.270
0에 가까워 집니다.

95
00 : 04 : 33.169 -> 00 : 04 : 38.360
이렇게하면 gradient descent의 속도가 느려집니다.

96
00 : 04 : 35.270 -> 00 : 04 : 41.810
이러한 문제를 해결하기 위해,
Machine learning 커뮤니티에서 매우 인기 있는

97
00 : 04 : 38.360 -> 00 : 04 : 44.900
rectified linear unit(ReLu) 이라고 불리는 
activation 함수 가 있습니다.

98
00 : 04 : 41.810 -> 00 : 04 : 50.720
ReLu는 다음과 같이 생긴 함수 입니다.

99
00 : 04 : 44.900 -> 00 : 04 : 57.110
ReLU의 공식은

100
00 : 04 : 50.720 -> 00 : 05 : 00.500
a = 0과 z 중 큰 값과 같습니다.

101
00 : 04 : 57.110 -> 00 : 05 : 03.530
z가 양수이면 미분 값은 1입니다.

102
00 : 05 : 00.500 -> 00 : 05 : 05.990
한편, z가 음수이면,

103
00 : 05 : 03.530 -> 00 : 05 : 07.580
기울기 또는 미분 값은 0입니다.

104
00 : 05 : 05.990 -> 00 : 05 : 10.190
기술적인 괌점에서 보면 사실

105
00 : 05 : 07.580 -> 00 : 05 : 12.349
z의 값이 정확히 0인 지점은 존재하지 않습니다.

106
00 : 05 : 10.190 -> 00 : 05 : 14.210
하지만 컴퓨터에 구현하면

107
00 : 05 : 12.349 -> 00 : 05 : 18.770
일반적으로

108
00 : 05 : 14.210 -> 00 : 05 : 21.229
z는 0.00000.....의 매우 작은 값입니다
(0이라고 봐도 무방할 만큼)

109
00 : 05 : 18.770 -> 00 : 05 : 22.940
그래서 걱정할 필요가 없습니다.
(z가 0 일 때)

110
00 : 05 : 21.229 -> 00 : 05 : 25.610
그래서 실제 적용에서는

111
00 : 05 : 22.940 -> 00 : 05 : 29.659
z가 0이라고 생각하시고 하면 될겁니다.

112
00 : 05 : 25.610 -> 00 : 05 : 32.270
그래서 미분 값은

113
00 : 05 : 29.659 -> 00 : 05 : 35.479
1 또는 0이라고 해도

114
00 : 05 : 32.270 -> 00 : 05 : 37.430
이것에 큰 문제는 없습니다.

115
00 : 05 : 35.479 -> 00 : 05 : 40.010
다음은 활성화 함수를 선택하는데 있어서

116
00 : 05 : 37.430 -> 00 : 05 : 43.280
몇 가지 일반적인 규칙입니다.

117
00 : 05 : 40.010 -> 00 : 05 : 45.620
당신이 binary classification을 활용해서
출력 값이 0과 1 이라면,

118
00 : 05 : 43.280 -> 00 : 05 : 47.539
sigmoid 활성화 함수를 출력 레이어에

119
00 : 05 : 45.620 -> 00 : 05 : 50.479
사용하는 것이 합리적인 선택입니다.

120
00 : 05 : 47.539 -> 00 : 05 : 59.419
하지만 다른 뉴런에서는 ReLU(rectified linear unit)

121
00 : 05 : 50.479 -> 00 : 06 : 04.460
를 사용하는 것이

122
00 : 05 : 59.419 -> 00 : 06 : 07.190
더 나은 선택 일 수 있습니다.

123
00 : 06 : 04.460 -> 00 : 06 : 10.280
Hidden layer에 사용할 

124
00 : 06 : 07.190 -> 00 : 06 : 13.849
활성화 함수를 잘 모르겠는 경우에는

125
00 : 06 : 10.280 -> 00 : 06 : 15.289
ReLu를 사용하는 것이 좋습니다.

126
00 : 06 : 13.849 -> 00 : 06 : 17.570
ReLU는 오늘날 사람들이 널리 사용하는 방법입니다.

127
00 : 06 : 15.289 -> 00 : 06 : 20.120
때로는

128
00 : 06 : 17.570 -> 00 : 06 : 21.350
사람들은 tanh 함수도 사용합니다.

129
00 : 06 : 20.120 -> 00 : 06 : 23.150
활성화 함수로서

130
00 : 06 : 21.350 -> 00 : 06 : 26.270
ReLU의 단점 중 하나는

131
00 : 06 : 23.150 -> 00 : 06 : 28.640
z가 음수 일 때

132
00 : 06 : 26.270 -> 00 : 06 : 31.700
그 미분 값이 0이라는 것입니다, 
이는 일반적인 활용에서는 문제가 되지 않습니다.

133
00 : 06 : 28.640 -> 00 : 06 : 33.890
그러나 ReLU의 다른 버전이 있습니다.

134
00 : 06 : 31.700 -> 00 : 06 : 35.420
Leaky Relu라고하는

135
00 : 06 : 33.890 -> 00 : 06 : 38.690
다음 슬라이드에서 수식을 제공합니다.

136
00 : 06 : 35.420 -> 00 : 06 : 40.520
z가 음수일때, 함수 값이 0이 아닙니다.

137
00 : 06 : 38.690 -> 00 : 06 : 42.940
여기에 아주 작은 경사가 있기 때문입니다.

138
00 : 06 : 40.520 -> 00 : 06 : 47.900
이것은 Leaky ReLU라고 불립니다.

139
00 : 06 : 42.940 -> 00 : 06 : 51.170
Leaky ReLU는 일반적으로

140
00 : 06 : 47.900 -> 00 : 06 : 53.900
ReLU 활성화 함수 보다

141
00 : 06 : 51.170 -> 00 : 06 : 54.860
실제로는 상대적으로 덜 사용됩니다.

142
00 : 06 : 53.900 -> 00 : 06 : 56.770
이것은 당신이 어느 것을 사용하든 문제가 크지 않기 때문입니다.

143
00 : 06 : 54.860 -> 00 : 06 : 59.330
두 가지 중 하나를 선택해야하는 경우

144
00 : 06 : 56.770 -> 00 : 07 : 01.460
나는 보통 ReLU를 사용합니다.

145
00 : 06 : 59.330 -> 00 : 07 : 04.460
ReLU와 Leaky ReLU의 공통된 이점은

146
00 : 07 : 01.460 -> 00 : 07 : 06.500
모든 z값에 대해서
 
147
00 : 07 : 04.460 -> 00 : 07 : 08.150
활성화 함수의 미분 또는

148
00 : 07 : 06.500 -> 00 : 07 : 11.870
활성화 함수의 기울기 값이

149
00 : 07 : 08.150 -> 00 : 07 : 13.970
0을 기준으로 확연하게 달라집니다.

150
00 : 07 : 11.870 -> 00 : 07 : 15.920
이 때문에,

151
00 : 07 : 13.970 -> 00 : 07 : 18.590
ReLU 활성화 함수를 활용하면

152
00 : 07 : 15.920 -> 00 : 07 : 20.810
신경망의 학습 속도는 일반적으로

153
00 : 07 : 18.590 -> 00 : 07 : 23.840
tanh 또는 Sigmoid 함수 보다 더 빨라집니다.

154
00 : 07 : 20.810 -> 00 : 07 : 26.420
주된 이유는 tanh 또는 Sigmoid 함수는 
학습을 늦추는 기울기가 0이되는

155
00 : 07 : 23.840 -> 00 : 07 : 28.700
지점이 있기 때문입니다.

156
00 : 07 : 26.420 -> 00 : 07 : 31.580
때문에 이 지점들의 미분 값은 0이되어 
학습 속도를 감소시킵니다.

157
00 : 07 : 28.700 -> 00 : 07 : 33.950
우리는 z의 값 범위의 음수쪽 절반에서

158
00 : 07 : 31.580 -> 00 : 07 : 36.710
ReLU의 기울기가 0 인걸 알고 있습니다.

159
00 : 07 : 33.950 -> 00 : 07 : 39.050
그러나 실제 사용시

160
00 : 07 : 36.710 -> 00 : 07 : 41.120
대부분의 Hidden unit 에서는 
z 값이 0 보다 크게 됩니다.

161
00 : 07 : 39.050 -> 00 : 07 : 43.700
때문에 이는 학습 속도에 큰 영향을 주진 않습니다.

162
00 : 07 : 41.120 -> 00 : 07 : 45.800
빠르게 복습 하겠습니다.

163
00 : 07 : 43.700 -> 00 : 07 : 47.600
다양한 활성화 함수의 장점과 단점을 배웠습니다.

164
00 : 07 : 45.800 -> 00 : 07 : 50.030
이것은 Sigmoid 활성화 함수입니다.

165
00 : 07 : 47.600 -> 00 : 07 : 52.790
저는 아까도 말했지만,

166
00 : 07 : 50.030 -> 00 : 07 : 54.410
이 함수는 가능하면 사용하지 마십시오

167
00 : 07 : 52.790 -> 00 : 07 : 56.330
당신이 binary classification을 
해결하려고 하는게 아니라면

168
00 : 07 : 54.410 -> 00 : 07 : 59.540
이 함수를 사용하지 마십시오.

169
00 : 07 : 56.330 -> 00 : 08 : 02.720
이 함수를 거의 사용하지 않는 이유는

170
00 : 07 : 59.540 -> 00 : 08 : 05.060
tanh 함수가 훨씬 좋기 때문입니다.

171
00 : 08 : 02.720 -> 00 : 08 : 12.080
Tanh 활성화 함수는 이렇게 되고,

172
00 : 08 : 05.060 -> 00 : 08 : 13.430
말씀드렸듯이, sigmoid 보다는 
tanh를 쓰시는게 좋습니다.

173
00 : 08 : 12.080 -> 00 : 08 : 15.490
그리고 가장 자주 사용되는 기본 활성화 함수는

174
00 : 08 : 13.430 -> 00 : 08 : 19.100
ReLU 함수 입니다.

175
00 : 08 : 15.490 -> 00 : 08 : 23.660
특히 좋은 선택이 없을 때
ReLU를 사용할 수 있습니다.

176
00 : 08 : 19.100 -> 00 : 08 : 26.600
Leaky ReLU 함수를

177
00 : 08 : 23.660 -> 00 : 08 : 31.930
사용하는 것도 좋은 선택일겁니다.

178
00 : 08 : 26.600 -> 00 : 08 : 36.659
leaky Relu의 식은

179
00 : 08 : 31.930 -> 00 : 08 : 40.390
a = Max(0.01z, z) 입니다.

180
00 : 08 : 36.659 -> 00 : 08 : 43.810
0.01z는 함수를 약간 기울어지게 만듭니다.

181
00 : 08 : 40.390 -> 00 : 08 : 46.200
당신은 왜 꼭 0.01을 사용해야 하는건지

182
00 : 08 : 43.810 -> 00 : 08 : 51.670
의문을 제기할 수도 있습니다.

183
00 : 08 : 46.200 -> 00 : 08 : 53.380
당신이 알고리즘 상에서

184
00 : 08 : 51.670 -> 00 : 08 : 54.670
다른 숫자를 생각 할 수도 있을 겁니다.

185
00 : 08 : 53.380 -> 00 : 08 : 58.480
어떤 사람들은 다른 숫자가 더 잘 작동된다고 주장하지만

186
00 : 08 : 54.670 -> 00 : 08 : 59.649
저는 실제로 더 잘 작동하는 
예를 거의 보지 못했습니다.

187
00 : 08 : 58.480 -> 00 : 09 : 01.360
응용 프로그램에서 시험해보고 싶다면

188
00 : 08 : 59.649 -> 00 : 09 : 03.430
그것을 시도하는 자세는 굉장히 훌륭합니다.

189
00 : 09 : 01.360 -> 00 : 09 : 05.800
잘 작동하는지 확인 하셨습니까?

190
00 : 09 : 03.430 -> 00 : 09 : 08.290
만약 잘 작동하는 경우 
그 숫자를 계속 사용하십시오.

191
00 : 09 : 05.800 -> 00 : 09 : 09.880
이 수업에서는

192
00 : 09 : 08.290 -> 00 : 09 : 11.620
여러분의 신경망에 활용될

193
00 : 09 : 09.880 -> 00 : 09 : 13.870
활성화 함수의 선택에 도움을 드렸습니다.

194
00 : 09 : 11.620 -> 00 : 09 : 15.940
사실, 딥러닝 에서

195
00 : 09 : 13.870 -> 00 : 09 : 18.130
여러분은 선택하셔야할 일이 굉장히 많습니다.

196
00 : 09 : 15.940 -> 00 : 09 : 20.110
신경망을 구축 하는 과정에서

197
00 : 09 : 18.130 -> 00 : 09 : 22.089
Hidden layer의 유닛 수,

198
00 : 09 : 20.110 -> 00 : 09 : 24.430
활성화 함수의 선택,

199
00 : 09 : 22.089 -> 00 : 09 : 25.839
추후에 얘기할 weight를 초기화 하는 방법등

200
00 : 09 : 24.430 -> 00 : 09 : 28.480
많은 선택이 있습니다.

201
00 : 09 : 25.839 -> 00 : 09 : 30.880
대부분의 경우,

202
00 : 09 : 28.480 -> 00 : 09 : 33.279
실제로 가장 잘 적용되는

203
00 : 09 : 30.880 -> 00 : 09 : 35.649
방법을 선택하는 것은

204
00 : 09 : 33.279 -> 00 : 09 : 37.270
쉽지 않습니다.

205
00 : 09 : 35.649 -> 00 : 09 : 39.070
이 일련의 과정을 통해

206
00 : 09 : 37.270 -> 00 : 09 : 40.839
어느 것이 더 인기가 있고 덜 인기가 있는지

207
00 : 09 : 39.070 -> 00 : 09 : 43.450
실제 산업에서 제가 본 것을 알려 드릴겁니다.

208
00 : 09 : 40.839 -> 00 : 09 : 45.520
그러나 여러분의

209
00 : 09 : 43.450 -> 00 : 09 : 46.930
어플리케이션에

210
00 : 09 : 45.520 -> 00 : 09 : 49.450
어떤게 가장 적합할지는

211
00 : 09 : 46.930 -> 00 : 09 : 51.400
사실, 사전에 결정하기가 어렵습니다

212
00 : 09 : 49.450 -> 00 : 09 : 52.930
제가 드릴수 있는 조언은

213
00 : 09 : 51.400 -> 00 : 09 : 54.940
어떤 선택을 해야할지 잘 모르는 경우

214
00 : 09 : 52.930 -> 00 : 09 : 57.700
하나씩 시도해 보라는 겁니다.

215
00 : 09 : 54.940 -> 00 : 10 : 00.010
우리가 나중에 배울

216
00 : 09 : 57.700 -> 00 : 10 : 02.529
holdout validation set 또는 
development set 등을 이용해서 

217
00 : 10 : 00.010 -> 00 : 10 : 04.480
어느 것이 가장 적합한 방법인지

218
00 : 10 : 02.529 -> 00 : 10 : 08.350
확인해 보시면 될겁니다.

219
00 : 10 : 04.480 -> 00 : 10 : 10.180
여러분의 어플리케이션에 이러한 

220
00 : 10 : 08.350 -> 00 : 10 : 13.510
다양한 옵션들을 적용해보는 연습을 한다면

221
00 : 10 : 10.180 -> 00 : 10 : 16.240
여러분은 미래 지향적 신경망을 설계 하는데 있어서

222
00 : 10 : 13.510 -> 00 : 10 : 18.130
더 나을 수 있습니다.

223
00 : 10 : 16.240 -> 00 : 10 : 20.550
문제에 해당하는 함수 를 선택하는데 있어서,

224
00 : 10 : 18.130 -> 00 : 10 : 23.440
제가 여러분에게 예를 들어,

225
00 : 10 : 20.550 -> 00 : 10 : 25.630
ReLU 활성화 함수만 사용하고

226
00 : 10 : 23.440 -> 00 : 10 : 27.339
다른 방법을 사용하지 마십시오.

227
00 : 10 : 25.630 -> 00 : 10 : 29.440
라고 말하는 것은 반드시 정확하지는 않기 때문입니다.

228
00 : 10 : 27.339 -> 00 : 10 : 30.790
가까운  또는 먼 미래를 보더라도 말이죠.

229
00 : 10 : 29.440 -> 00 : 10 : 32.410
좋습니다. 여기서 해결한 문제는

230
00 : 10 : 30.790 -> 00 : 10 : 36.220
활성화 함수의 선택에 관한

231
00 : 10 : 32.410 -> 00 : 10 : 37.870
내용 이었습니다.

232
00 : 10 : 36.220 -> 00 : 10 : 39.310
그리고 당신은 가장 인기있는 
함수를 보았습니다.

233
00 : 10 : 37.870 -> 00 : 10 : 41.459
몇 가지 활성화 함수에 대해서

234
00 : 10 : 39.310 -> 00 : 10 : 44.260
자주 언급되는 문제도 있습니다.

235
00 : 10 : 41.459 -> 00 : 10 : 45.160
이게 왜 필요한 거지?

236
00 : 10 : 44.260 -> 00 : 10 : 46.959
활성화 함수를

237
00 : 10 : 45.160 -> 00 : 10 : 49.240
왜 꼭 사용해야만 하는거지?

238
00 : 10 : 46.959 -> 00 : 10 : 49.779
우리는 다음 비디오에서 논의 할 것입니다.

239
00 : 10 : 49.240 -> 00 : 10 : 52.240
여러분은 왜 신경망을 설계 하는데 있어서

240
00 : 10 : 49.779 -> 00 : 10 : 54.430
이와 같은 비선형함수들이

241
00 : 10 : 52.240 -> 00 : 10 : 58.259
왜 필요한지에 대해

242
00 : 10 : 54.430 -> 00 : 10 : 58.259
이해하게 될것입니다.