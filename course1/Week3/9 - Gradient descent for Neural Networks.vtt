WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 03.840
저는 이것이 흥미로운 비디오 일 것이라고 믿습니다.

2
00 : 00 : 01.800 -> 00 : 00 : 06.240
이 비디오에서 우리는

3
00 : 00 : 03.840 -> 00 : 00 : 08.730
1개의 hidden layer를 가진 신경 네트워크에서

4
00 : 00 : 06.240 -> 00 : 00 : 10.530
어떻게 gradient descent를 실행하는지 볼겁니다.

5
00 : 00 : 08.730 -> 00 : 00 : 12.809
이 비디오에서 저는 gradient descent의


6
00 : 00 : 10.530 -> 00 : 00 : 14.639
back propagation을 실행하기 위해

7
00 : 00 : 12.809 -> 00 : 00 : 17.039
필요한 방정식을 

8
00 : 00 : 14.639 -> 00 : 00 : 19.410
알려 드릴 겁니다.

9
00 : 00 : 17.039 -> 00 : 00 : 21.510
다음 동영상에서는 저는

10
00 : 00 : 19.410 -> 00 : 00 : 24.150
이 방정식이 왜

11
00 : 00 : 21.510 -> 00 : 00 : 26.070
당신 신경망의 gradient를

12
00 : 00 : 24.150 -> 00 : 00 : 27.630
계산하기 위해

13
00 : 00 : 26.070 -> 00 : 00 : 28.289
올바른 방정식인지에 대한

14
00 : 00 : 27.630 -> 00 : 00 : 30.090
직관적인 부분을 알려 드릴겁니다.

15
00 : 00 : 28.289 -> 00 : 00 : 32.520
이제 하나의 Hidden 유닛이 있는

16
00 : 00 : 30.090 -> 00 : 00 : 39.930
신경망이 있다고 해봅시다.

17
00 : 00 : 32.520 -> 00 : 00 : 44.760
매개 변수는 W1 b1 W2 b2 가 될겁니다.

18
00 : 00 : 39.930 -> 00 : 00 : 50.399
이들은 nx의 차원을 가질 겁니다

19
00 : 00 : 44.760 -> 00 : 00 : 56.640
즉  N0는 입력 feature,

20
00 : 00 : 50.399 -> 00 : 00 : 59.149
N1은 Hidden unit, 
n2는 output unit의 차원입니다.

21
00 : 00 : 56.640 -> 00 : 01 : 05.670
이 예에서 n2 = 1 입니다.

22
00 : 00 : 59.149 -> 00 : 01 : 08.880
그러면 행렬 W1은 n1 * n0로 표현되고

23
00 : 01 : 05.670 -> 00 : 01 : 11.250
B1은 n1 차원의

24
00 : 01 : 08.880 -> 00 : 01 : 13.350
열벡터가 될거니까,

25
00 : 01 : 11.250 -> 00 : 01 : 16.500
n1 * 1 행렬이 됩니다.

26
00 : 01 : 13.350 -> 00 : 01 : 20.750
W2의 차원은 n2 * n1입니다.

27
00 : 01 : 16.500 -> 00 : 01 : 26.759
b2의 차원은 n2 * 1입니다.

28
00 : 01 : 20.750 -> 00 : 01 : 28.590
이 예에서

29
00 : 01 : 26.759 -> 00 : 01 : 30.930
N2 = 1 입니다.

30
00 : 01 : 28.590 -> 00 : 01 : 36.930
즉, 숨겨진 유닛이 하나뿐입니다.

31
00 : 01 : 30.930 -> 00 : 01 : 39.570
그리고 이 신경망에서

32
00 : 01 : 36.930 -> 00 : 01 : 41.340
우리는 또한 손실 함수도 생각할 수 있습니다. 

33
00 : 01 : 39.570 -> 00 : 01 : 44.220
이제 우리가 binary classification 문제를
34
00 : 01 : 41.340 -> 00 : 01 : 48.659
해결 하고 있다고 가정 해 봅시다.

35
00 : 01 : 44.220 -> 00 : 01 : 51.740
그런 다음 매개 변수의 전체 손실값은

36
00 : 01 : 48.659 -> 00 : 01 : 57.090
1 / m 을 곱한 값인

37
00 : 01 : 51.740 -> 00 : 01 : 59.969
손실 값들의 평균이 될 것입니다.

38
00 : 01 : 57.090 -> 00 : 02 : 03.420
여기서 손실함수 L은

39
00 : 01 : 59.969 -> 00 : 02 : 06.240
실제 레이블이 y 일 때,

40
00 : 02 : 03.420 -> 00 : 02 : 07.649
모델이 y hat(즉, a2)을 예측하는 경우 입니다.

41
00 : 02 : 06.240 -> 00 : 02 : 09.629
그리고 binary classification를 수행 할 때의

42
00 : 02 : 07.649 -> 00 : 02 : 12.510
손실 함수는

43
00 : 02 : 09.629 -> 00 : 02 : 15.030
로지스틱 회귀와 같아집니다.

44
00 : 02 : 12.510 -> 00 : 02 : 18.420
그리고 알고리즘의 매개 변수를 훈련시키기 위해서

45
00 : 02 : 15.030 -> 00 : 02 : 21.450
gradient descent를 사용해야합니다.

46
00 : 02 : 18.420 -> 00 : 02 : 23.189
우리가 신경 네트워크를 훈련 할 때

47
00 : 02 : 21.450 -> 00 : 02 : 25.379
매개 변수를 0 근방의 값으로

48
00 : 02 : 23.189 -> 00 : 02 : 26.129
설정하는 것은 매우 중요합니다.

49
00 : 02 : 25.379 -> 00 : 02 : 28.140
이게 왜 중요한지에 대해서는

50
00 : 02 : 26.129 -> 00 : 02 : 30.030
나중에 더 얘기 할 것입니다.

51
00 : 02 : 28.140 -> 00 : 02 : 32.069
매개 변수를 초기화한 후에

52
00 : 02 : 30.030 -> 00 : 02 : 34.140
gradient descent의 각 사이클들은

53
00 : 02 : 32.069 -> 00 : 02 : 36.780
샘플에 대한 예측값을 계산할 겁니다.

54
00 : 02 : 34.140 -> 00 : 02 : 42.359
그래서 우리는 y hat i을 계산합니다.

55
00 : 02 : 36.780 -> 00 : 02 : 44.519
i는 1에서 m까지가 될것이구요.

56
00 : 02 : 42.359 -> 00 : 02 : 49.440
미분 값도 계산을 해야 하니까

57
00 : 02 : 44.519 -> 00 : 02 : 51.420
dW1을 계산하게 됩니다.

58
00 : 02 : 49.440 -> 00 : 02 : 54.359
손실 함수의 변수인

59
00 : 02 : 51.420 -> 00 : 02 : 56.489
W1의 미분 값을 계산하려면

60
00 : 02 : 54.359 -> 00 : 02 : 59.220
또 다른 변수인

61
00 : 02 : 56.489 -> 00 : 03 : 00.870
Db1을 계산해야 됩니다.

62
00 : 02 : 59.220 -> 00 : 03 : 04.109
Db1은 B1에 관한 손실함수의

63
00 : 03 : 00.870 -> 00 : 03 : 07.349
미분 값을 의미합니다.

64
00 : 03 : 04.109 -> 00 : 03 : 10.170
이와 비슷하게

65
00 : 03 : 07.349 -> 00 : 03 : 12.629
W2 및 B2에 대한 미분값도 계산 하시면 됩니다.

66
00 : 03 : 10.170 -> 00 : 03 : 17.879
마지막으로 gradient descent에서

67
00 : 03 : 12.629 -> 00 : 03 : 22.709
W1은 W1에서 (learning rate)α곱하기 dw1을 

68
00 : 03 : 17.879 -> 00 : 03 : 26.129
뺀 값으로 업데이트됩니다.

69
00 : 03 : 22.709 -> 00 : 03 : 31.620
B1은 b1에서 (learning rate)α 곱하기 db1
을 뺀 값으로 업데이트됩니다.

70
00 : 03 : 26.129 -> 00 : 03 : 34.739
마찬가지로 W2 및 b2 도 업데이트 됩니다.

71
00 : 03 : 31.620 -> 00 : 03 : 36.299
콜론 등호를 써도 되고

72
00 : 03 : 34.739 -> 00 : 03 : 38.489
때로는 등호를 써도 됩니다.

73
00 : 03 : 36.299 -> 00 : 03 : 40.829
두 가지 방법 모두 가능합니다.

74
00 : 03 : 38.489 -> 00 : 03 : 42.510
이것이 gradient descent의 단일 사이클입니다.

75
00 : 03 : 40.829 -> 00 : 03 : 44.280
매개 변수가 수렴 할 때까지

76
00 : 03 : 42.510 -> 00 : 03 : 46.079
이 사이클을 여러 번 반복하십시오.

77
00 : 03 : 44.280 -> 00 : 03 : 48.150
이전 비디오에서

78
00 : 03 : 46.079 -> 00 : 03 : 50.099
예측 값을 계산하는 방법에 대해 얘기했습니다.

79
00 : 03 : 48.150 -> 00 : 03 : 51.629
그것이 출력을 계산하는 방법입니다.

80
00 : 03 : 50.099 -> 00 : 03 : 54.060
벡터화하는 방법도 소개했습니다.

81
00 : 03 : 51.629 -> 00 : 03 : 56.269
그러나 핵심은

82
00 : 03 : 54.060 -> 00 : 04 : 00.180
dw1 db1 dw2 db2와 같은

83
00 : 03 : 56.269 -> 00 : 04 : 04.079
부분적인 미분 값들의 계산 방법을

84
00 : 04 : 00.180 -> 00 : 04 : 06.780
이해하는 것입니다.

85
00 : 04 : 04.079 -> 00 : 04 : 09.419
저는 이러한 미분을 계산하는 방정식을

86
00 : 04 : 06.780 -> 00 : 04 : 12.150
직접 여러분에게 알려드리려고 합니다.

87
00 : 04 : 09.419 -> 00 : 04 : 14.699
저는 다음에 있는 optional 비디오에서

88
00 : 04 : 12.150 -> 00 : 04 : 17.430
미분하는 방법에 대한 이 공식들이

89
00 : 04 : 14.699 -> 00 : 04 : 19.090
어떻게 도출 됬는지에 대한

90
00 : 04 : 17.430 -> 00 : 04 : 21.400
자세한 설명을 해드릴 겁니다.

91
00 : 04 : 19.090 -> 00 : 04 : 26.169
이제 Forward propagation에 대한

92
00 : 04 : 21.400 -> 00 : 04 : 33.250
방정식을 요약해 보겠습니다.

93
00 : 04 : 26.169 -> 00 : 04 : 37.900
Z1 = W1X + b1

94
00 : 04 : 33.250 -> 00 : 04 : 41.680
A1은 이 레이어의 활성화 함수입니다.

95
00 : 04 : 37.900 -> 00 : 04 : 49.690
Z1을 이곳에 대입합니다.

96
00 : 04 : 41.680 -> 00 : 04 : 53.530
그리고, Z2 = W2A1 + B2

97
00 : 04 : 49.690 -> 00 : 04 : 55.180
이들 중 마지막은

98
00 : 04 : 53.530 -> 00 : 05 : 01.210
(물론 이 데이터들은 모두 벡터화 되어 있습니다.)

99
00 : 04 : 55.180 -> 00 : 05 : 02.740
A2 = g2 (Z2)

100
00 : 05 : 01.210 -> 00 : 05 : 04.870
우리가 binary classification을 한다고 가정합니다.

101
00 : 05 : 02.740 -> 00 : 05 : 06.610
그렇다면 활성화 함수는

102
00 : 05 : 04.870 -> 00 : 05 : 08.560
sigmoid 함수 여야합니다.

103
00 : 05 : 06.610 -> 00 : 05 : 11.080
그래서 이렇게 쓰겠습니다.

104
00 : 05 : 08.560 -> 00 : 05 : 13.870
그래요, 이것이 forward propagation입니다.

105
00 : 05 : 11.080 -> 00 : 05 : 15.729
여러분의 신경망에서

106
00 : 05 : 13.870 -> 00 : 05 : 18.430
왼쪽에서 오른쪽으로 계산되는 과정입니다.

107
00 : 05 : 15.729 -> 00 : 05 : 24.750
이제 미분 값을 계산해 봅시다.

108
00 : 05 : 18.430 -> 00 : 05 : 30.750
즉, back propagation 입니다.

109
00 : 05 : 24.750 -> 00 : 05 : 33.610
dZ2는 A2와 실제 레이블 값인 Y를 
뺀 값과 같습니다.

110
00 : 05 : 30.750 -> 00 : 05 : 36.580
기억을 되살려 봅시다.

111
00 : 05 : 33.610 -> 00 : 05 : 41.289
이 예제에서는 벡터화를 완료했습니다.

112
00 : 05 : 36.580 -> 00 : 05 : 45.280
따라서 행렬 Y는 m 개의 모든 샘플을

113
00 : 05 : 41.289 -> 00 : 05 : 51.370
가로로 나열한 1 * m 행렬입니다.

114
00 : 05 : 45.280 -> 00 : 05 : 55.330
다음 dW2는 이 공식과 같습니다.

115
00 : 05 : 51.370 -> 00 : 05 : 58.870
실제로 여기 있는 세 가지 수식은

116
00 : 05 : 55.330 -> 00 : 06 : 00.900
로지스틱 회귀의 gradient descent와

117
00 : 05 : 58.870 -> 00 : 06 : 00.900
상당히 유사합니다.

118
00 : 06 : 00.960 -> 00 : 06 : 12.610
(축 = 1, keepdims = True)

119
00 : 06 : 07.419 -> 00 : 06 : 15.580
여기서 작은 세부 사항을 말씀 드리겠습니다.

120
00 : 06 : 12.610 -> 00 : 06 : 18.070
이 np.sum은 파이썬 numpy 지시어입니다.

121
00 : 06 : 15.580 -> 00 : 06 : 21.100
이는 행렬의 특정 차원에 작용합니다.

122
00 : 06 : 18.070 -> 00 : 06 : 24.810
이 경우에는 가로로 합계 하는 겁니다.

123
00 : 06 : 21.100 -> 00 : 06 : 27.600
그리고 여기 DIMMs는

124
00 : 06 : 24.810 -> 00 : 06 : 31.230
(n,)와 같은 차원을 가진

125
00 : 06 : 27.600 -> 00 : 06 : 34.650
rank1 array의 출력을

126
00 : 06 : 31.230 -> 00 : 06 : 37.010
방지합니다.

127
00 : 06 : 34.650 -> 00 : 06 : 41.280
그래서 keepdims = True로 설정하십시오.

128
00 : 06 : 37.010 -> 00 : 06 : 44.580
이는 파이썬의 db2 출력이 (n, 1) 차원임을 
보장 할 수 있습니다.

129
00 : 06 : 41.280 -> 00 : 06 : 47.820
엄밀히 말하면, 여기는 N[2] * 1로 쓰여져야 합니다.

130
00 : 06 : 44.580 -> 00 : 06 : 50.130
하지만 이 경우에는  1 * 1 즉 그냥 숫자가

131
00 : 06 : 47.820 -> 00 : 06 : 53.520
쓰여지면 됩니다.

132
00 : 06 : 50.130 -> 00 : 06 : 56.790
지금은 보이지 않을 수도 있습니다.

133
00 : 06 : 53.520 -> 00 : 06 : 58.500
하지만 나중에 중요하게 설명 할 것입니다.

134
00 : 06 : 56.790 -> 00 : 07 : 01.320
지금까지 우리가 한 과정은 
로지스틱 회귀와 매우 흡사했습니다.

135
00 : 06 : 58.500 -> 00 : 07 : 03.919
그러나 back propagation을 실행할 때까지 계산을 계속한다면

136
00 : 07 : 01.320 -> 00 : 07 : 14.370
계산을 계속한다면

137
00 : 07 : 03.919 -> 00 : 07 : 19.380
이 식을 계산 해야합니다.

138
00 : 07 : 14.370 -> 00 : 07 : 20.880
여기 g1 ' 은

139
00 : 07 : 19.380 -> 00 : 07 : 22.919
숨겨진 레이어의 활성화 함수를

140
00 : 07 : 20.880 -> 00 : 07 : 25.770
미분한 값 입니다.

141
00 : 07 : 22.919 -> 00 : 07 : 27.030
출력 레이어의 경우

142
00 : 07 : 25.770 -> 00 : 07 : 29.400
여러분이 sigmoid 함수를 활용해서

143
00 : 07 : 27.030 -> 00 : 07 : 30.780
binary classification을 

144
00 : 07 : 29.400 -> 00 : 07 : 34.620
계산 중이라고 가정해보면

145
00 : 07 : 30.780 -> 00 : 07 : 39.090
여기있는 dz2 공식을 그대로 사용하면 됩니다.

146
00 : 07 : 34.620 -> 00 : 07 : 43.050
그리고 이 곱셈 기호는 행렬의 곱을 의미합니다.

147
00 : 07 : 39.090 -> 00 : 07 : 46.950
따라서 이 부분은 n1 * m의 행렬이어야합니다.

148
00 : 07 : 43.050 -> 00 : 07 : 48.990
그리고 이 부분

149
00 : 07 : 46.950 -> 00 : 07 : 52.680
또한 동일하게

150
00 : 07 : 48.990 -> 00 : 07 : 54.720
N1 * m 행렬이어야 합니다.

151
00 : 07 : 52.680 -> 00 : 07 : 59.669
따라서 여기서 곱셈 기호는 행렬의 
해당 요소를 곱하는 것을 의미합니다.

152
00 : 07 : 54.720 -> 00 : 08 : 08.490
마지막 dW1은 다음과 같습니다.

153
00 : 07 : 59.669 -> 00 : 08 : 18.950
그리고 db1은

154
00 : 08 : 08.490 -> 00 : 08 : 21.900
이것과 같습니다.

155
00 : 08 : 18.950 -> 00 : 08 : 23.430
그러나 여기에서 keepdimms은

156
00 : 08 : 21.900 -> 00 : 08 : 27.210
중요하지 않을 수도 있습니다.

157
00 : 08 : 23.430 -> 00 : 08 : 28.590
n[2]는 단지 1 * 1 행렬이므로

158
00 : 08 : 27.210 -> 00 : 08 : 35.729
실수였지만,

159
00 : 08 : 28.590 -> 00 : 08 : 38.370
여기서 db1은 n1 * 1 벡터입니다.

160
00 : 08 : 35.729 -> 00 : 08 : 40.110
우리는 파이썬이

161
00 : 08 : 38.370 -> 00 : 08 : 43.110
여기에 표시된 차원의 행렬을

162
00 : 08 : 40.110 -> 00 : 08 : 46.529
출력하길 원할 겁니다.

163
00 : 08 : 43.110 -> 00 : 08 : 48.360
이후에 당신의 계산을 망쳐버릴 수도 있는

164
00 : 08 : 46.529 -> 00 : 08 : 50.580
이상한 차원의 행렬이 아니라 말이죠

165
00 : 08 : 48.360 -> 00 : 08 : 53.310
또 다른 방법은

166
00 : 08 : 50.580 -> 00 : 08 : 56.910
이러한 매개 변수를 유지하지 말고

167
00 : 08 : 53.310 -> 00 : 09 : 00.060
명시적으로 reshape 구문을
호출하는 겁니다.

168
00 : 08 : 56.910 -> 00 : 09 : 04.400
이 구문을 이용해서 np.sum의 출력을

169
00 : 09 : 00.060 -> 00 : 09 : 08.310
원하는 데이터베이스의 차원으로 
바꿔 주는 겁니다.

170
00 : 09 : 04.400 -> 00 : 09 : 11.339
그래서 우리는 forward propagation의

171
00 : 09 : 08.310 -> 00 : 09 : 14.310
4 개의 방정식과 back propagation의

172
00 : 09 : 11.339 -> 00 : 09 : 16.680
6 개의 방정식에 대해서 배웠습니다.

173
00 : 09 : 14.310 -> 00 : 09 : 18.870
그러나 다음 선택 비디오에서

174
00 : 09 : 16.680 -> 00 : 09 : 22.050
우리는 bkac propagation 알고리즘에 
대한 6 개의 방정식

175
00 : 09 : 18.870 -> 00 : 09 : 23.940
이 도출되는 방법에 대한

176
00 : 09 : 22.050 -> 00 : 09 : 25.830
직관적인 이해를 해볼겁니다.

177
00 : 09 : 23.940 -> 00 : 09 : 27.750
직접 볼 것인지 여부를 결정하십시오.

178
00 : 09 : 25.830 -> 00 : 09 : 30.000
이 optional을 보던 안보던

179
00 : 09 : 27.750 -> 00 : 09 : 32.730
제가 써 놓은 알고리즘을 구현한다면

180
00 : 09 : 30.000 -> 00 : 09 : 34.650
올바른 forward, back propagation을 
달성 할 수 있습니다.

181
00 : 09 : 32.730 -> 00 : 09 : 36.750
그리고 신경망의 매개변수를 학습 시키기 위한

182
00 : 09 : 34.650 -> 00 : 09 : 39.029
gradient descent에 필요한 미분 값을

183
00 : 09 : 36.750 -> 00 : 09 : 41.520
계산할 수 있을 겁니다.

184
00 : 09 : 39.029 -> 00 : 09 : 43.680
이 알고리즘을 구현하여

185
00 : 09 : 41.520 -> 00 : 09 : 45.209
효과적으로 만들 수도 있습니다

186
00 : 09 : 43.680 -> 00 : 09 : 47.130
비록 당신이 그것을 이해하지 않더라도

187
00 : 09 : 45.209 -> 00 : 09 : 50.520
뒤에 나오는 대수 연산에 대해서는

188
00 : 09 : 47.130 -> 00 : 09 : 52.320
많은 딥러닝의 실무자가 이 작업을 수행합니다.

189
00 : 09 : 50.520 -> 00 : 09 : 54.180
그러나 당신이 이해하고 싶다면 
다음 비디오를 보시면 됩니다.

190
00 : 09 : 52.320 -> 00 : 09 : 56.580
이 방정식의

191
00 : 09 : 54.180 -> 00 : 09 : 58.820
미분계산에 대한

192
00 : 09 : 56.580 -> 00 : 09 : 58.820
직관을 배우시기 바랍니다.