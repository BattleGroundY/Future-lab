WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 03.090
딥러닝의 뒤에 있는 기본적인 테크닉들이

2
00 : 00 : 03.090 -> 00 : 00 : 06.162
수십 년 전부터 있었다면,

3
00 : 00 : 06.162 -> 00 : 00 : 08.040
왜 이제 와서
이렇게 잘 활용 되는 건가요?

4
00 : 00 : 08.040 -> 00 : 00 : 10.290
이 동영상에서는

5
00 : 00 : 10.290 -> 00 : 00 : 14.040
무엇이 딥러닝의 번영에 기여했는지에 대해
이야기합니다.

6
00 : 00 : 14.040 -> 00 : 00 : 19.990
이것은 당신이 실제로 이러한 도구를 사용할 기회를
발견하는데 도움이 될 것입니다.

7
00 : 00 : 19.990 -> 00 : 00 : 22.540
2, 3 년 동안 나는 많은 사람들에게
이런 질문을 받았습니다.

8
00 : 00 : 22.540 -> 00 : 00 : 25.735
"Andrew 왜 갑자기 딥러닝이
이렇게 잘 활용 되는 거야? "

9
00 : 00 : 25.735 -> 00 : 00 : 27.340
이렇게 물었을 때,

10
00 : 00 : 27.340 -> 00 : 00 : 30.090
나는 대개 다음과 같은 그림을 그립니다.

11
00 : 00 : 30.090 -> 00 : 00 : 33.300
수평 축이 우리가 가지고있는 데이터의 양을 나타내는

12
00 : 00 : 33.300 -> 00 : 00 : 37.025
그래프를 그려 봅니다.

13
00 : 00 : 37.025 -> 00 : 00 : 43.090
그리고 수직 축은 모든 학습의 알고리즘
성능을 나타냅니다.

14
00 : 00 : 43.090 -> 00 : 00 : 46.225
이것은 스팸(메일) 분류 정확도이거나,

15
00 : 00 : 46.225 -> 00 : 00 : 48.700
광고 클릭 예측 기능의 성능이거나,

16
00 : 00 : 48.700 -> 00 : 00 : 51.820
자율주행 차량이 다른 차량의 위치를 추측하는

17
00 : 00 : 51.820 -> 00 : 00 : 55.860
신경망의 정확성을 나타내는 겁니다.

18
00 : 00 : 55.860 -> 00 : 00 : 59.570
옛날부터 있던 학습 알고리즘,
예를 들어 support vector machine, logistic regression 등의

19
00 : 00 : 59.570 -> 00 : 01 : 01.540
성능을 정리하여

20
00 : 01 : 01.540 -> 00 : 01 : 05.895
가지고있는 데이터의 양의 함수로 표현하면

21
00 : 01 : 05.895 -> 00 : 01 : 09.155
이러한 곡선 그래프가 될 것입니다.

22
00 : 01 : 09.155 -> 00 : 01 : 12.510
데이터를 늘리면
잠시 성능은 향상되지만

23
00 : 01 : 12.510 -> 00 : 01 : 16.774
그 이후는 별로 차이가 없습니다.

24
00 : 01 : 16.774 -> 00 : 01 : 20.070
잘 그리진 못했지만
수평선이라고 생각합시다.

25
00 : 01 : 20.070 -> 00 : 01 : 26.130
이 함수는 데이터가 많이 있어도
어떻게하면 좋을지 모르겠다는 느낌입니다.

26
00 : 01 : 26.130 -> 00 : 01 : 28.700
이 사회에서는

27
00 : 01 : 28.700 -> 00 : 01 : 31.640
여러 문제에 대해

28
00 : 01 : 31.640 -> 00 : 01 : 35.295
20 년 전은 비교적 적은 데이터 량 밖에
없었습니다.

29
00 : 01 : 35.295 -> 00 : 01 : 39.370
하지만 지금은 꽤 많은 데이터를
가질 수 있게 되었습니다.

30
00 : 01 : 39.370 -> 00 : 01 : 42.890
그 대부분은 디지털화 덕분이라고 생각합니다.

31
00 : 01 : 42.890 -> 00 : 01 : 47.450
오늘날 인간의 활동 대부분이 디지털 영역에 있습니다.

32
00 : 01 : 47.450 -> 00 : 01 : 50.270
컴퓨터와 웹 사이트, 모바일 앱으로
많은 시간을 보내고 있으며,

33
00 : 01 : 50.270 -> 00 : 01 : 56.120
이러한 디지털 장치에서의 활동은
데이터를 만들어냅니다.

34
00 : 01 : 56.120 -> 00 : 02 : 01.445
휴대폰에 비치 된
저렴한 카메라와 가속도 센서 등

35
00 : 02 : 01.445 -> 00 : 02 : 05.349
"사물의 인터넷"에서
모든 종류의 센서 덕분에

36
00 : 02 : 05.349 -> 00 : 02 : 09.140
더욱 더 많은 데이터를
수집 할 수 있습니다.

37
00 : 02 : 09.140 -> 00 : 02 : 12.080
지난 20 년 동안

38
00 : 02 : 12.080 -> 00 : 02 : 15.380
기존의 학습 알고리즘이
효율적으로 활용할 수있는 범위를 넘어선

39
00 : 02 : 15.380 -> 00 : 02 : 19.330
방대한 양의 데이터를 축적 할 수 있게 되었습니다.

40
00 : 02 : 19.330 -> 00 : 02 : 20.960
신경망에 대해 살펴 보자면

41
00 : 02 : 20.960 -> 00 : 02 : 24.550
당신이 작은 신경망을 훈련하는 경우

42
00 : 02 : 24.550 -> 00 : 02 : 27.294
괜찮은 수준의 성능이 나오게 됩니다.

43
00 : 02 : 27.294 -> 00 : 02 : 30.779
좀 더 큰 신경망
훈련하면

44
00 : 02 : 30.779 -> 00 : 02 : 33.649
(이를 중간 크기의
신경망 이라고 하겠습니다.)

45
00 : 02 : 33.649 -> 00 : 02 : 36.995
그 성능은 조금 좋아집니다.

46
00 : 02 : 36.995 -> 00 : 02 : 39.560
그리고 매우 큰 신경망
훈련하면

47
00 : 02 : 39.560 -> 00 : 02 : 43.230
그 성능은 훨씬 더 좋아집니다.

48
00 : 02 : 43.230 -> 00 : 02 : 44.844
여기에서 몇 가지 알 수 있습니다.

49
00 : 02 : 44.844 -> 00 : 02 : 50.140
첫째, 높은 수준의 성능을 얻고 싶다면,
두 가지가 필요하다는 것입니다.

50
00 : 02 : 50.140 -> 00 : 02 : 53.120
하나는 방대한 양의 데이터를 활용 할 수있는

51
00 : 02 : 53.120 -> 00 : 02 : 58.355
충분한 크기의 신경망을
훈련 할 수 있어야합니다.

52
00 : 02 : 58.355 -> 00 : 03 : 01.220
둘째, x값이 축의 오른쪽에 위치에 있어야합니다.

53
00 : 03 : 01.220 -> 00 : 03 : 03.520
즉, 많은 양의 데이터가 필요하다는 얘기입니다.

54
00 : 03 : 03.520 -> 00 : 03 : 08.150
우리는 흔히 스케일이 딥러닝의 진보를 
이끌었다고 말합니다.

55
00 : 03 : 08.150 -> 00 : 03 : 10.855
「스케일」라고하는 것은,

56
00 : 03 : 10.855 -> 00 : 03 : 14.595
신경망의 크기,
즉 숨겨진 레이어가 많아

57
00 : 03 : 14.595 -> 00 : 03 : 15.815
많은 매개 변수가 있으며,

58
00 : 03 : 15.815 -> 00 : 03 : 20.640
많은 관계가 있는 신경망 이라는 것 이외에,
데이터의 양을 말합니다.

59
00 : 03 : 20.640 -> 00 : 03 : 24.620
실제로 오늘날 신경망에서
좋은 성능을 얻을 수있는 가장 확실한 방법은

60
00 : 03 : 24.620 -> 00 : 03 : 29.576
더 큰 신경망을 훈련 시키거나
그것에 들어가는 데이터를 늘리거나, 이 두가지 입니다.

61
00 : 03 : 29.576 -> 00 : 03 : 33.230
그것도 어느정도 지점까지만 가능합니다. 왜냐하면 
당신이 투입할 수 있는 데이터가 바닥날 수도 있고

62
00 : 03 : 33.230 -> 00 : 03 : 37.110
신경망이 크기가 너무 크다면
훈련 시간이 너무 길어지기 때문입니다.

63
00 : 03 : 37.110 -> 00 : 03 : 42.540
그러나 scale을 개선하는 것은
딥러닝에 있어서 매우 중요합니다.

64
00 : 03 : 42.540 -> 00 : 03 : 46.425
이 그림을 좀 더 전문적으로 
정확하게 그리기 위해,

65
00 : 03 : 46.425 -> 00 : 03 : 48.405
몇가지를 좀 더 추가해 보겠습니다.

66
00 : 03 : 48.405 -> 00 : 03 : 51.065
x 축으로 데이터의 양을 썼습니다.

67
00 : 03 : 51.065 -> 00 : 03 : 57.530
조금 전문적으로 말하면
이것은 레이블 된 데이터

68
00 : 03 : 57.530 -> 00 : 03 : 59.211
즉 입력값 인 x와 출력값 y가 모두있는

69
00 : 03 : 59.211 -> 00 : 04 : 02.600
training 샘플 데이터를 의미합니다.

70
00 : 04 : 02.600 -> 00 : 04 : 06.500
이 과정에서 나중에 사용 하게될
일부 표기법을 소개합니다.

71
00 : 04 : 06.500 -> 00 : 04 : 11.921
training 샘플의 수를 소문자 m로 나타냅니다.

72
00 : 04 : 11.921 -> 00 : 04 : 17.180
이 m값은 곧 수평 축을 의미합니다

73
00 : 04 : 17.180 -> 00 : 04 : 19.825
이 그림에 대해 좀 더 자세히 말하면,

74
00 : 04 : 19.825 -> 00 : 04 : 23.375
여기 training 샘플의 개수가 적은 범위에서는

75
00 : 04 : 23.375 -> 00 : 04 : 28.720
알고리즘의 성능의 차이가
명확하게 나타나 있지 않습니다.

76
00 : 04 : 28.720 -> 00 : 04 : 32.120
그래서 만약 training 샘플이 적으면

77
00 : 04 : 32.120 -> 00 : 04 : 36.500
알고리즘의 성능은 당신이 feature들을
얼마나 잘 관리 할 수 있는가에 달려 있습니다.

78
00 : 04 : 36.500 -> 00 : 04 : 42.290
만약 SVM(support vector machines)을 사용하고
feature를 더 잘 조작 하고 있는 사람과

79
00 : 04 : 42.290 -> 00 : 04 : 44.810
훨씬 더 큰 신경망을
사용하고 있는 사람이 있으면,

80
00 : 04 : 44.810 -> 00 : 04 : 48.640
이 training 샘플의 적은 범위 내에서는

81
00 : 04 : 48.640 -> 00 : 04 : 50.270
SVM이 더 잘 될수도 있다는 것입니다.

82
00 : 04 : 50.270 -> 00 : 04 : 53.570
그래서 그림의 왼쪽 영역에서는

83
00 : 04 : 53.570 -> 00 : 04 : 58.490
알고리즘끼리의 상대적인 위치는
그만큼 제대로 정해져 있지 않으며,

84
00 : 04 : 58.490 -> 00 : 05 : 0.500
그 성능은 당신의 feature를 조작하는 기술과

85
00 : 05 : 0.500 -> 00 : 05 : 03.950
알고리즘의 세세한 부분을 조작하는 
능력에 달려 있습니다.

86
00 : 05 : 03.950 -> 00 : 05 : 07.130
이 training 샘플의 큰 영역,

87
00 : 05 : 07.130 -> 00 : 05 : 08.190
즉 m값이 매우 커진 오른쪽 영역에서

88
00 : 05 : 08.190 -> 00 : 05 : 11.824
비로소 크기가 큰 신경망이

89
00 : 05 : 11.824 -> 00 : 05 : 16.830
일관되게 좋은 성능을 보여 주는 것입니다.

90
00 : 05 : 16.830 -> 00 : 05 : 18.800
만약 당신이 친구에게

91
00 : 05 : 18.800 -> 00 : 05 : 20.744
"왜 신경망이 최근들어서
이렇게 성공하고 있는거야? "라고 물으면

92
00 : 05 : 20.744 -> 00 : 05 : 24.320
꼭 이 그림을 그려서 친구에게 설명 해주세요

93
00 : 05 : 24.320 -> 00 : 05 : 27.170
딥러닝의 발전 초기에는

94
00 : 05 : 27.170 -> 00 : 05 : 28.865
중요한 것은

95
00 : 05 : 28.865 -> 00 : 05 : 32.885
데이터의 규모와 계산의 규모였습니다.

96
00 : 05 : 32.885 -> 00 : 05 : 36.890
CPU와 GPU로 큰 신경망을
훈련 할 수 있다는 것만으로

97
00 : 05 : 36.890 -> 00 : 05 : 40.995
큰 발전을 이룰 수있었습니다.

98
00 : 05 : 40.995 -> 00 : 05 : 43.995
그러나 특히 지난 몇 년 동안,

99
00 : 05 : 43.995 -> 00 : 05 : 47.595
알고리즘의 엄청난 혁신이 일어나고 있습니다.

100
00 : 05 : 47.595 -> 00 : 05 : 50.320
그것도 경시 할 수 없습니다.

101
00 : 05 : 50.320 -> 00 : 05 : 54.860
흥미롭게도,
알고리즘의 혁신의 대부분은

102
00 : 05 : 54.860 -> 00 : 06 : 00.440
신경망을 더 빨리
작동 시키려고하는 것입니다.

103
00 : 06 : 00.440 -> 00 : 06 : 02.375
구체적인 예로는

104
00 : 06 : 02.375 -> 00 : 06 : 05.840
신경망의
큰 진보 중 하나는

105
00 : 06 : 05.840 -> 00 : 06 : 12.140
이런 형태의 sigmoid 함수에서
ReLU 함수로 바꾼 것입니다.

106
00 : 06 : 12.140 -> 00 : 06 : 17.480
ReLU 함수는 앞의 동영상에서 언급 했어요,
이런 형태의 것입니다.

107
00 : 06 : 17.480 -> 00 : 06 : 21.450
지금부터 말하는 내용의 자세한 의미를
몰라도 걱정하지 마십시오.

108
00 : 06 : 21.450 -> 00 : 06 : 25.280
sigmoid 함수를 machine learning에 사용하여
발생하는 문제 중 하나는

109
00 : 06 : 25.280 -> 00 : 06 : 29.390
이렇게 기울기가 거의 0인 영역이

110
00 : 06 : 29.390 -> 00 : 06 : 31.430
있다는 것입니다.

111
00 : 06 : 31.430 -> 00 : 06 : 33.290
그 탓에 학습은 매우 늦어 져 버립니다.

112
00 : 06 : 33.290 -> 00 : 06 : 36.049
왜냐하면 gradient descent를 구현했을 때 
기울기가 0에 가까우면

113
00 : 06 : 36.049 -> 00 : 06 : 41.285
매개 변수는 천천히 변하기 때문에
학습이 느려지게 됩니다.

114
00 : 06 : 41.285 -> 00 : 06 : 46.430
반면 이런 신경망
활성화 함수라는 것을

115
00 : 06 : 46.430 -> 00 : 06 : 51.550
Rectified Linear Unit,
줄여서 ReLU 함수로 바꾸게 되면,

116
00 : 06 : 51.550 -> 00 : 06 : 57.060
입력값이 양수이면 기울기는 항상 1입니다.

117
00 : 06 : 57.060 -> 00 : 07 : 02.570
그래서 경사가 점차 0으로 되어 가는
일은 없습니다.

118
00 : 07 : 02.570 -> 00 : 07 : 05.697
여기 왼쪽에서 선의 기울기가 0이지만,

119
00 : 07 : 05.697 -> 00 : 07 : 10.410
단지 sigmoid 함수에서 ReLU 함수로 바꾸는 것만으로

120
00 : 07 : 10.410 -> 00 : 07 : 15.890
경사 하강 법이라는 알고리즘의 연산 속도가
매우 빨라지는 것입니다.

121
00 : 07 : 15.890 -> 00 : 07 : 20.495
이것은 알고리즘의 혁신 중에서도
비교적 간단한 예입니다.

122
00 : 07 : 20.495 -> 00 : 07 : 22.130
그러나 이 혁신은 마지막으로,

123
00 : 07 : 22.130 -> 00 : 07 : 26.660
계산속도 또한 크게 증가 시켰습니다.

124
00 : 07 : 26.660 -> 00 : 07 : 29.990
이처럼 빠른 코드를 돌리기 위해

125
00 : 07 : 29.990 -> 00 : 07 : 33.420
알고리즘을 변경 한 예는 많이 있습니다.

126
00 : 07 : 33.420 -> 00 : 07 : 34.700
그 덕분에 우리는

127
00 : 07 : 34.700 -> 00 : 07 : 36.929
큰 신경망이나
많은 데이터가 있더라도

128
00 : 07 : 36.929 -> 00 : 07 : 41.680
신경망을 빠른 시간안에
훈련 할 수 있게 된 것입니다.

129
00 : 07 : 41.680 -> 00 : 07 : 46.490
빠른 계산이 중요한 또 다른 이유는

130
00 : 07 : 46.490 -> 00 : 07 : 52.220
신경망을 훈련하는 과정이
반복이라는 것입니다.

131
00 : 07 : 52.220 -> 00 : 07 : 53.520
우선 당신이 신경망 구조를 만들기 위한 
아이디어가 있다면

132
00 : 07 : 53.520 -> 00 : 07 : 57.770
당신은 그 아이디어를 코드로 구현합니다.

133
00 : 07 : 57.770 -> 00 : 08 : 00.670
그리고 그것 얼마나 잘하고 있는지 
알아보기 위해서

134
00 : 08 : 00.670 -> 00 : 08 : 03.875
실험도 해봐야 할 것이고,

135
00 : 08 : 03.875 -> 00 : 08 : 08.740
그것을 보고 또한 신경망을
수정해야 할 수도 있습니다.

136
00 : 08 : 08.740 -> 00 : 08 : 12.675
그렇게이 사이클을 반복합니다.

137
00 : 08 : 12.675 -> 00 : 08 : 16.415
신경망을 훈련하는데
오랜 시간이 걸리면

138
00 : 08 : 16.415 -> 00 : 08 : 20.000
이 사이클을 도는 데에도
오랜 시간이 걸립니다.

139
00 : 08 : 20.000 -> 00 : 08 : 21.830
아이디어가 떠올라서 그것을 시도하는데

140
00 : 08 : 21.830 -> 00 : 08 : 26.030
10 분 걸리는지 하루 걸릴지,

141
00 : 08 : 26.030 -> 00 : 08 : 32.870
혹은 한 달 걸리는지는

142
00 : 08 : 32.870 -> 00 : 08 : 35.435
생산성에 큰 차이가 발생합니다.

143
00 : 08 : 35.435 -> 00 : 08 : 40.040
실제로 한 달이 걸릴 수도 있습니다.

144
00 : 08 : 40.040 -> 00 : 08 : 44.150
10 분, 하루 만에 결과를 알면
더 많은 아이디어를 시도 할 수 있을 것이고,

145
00 : 08 : 44.150 -> 00 : 08 : 47.180
당신의 프로그램에서
잘 작동되는 신경망을

146
00 : 08 : 47.180 -> 00 : 08 : 50.890
발견 할 수있는 가능성이 높아집니다.

147
00 : 08 : 50.890 -> 00 : 08 : 55.580
이처럼 빠른 계산은

148
00 : 08 : 55.580 -> 00 : 09 : 01.210
실험의 결과를 얻는 속도를 높여주는데에
매우 도움이 됩니다.

149
00 : 09 : 01.210 -> 00 : 09 : 05.795
이것은 신경망을 구현하는 사람

150
00 : 09 : 05.795 -> 00 : 09 : 08.390
그리고 딥러닝 연구자들이

151
00 : 09 : 08.390 -> 00 : 09 : 14.025
빠르게 이를 반복하고 빠르게 아이디어를 개선하는데
도움이 되었습니다.

152
00 : 09 : 14.025 -> 00 : 09 : 15.600
물론 이들 모두가

153
00 : 09 : 15.600 -> 00 : 09 : 19.760
놀랍게도
거의 매달 새로운 알고리즘을 개발하고 있습니다.

154
00 : 09 : 19.760 -> 00 : 09 : 24.365
이것은 전체 딥러닝 연구 커뮤니티에게 
큰 힘이되었습니다.

155
00 : 09 : 24.365 -> 00 : 09 : 25.290
이들 모두가

156
00 : 09 : 25.290 -> 00 : 09 : 25.680
놀라운 속도로 새로운 알고리즘을 발명하고

157
00 : 09 : 25.680 -> 00 : 09 : 26.618
일취월장으로 발전하고 있습니다.

158
00 : 09 : 26.618 -> 00 : 09 : 27.300
이 역시 딥러닝 연구 커뮤니티 전체에 있어서
큰 보탬이되고 있습니다.

159
00 : 09 : 27.300 -> 00 : 09 : 30.755
여기까지가 딥러닝을 발전시켜 온 요인들 입니다.
(여기부터 자막 싱크가 좀 안맞아요 ㅠ)

160
00 : 09 : 30.755 -> 00 : 09 : 34.270
놀라운 것은, 이러한 요인들은

161
00 : 09 : 34.270 -> 00 : 09 : 38.725
딥러닝을 더욱 개선하기 위해
계속해서 활발하게 작동하고 있습니다.

162
00 : 09 : 38.725 -> 00 : 09 : 42.115
오늘날에도 이 사회는 많은 디지털 데이터를
창출하고 있습니다.

163
00 : 09 : 42.115 -> 00 : 09 : 45.230
빠른 계산을 위한 GPU 나 더 빠른 네트워크,

164
00 : 09 : 45.230 -> 00 : 09 : 48.514
온갖 하드웨어 등

165
00 : 09 : 48.514 -> 00 : 09 : 51.060
특화된 하드웨어의 개발 또한 활발 해지고 있습니다.

166
00 : 09 : 51.060 -> 00 : 09 : 54.360
계산이라는 관점 에서

167
00 : 09 : 54.360 -> 00 : 09 : 57.870
우리가 거대한 신경망을 활용하는 능력은

168
00 : 09 : 57.870 -> 00 : 10 : 00.130
계속 올라가고 있다고 자신있게 말할 수 있습니다.

169
00 : 10 : 00.130 -> 00 : 10 : 03.260
알고리즘의 경우에도

170
00 : 10 : 03.260 -> 00 : 10 : 06.530
딥러닝 연구 커뮤니티 전체가
급격하게 발전하고 있기 때문에,

171
00 : 10 : 06.530 -> 00 : 10 : 09.315
전망이 낙관적이라고 생각합니다.

172
00 : 10 : 09.315 -> 00 : 10 : 12.650
나 자신도 딥러닝은 앞으로 몇 년

173
00 : 10 : 12.650 -> 00 : 10 : 15.700
개선이 계속될 것이라고
낙관적으로 생각하고 있습니다.

174
00 : 10 : 15.700 -> 00 : 10 : 18.575
이제 이 섹션의 마지막 동영상에 가서

175
00 : 10 : 18.575 -> 00 : 10 : 21.000
이 과정에서 무엇을 배웠는가?에 대해
좀 더 얘기해 봅시다.