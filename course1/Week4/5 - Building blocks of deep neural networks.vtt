WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 02.705
저번주

2
00 : 00 : 02.705 -> 00 : 00 : 05.735
비디오에서,

3
00 : 00 : 05.735 -> 00 : 00 : 08.075
여러분은 이미 Forward prop 및 Back prop

4
00 : 00 : 08.075 -> 00 : 00 : 10.456
을 위한 기본 구조와, 
심층 신경 네트워크를 구축하기위한

5
00 : 00 : 10.456 -> 00 : 00 : 14.090
핵심 포인트를 이해 했습니다.

6
00 : 00 : 14.090 -> 00 : 00 : 18.303
이제 이러한 점들을 결합하여 심층 신경 
네트워크를 구성하는 방법을 살펴 보겠습니다.

7
00 : 00 : 18.303 -> 00 : 00 : 20.333
이것은 몇 개의 레이어의 신경 네트워크입니다.

8
00 : 00 : 20.333 -> 00 : 00 : 27.290
여기 있는 레이어를 선택하고 
해당 레이어의 계산에만 집중하겠습니다.

9
00 : 00 : 27.290 -> 00 : 00 : 33.410
L 레이어의 경우 WL 및 BL 매개 변수가 있습니다.

10
00 : 00 : 33.410 -> 00 : 00 : 35.645
Forward prop의 경우,

11
00 : 00 : 35.645 -> 00 : 00 : 42.180
이전 레이어에서의 벡터 a[L-1]을 입력합니다.

12
00 : 00 : 42.180 -> 00 : 00 : 48.895
그런 다음 a[L]을 출력 하게 됩니다.

13
00 : 00 : 48.895 -> 00 : 01 : 00.105
그래서 우리는 이전에 이것을 계산했습니다 
ZL = WLxAL-1 + BL

14
00 : 01 : 00.105 -> 00 : 01 : 08.025
그리고 A[L] = G [Z[L]] 맞죠?

15
00 : 01 : 08.025 -> 00 : 01 : 12.990
이것은 입력 A[L-1]로부터 출력 A[L]을 계산하는 방법입니다.

16
00 : 01 : 12.990 -> 00 : 01 : 15.330
나중에 계산할 때

17
00 : 01 : 15.330 -> 00 : 01 : 20.640
Z[L] 값을 cache 하는 것이 유용합니다.

18
00 : 01 : 20.640 -> 00 : 01 : 24.060
Z[L]을 cache로 저장하면

19
00 : 01 : 24.060 -> 00 : 01 : 28.170
Back prop할 때

20
00 : 01 : 28.170 -> 00 : 01 : 31.290
유용합니다.

21
00 : 01 : 31.290 -> 00 : 01 : 32.745
Backward step 과

22
00 : 01 : 32.745 -> 00 : 01 : 34.785
back prop을 위해서

23
00 : 01 : 34.785 -> 00 : 01 : 37.390
우리는 이번에도 L 레이어의 계산에만 
초점을 맞춥니다.

24
00 : 01 : 37.390 -> 00 : 01 : 41.635
DA[L]이 입력 값이고 출력 값은 DA[L-1]인

25
00 : 01 : 41.635 -> 00 : 01 : 51.840
함수를 구현할 것입니다.

26
00 : 01 : 51.840 -> 00 : 01 : 53.650
자세히 살펴보자면,

27
00 : 01 : 53.650 -> 00 : 01 : 56.050
실제로 L 레이어의 입력 값은 DA[l]이고

28
00 : 01 : 56.050 -> 00 : 01 : 59.130
이때 cache를 이용해서

29
00 : 01 : 59.130 -> 00 : 02 : 04.040
(이미 계산 된 Z[L] 값을 사용할 수 있습니다.)

30
00 : 02 : 04.040 -> 00 : 02 : 06.755
그런 다음 출력 값은 DA[L-1] 이고,

31
00 : 02 : 06.755 -> 00 : 02 : 09.940
gradient descent 학습 알고리즘을 적용해서

32
00 : 02 : 09.940 -> 00 : 02 : 14.025
원하는 경우 gradient를 출력 할 수도 있습니다.

33
00 : 02 : 14.025 -> 00 : 02 : 19.772
이것이 바로  
Forward prop(또는 Forward function)와

34
00 : 02 : 19.772 -> 00 : 02 : 22.680
back prop(또는 backward function) 구현의

35
00 : 02 : 22.680 -> 00 : 02 : 24.970
기본 구조 입니다.

36
00 : 02 : 24.970 -> 00 : 02 : 27.535
요약하자면, L 레이어에서,

37
00 : 02 : 27.535 -> 00 : 02 : 32.650
A[L-1]을 입력 값으로 하고 
A[L]을 출력 하는

38
00 : 02 : 32.650 -> 00 : 02 : 38.986
Forward step 또는 Forward function이 
있어야 합니다.

39
00 : 02 : 38.986 -> 00 : 02 : 42.070
그리고 계산을 위해서,

40
00 : 02 : 42.070 -> 00 : 02 : 45.650
WL과 BL을 사용해야합니다.

41
00 : 02 : 45.650 -> 00 : 02 : 54.185
그리고, 캐시를 z[l]으로 설정합니다.

42
00 : 02 : 54.185 -> 00 : 02 : 56.357
다음으로는 backward function을 
설정 하려고 합니다.

43
00 : 02 : 56.357 -> 00 : 03 : 01.650
이렇게 또 다른 함수가 될 것이고

44
00 : 03 : 01.650 -> 00 : 03 : 08.065
DA[L]을 입력해서 DA[L-1]을 출력 합니다.

45
00 : 03 : 08.065 -> 00 : 03 : 13.820
이곳에서는 Da[L]의 미분값을 통해서

46
00 : 03 : 13.820 -> 00 : 03 : 16.950
a[L-1]의 변화량을

47
00 : 03 : 16.950 -> 00 : 03 : 20.400
계산하고 싶은겁니다.

48
00 : 03 : 20.400 -> 00 : 03 : 24.590
이전 계산의 미분값 Da[L-1]으로 말이죠.

49
00 : 03 : 24.590 -> 00 : 03 : 29.870
이 상자에는 WL과 BL을 사용합니다.

50
00 : 03 : 29.870 -> 00 : 03 : 31.200
그 과정 속에서

51
00 : 03 : 31.200 -> 00 : 03 : 34.144
DZL을 계산 하게 될겁니다.

52
00 : 03 : 34.144 -> 00 : 03 : 36.400
그런 다음 이 상자에서

53
00 : 03 : 36.400 -> 00 : 03 : 43.515
backward 함수는 또한 DWL과 DBL을 출력합니다.

54
00 : 03 : 43.515 -> 00 : 03 : 47.740
때로는 빨간색 화살표를 사용하여 
backward를 나타냅니다.

55
00 : 03 : 47.740 -> 00 : 03 : 51.668
원한다면 빨간색 화살표를 그려도 됩니다.

56
00 : 03 : 51.668 -> 00 : 03 : 55.305
여러분이 이 두 가지 함수를 구현 했다면

57
00 : 03 : 55.305 -> 00 : 03 : 59.390
그 다음 신경망의 기본 계산은 다음과 같습니다.

58
00 : 03 : 59.390 -> 00 : 04 : 02.453
입력 벡터 A0를 취하고, 

59
00 : 04 : 02.453 -> 00 : 04 : 07.710
다음 이것은 첫 번째 레이어의 
activation을 계산할 겁니다,

60
00 : 04 : 07.710 -> 00 : 04 : 09.624
이것을 A1 이라고 부릅니다.

61
00 : 04 : 09.624 -> 00 : 04 : 14.535
물론 계산 과정에서, W1과 B1이 필요합니다.

62
00 : 04 : 14.535 -> 00 : 04 : 21.090
또한 Z[1]을 cache 하십시오.

63
00 : 04 : 21.090 -> 00 : 04 : 22.475
좋습니다, 그 후,

64
00 : 04 : 22.475 -> 00 : 04 : 25.290
우리는 그들을 2번째 레이어에 놓고,

65
00 : 04 : 25.290 -> 00 : 04 : 28.026
그런 다음 W2와 B2를 사용하고,

66
00 : 04 : 28.026 -> 00 : 04 : 32.355
두번째 레이어의 activation 벡터인 
A2를 계산하고,

67
00 : 04 : 32.355 -> 00 : 04 : 36.475
Y_hat에 해당하는

68
00 : 04 : 36.475 -> 00 : 04 : 40.065
a[L] 값이 출력될 때까지

69
00 : 04 : 40.065 -> 00 : 04 : 42.655
반복합니다.

70
00 : 04 : 42.655 -> 00 : 04 : 51.595
이 방향을 따라 모든 Z 값을 캐시했습니다.

71
00 : 04 : 51.595 -> 00 : 04 : 55.161
이것이 Forward prop step입니다.

72
00 : 04 : 55.161 -> 00 : 04 : 57.585
이제 backward prop step를 보겠습니다.

73
00 : 04 : 57.585 -> 00 : 05 : 03.220
우리가 할 일은 일련의 연산 과정을 
거꾸로 돌려서

74
00 : 05 : 03.220 -> 00 : 05 : 12.200
gradients를 뒤로 전파하며 
계산하는걸 반복합니다.

75
00 : 05 : 12.200 -> 00 : 05 : 17.350
이런 식으로 DA [L]을 입력하면

76
00 : 05 : 17.350 -> 00 : 05 : 21.674
이 상자에서 DA [L-1]을 계산합니다.

77
00 : 05 : 21.674 -> 00 : 05 : 30.424
DA2, DA1을 얻을 때까지 반복하십시오.

78
00 : 05 : 30.424 -> 00 : 05 : 35.500
또한 DA0가 나오는데

79
00 : 05 : 35.500 -> 00 : 05 : 40.250
그러나 이 입력 feature 벡터의 
미분 값은 쓸모가 없습니다.

80
00 : 05 : 40.250 -> 00 : 05 : 46.237
적어도  supervised 신경 네트워크에서 
weight값을 훈련 시킬때는 말이죠.

81
00 : 05 : 46.237 -> 00 : 05 : 48.160
그럼 여기서 그냥 멈추셔도 상관 없습니다.

82
00 : 05 : 48.160 -> 00 : 05 : 54.365
이 방향을 따라 backward prop는 
DWL, DBL도 출력합니다.

83
00 : 05 : 54.365 -> 00 : 05 : 58.800
파라미터 WL 및 BL을 사용하여,

84
00 : 05 : 58.800 -> 00 : 06 : 04.810
이렇게하면 DW3,

85
00 : 06 : 04.810 -> 00 : 06 : 09.238
DB3 등등.

86
00 : 06 : 09.238 -> 00 : 06 : 16.660
여러분에게 필요한 모든 미분 값을 계산 합니다.

87
00 : 06 : 16.660 -> 00 : 06 : 21.235
사실 이 구조는 더 많은 
매개 변수를 채울 필요가 있습니다.

88
00 : 06 : 21.235 -> 00 : 06 : 28.500
매개 변수 WL과 BL이 이렇게 들어가고,

89
00 : 06 : 28.500 -> 00 : 06 : 34.560
그리고 추후에 배우겠지만 
이 상자 안에서 결국

90
00 : 06 : 34.560 -> 00 : 06 : 37.460
DZ를 마침내 계산합니다.

91
00 : 06 : 37.460 -> 00 : 06 : 43.305
신경 회로망을 훈련 시키기 위한 
단계의 반복은 A0(x)로 시작합니다.

92
00 : 06 : 43.305 -> 00 : 06 : 46.725
다음 Forward prop를 완료하고,

93
00 : 06 : 46.725 -> 00 : 06 : 50.715
Y hat을 계산하고 이것을 사용하여 
이것을 계산합니다.

94
00 : 06 : 50.715 -> 00 : 06 : 55.680
그런 다음 뒤로 이동하고 또 이동 합니다.

95
00 : 06 : 55.680 -> 00 : 06 : 59.880
그래서 이제 당신은 모든 
미분 값을 가지고 있습니다.

96
00 : 06 : 59.880 -> 00 : 07 : 08.830
그런 다음 learning rate에 레이어 마다의
Dw를 곱하여 W를 업데이트 할 수 있습니다.

97
00 : 07 : 08.830 -> 00 : 07 : 12.570
B도 비슷합니다.

98
00 : 07 : 12.570 -> 00 : 07 : 17.757
이제 우리는 back prop와 
모든 미분 값을 계산한 겁니다.

99
00 : 07 : 17.757 -> 00 : 07 : 21.845
이것이 신경망 그라디언트의 
단계별 반복 과정 입니다.

100
00 : 07 : 21.845 -> 00 : 07 : 25.125
계속하기 전에 하나 더 자세한 내용이 있습니다.

101
00 : 07 : 25.125 -> 00 : 07 : 29.665
개념적으로 여기서 캐시는 backward 함수에서

102
00 : 07 : 29.665 -> 00 : 07 : 34.361
사용될 Z의 값을 저장하는
저장소로 이해됩니다.

103
00 : 07 : 34.361 -> 00 : 07 : 35.635
그러나 당신이 그것을 실행할 때,

104
00 : 07 : 35.635 -> 00 : 07 : 38.496
프로그래밍 과정에서 그것을 사용할 때,

105
00 : 07 : 38.496 -> 00 : 07 : 40.840
당신은 매개 변수 W1, B1 가져오는 것에도

106
00 : 07 : 40.840 -> 00 : 07 : 43.730
캐싱을 사용하는 것이

107
00 : 07 : 43.730 -> 00 : 07 : 46.648
backward 함수의 값을 계산하기에 
편리한 방법이라는 것을 알 수 있을겁니다.

108
00 : 07 : 46.648 -> 00 : 07 : 47.868
프로그래밍 과정에서는

109
00 : 07 : 47.868 -> 00 : 07 : 49.795
실제로 Z 뿐만 아니라

110
00 : 07 : 49.795 -> 00 : 07 : 52.239
W와 B도 캐시에 저장합니다.

111
00 : 07 : 52.239 -> 00 : 07 : 57.715
또한 Z2, W2, B2도 저장합니다.

112
00 : 07 : 57.715 -> 00 : 07 : 59.860
조작성의 측면에서,

113
00 : 07 : 59.860 -> 00 : 08 : 04.120
나는 당신이 back prop를 계산할 때, 매개 변수들을

114
00 : 08 : 04.120 -> 00 : 08 : 08.975
캐싱 해놓는 것이 매우 편리할 거라고 생각합니다.

115
00 : 08 : 08.975 -> 00 : 08 : 15.446
물론 이것은 프로그래밍 연습을 할 때 
도움이 되는 부분들 입니다.

116
00 : 08 : 15.446 -> 00 : 08 : 20.061
이제는 심층 신경 네트워크를 구현하기 위한 
기본 구조를 알게 되었습니다.

117
00 : 08 : 20.061 -> 00 : 08 : 21.640
각 레이어에 해당되는 Forward prop가 있고

118
00 : 08 : 21.640 -> 00 : 08 : 24.260
그리고 back prop도 있고

119
00 : 08 : 24.260 -> 00 : 08 : 27.675
그 사이의 값을 전달하는 캐시도 있습니다.

120
00 : 08 : 27.675 -> 00 : 08 : 28.930
다음 비디오에서,

121
00 : 08 : 28.930 -> 00 : 08 : 32.250
실제로 이 기본 구조를 사용하는 방법에 대해 
논의 할 것입니다.

122
00 : 08 : 32.250 -> 00 : 08 : 33.500
다음 동영상을 살펴 보겠습니다.