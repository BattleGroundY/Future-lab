WEBVTT

1
00 : 00 : 00.060 -> 00 : 00 : 04.380
당신의 심층 신경 네트워크를

2
00 : 00 : 02.669 -> 00 : 00 : 06.870
보다 효율적으로 만들고 싶다면

3
00 : 00 : 04.380 -> 00 : 00 : 09.269
매개 변수를 설정해야 할뿐만 아니라

4
00 : 00 : 06.870 -> 00 : 00 : 11.759
하이퍼 파라미터를 신중하게 구성 해야합니다.

5
00 : 00 : 09.269 -> 00 : 00 : 15.170
하이퍼 파라미터가 무엇인지 살펴 보겠습니다.

6
00 : 00 : 11.759 -> 00 : 00 : 17.820
신경망 모델에서 공통 파라미터는 W와 b입니다.

7
00 : 00 : 15.170 -> 00 : 00 : 21.720
그리고 전체 학습 알고리즘 모델에는

8
00 : 00 : 17.820 -> 00 : 00 : 26.220
다른 매개 변수가 있습니다.

9
00 : 00 : 21.720 -> 00 : 00 : 28.920
예를 들어,

10
00 : 00 : 26.220 -> 00 : 00 : 32.329
신경망 모델의 매개 변수의 변화를

11
00 : 00 : 28.920 -> 00 : 00 : 34.890
결정하는 learning rate α

12
00 : 00 : 32.329 -> 00 : 00 : 38.190
그리고 gradient descent

13
00 : 00 : 34.890 -> 00 : 00 : 40.170
알고리즘의 반복 횟수 등이 있을 겁니다.

14
00 : 00 : 38.190 -> 00 : 00 : 42.629
또한 학습 알고리즘에

15
00 : 00 : 40.170 -> 00 : 00 : 47.340
다른 하이퍼 매개 변수를 설정해야합니다.

16
00 : 00 : 42.629 -> 00 : 00 : 50.629
예를 들어 숨겨진 레이어의 수 L이나

17
00 : 00 : 47.340 -> 00 : 00 : 56.039
숨겨진 유닛의 개수 또한 있습니다

18
00 : 00 : 50.629 -> 00 : 00 : 59.670
n [1] n [2] 등과 같이 말이죠.

19
00 : 00 : 56.039 -> 00 : 01 : 03.329
활성화 함수를

20
00 : 00 : 59.670 -> 00 : 01 : 05.610
선택할 수도 있습니다.

21
00 : 01 : 03.329 -> 00 : 01 : 06.869
주로 숨겨진 레이어에서 사용 되는

22
00 : 01 : 05.610 -> 00 : 01 : 11.760
ReLu 또는 tanh 또는 sigmoid 등을 말이죠.

23
00 : 01 : 06.869 -> 00 : 01 : 13.590
이 모든 것들이

24
00 : 01 : 11.760 -> 00 : 01 : 15.990
학습 알고리즘에서 설정해야하는.

25
00 : 01 : 13.590 -> 00 : 01 : 19.640
매개 변수 입니다.

26
00 : 01 : 15.990 -> 00 : 01 : 22.200
최종 매개 변수 W 및 b의

27
00 : 01 : 19.640 -> 00 : 01 : 25.640
결과에 영향을 줍니다.

28
00 : 01 : 22.200 -> 00 : 01 : 29.340
우리는 그것을 hyperparameter라고 부릅니다.

29
00 : 01 : 25.640 -> 00 : 01 : 30.750
Learning rate 알파,
(같은 말을 반복 하시네요 ㅠ)

30
00 : 01 : 29.340 -> 00 : 01 : 32.369
Hidden 레이어의 개수,

31
00 : 01 : 30.750 -> 00 : 01 : 36.000
Hidden 유닛의 개수,

32
00 : 01 : 32.369 -> 00 : 01 : 39.290
이 변수들은 W 및 B에 영향을 주고,

33
00 : 01 : 36.000 -> 00 : 01 : 41.970
그래서 우리는 이것을 hyperparameter라고 부릅니다.

34
00 : 01 : 39.290 -> 00 : 01 : 44.250
이것들은 말 그대로 hyperparameter 이기 때문에

35
00 : 01 : 41.970 -> 00 : 01 : 46.950
W와 b의 최종 결과를

36
00 : 01 : 44.250 -> 00 : 01 : 50.100
결정합니다.

37
00 : 01 : 46.950 -> 00 : 01 : 53.520
사실 딥러닝 학습에는 

38
00 : 01 : 50.100 -> 00 : 01 : 55.470
많은 다른 하이퍼 파라미터가 있습니다.

39
00 : 01 : 53.520 -> 00 : 01 : 57.899
나중에 수업에서

40
00 : 01 : 55.470 -> 00 : 02 : 02.990
다른 하이퍼 파라미터를 배우게 됩니다.

41
00 : 01 : 57.899 -> 00 : 02 : 02.990
예를 들어 momentum, mini batch size

42
00 : 02 : 05.150 -> 00 : 02 : 13.020
다양한 형태의 정규화 매개 변수 등이 있죠.

43
00 : 02 : 07.220 -> 00 : 02 : 14.700
지금 이 조건들을

44
00 : 02 : 13.020 -> 00 : 02 : 16.020
이해하지 못해도

45
00 : 02 : 14.700 -> 00 : 02 : 18.810
걱정 마세요.

46
00 : 02 : 16.020 -> 00 : 02 : 21.870
우리는 두 번째 단원에서 이 지식을 
배울 것입니다.

47
00 : 02 : 18.810 -> 00 : 02 : 24.120
이전 세대의 머신러닝 알고리즘과 비교해서

48
00 : 02 : 21.870 -> 00 : 02 : 26.370
딥러닝 알고리즘에는 많은 하이퍼 파라미터가 있기 때문에

49
00 : 02 : 24.120 -> 00 : 02 : 28.890
이제부터 저는

50
00 : 02 : 26.370 -> 00 : 02 : 31.050
learning rate α를

51
00 : 02 : 28.890 -> 00 : 02 : 33.480
파라미터가 아닌 하이퍼 파라미터로

52
00 : 02 : 31.050 -> 00 : 02 : 35.250
언급 할 것입니다.

53
00 : 02 : 33.480 -> 00 : 02 : 37.920
이전의 머신러닝 시대에는

54
00 : 02 : 35.250 -> 00 : 02 : 39.600
하이퍼 파라미터가 많이 없었기 때문에

55
00 : 02 : 37.920 -> 00 : 02 : 42.120
많은 사람들이 종종 α를

56
00 : 02 : 39.600 -> 00 : 02 : 44.610
그냥 파라미터라고 불러 왔습니다.

57
00 : 02 : 42.120 -> 00 : 02 : 47.580
사실, α는 기술적으로는 매개 변수입니다.

58
00 : 02 : 44.610 -> 00 : 02 : 50.280
W나 B같은 실제 매개 변수를 결정할 수 있는 
매개 변수 일뿐입니다.

59
00 : 02 : 47.580 -> 00 : 02 : 51.570
따라서 여기서는 개념을 명확하게하기 위해

60
00 : 02 : 50.280 -> 00 : 02 : 54.180
우리는 α와 iterations 같은 매개 변수를 

61
00 : 02 : 51.570 -> 00 : 02 : 55.769
계속 하이퍼 매개 변수라고 부르겠습니다.

62
00 : 02 : 54.180 -> 00 : 02 : 57.810
심층 신경 네트워크를 훈련 할 때

63
00 : 02 : 55.769 -> 00 : 02 : 59.940
하이퍼 매개 변수의 값을 설정하는데

64
00 : 02 : 57.810 -> 00 : 03 : 01.560
다양한 옵션이 있음을 알 수 있을 겁니다.

65
00 : 02 : 59.940 -> 00 : 03 : 04.440
그리고 가능한 많은 값을 시도해야합니다.

66
00 : 03 : 01.560 -> 00 : 03 : 07.230
그래서 딥러닝 알고리즘에서

67
00 : 03 : 04.440 -> 00 : 03 : 09.840
하이퍼 파라미터를 설정하는 것은

68
00 : 03 : 07.230 -> 00 : 03 : 12.150
경험에 기반한 프로세스 라고 할 수 있습니다.

69
00 : 03 : 09.840 -> 00 : 03 : 13.549
예를 들어, learning rate를 설정할 때

70
00 : 03 : 12.150 -> 00 : 03 : 16.739
당신은 알파 값을

71
00 : 03 : 13.549 -> 00 : 03 : 20.670
0.01로 설정해야한다고 생각하고

72
00 : 03 : 16.739 -> 00 : 03 : 22.530
실제로 그것을 실행한 다음,

73
00 : 03 : 20.670 -> 00 : 03 : 23.910
최종 결과를 얻을 겁니다.

74
00 : 03 : 22.530 -> 00 : 03 : 25.890
하지만 결과에 따라서는 마음을 바꿔서

75
00 : 03 : 23.910 -> 00 : 03 : 28.620
learning rate를 0.05로 높이는 것이 좋다고

76
00 : 03 : 25.890 -> 00 : 03 : 30.930
생각할 수도 있곘지요.

77
00 : 03 : 28.620 -> 00 : 03 : 32.970
이 처럼 알파 값이 확실하지 않으면

78
00 : 03 : 30.930 -> 00 : 03 : 35.010
learning rate의 최적 값은 무엇입니까?

79
00 : 03 : 32.970 -> 00 : 03 : 37.680
여러분은 learning rate 알파를 자유롭게 
사용해 볼 수 있습니다.

80
00 : 03 : 35.010 -> 00 : 03 : 39.690
비용 함수 J가 이렇게 떨어지는 것을 발견하면

81
00 : 03 : 37.680 -> 00 : 03 : 41.820
더 큰 알파 값을 대입해서

82
00 : 03 : 39.690 -> 00 : 03 : 43.650
시험해 볼 수 있습니다.

83
00 : 03 : 41.820 -> 00 : 03 : 45.060
그러나 비용 함수가 증가해 버려서 계획을 
벗어나 버렸다면,

84
00 : 03 : 43.650 -> 00 : 03 : 47.250
다른 값으로 또 변경할 수 있습니다.

85
00 : 03 : 45.060 -> 00 : 03 : 49.709
이 경우 비용이 빠르게 떨어지긴 하지만

86
00 : 03 : 47.250 -> 00 : 03 : 51.780
그러나 더 높은 비용으로 수렴하게 됩니다.

87
00 : 03 : 49.709 -> 00 : 03 : 53.670
그래서 또 다른 값을 넣어 보는겁니다.

88
00 : 03 : 51.780 -> 00 : 03 : 55.530
표시된 것처럼 비용 함수 J를 발견한다면

89
00 : 03 : 53.670 -> 00 : 03 : 57.840
시도한 값의 집합을 기반으로

90
00 : 03 : 55.530 -> 00 : 04 : 00.870
당신은 α의 마지막 값이

91
00 : 03 : 57.840 -> 00 : 04 : 02.790
효율적인 학습을 하게 해주고,

92
00 : 04 : 00.870 -> 00 : 04 : 04.290
비용 함수 J를 더 낮은 값으로 수렴하게 
해준다는 것을 발견할 겁니다.

93
00 : 04 : 02.790 -> 00 : 04 : 06.720
그래서 이 값을 learning rate로

94
00 : 04 : 04.290 -> 00 : 04 : 08.040
사용하기로 결정하는 겁니다.

95
00 : 04 : 06.720 -> 00 : 04 : 10.170
이전 슬라이드에서 여러분은 다양한

96
00 : 04 : 08.040 -> 00 : 04 : 11.489
하이퍼 매개 변수가 있다는 것을 보셨을 겁니다.

97
00 : 04 : 10.170 -> 00 : 04 : 13.830
여러분이

98
00 : 04 : 11.489 -> 00 : 04 : 15.450
새 모델을 만들 때

99
00 : 04 : 13.830 -> 00 : 04 : 17.940
하이퍼 매개 변수의 최적 값이 무엇일지

100
00 : 04 : 15.450 -> 00 : 04 : 20.580
미리 아는 것은 어렵습니다.

101
00 : 04 : 17.940 -> 00 : 04 : 22.169
그래서 보통 우리는

102
00 : 04 : 20.580 -> 00 : 04 : 24.570
많은 다른 값을 먼저 시도하는 겁니다.

103
00 : 04 : 22.169 -> 00 : 04 : 26.970
여기 있는 그림의 모델처럼

104
00 : 04 : 24.570 -> 00 : 04 : 28.440
다른 매개 변수를 설정 해보세요.

105
00 : 04 : 26.970 -> 00 : 04 : 31.140
예를 들어 5 개의 숨겨진 레이어로 하고, 
몇 개의 숨겨진 유닛을 설정해서

106
00 : 04 : 28.440 -> 00 : 04 : 34.140
모델을 빌드하고 실행하여 작동 방식을 확인 해보세요.

107
00 : 04 : 31.140 -> 00 : 04 : 36.180
그런 다음 최적 값을 찾을 때까지 반복하십시오.

108
00 : 04 : 34.140 -> 00 : 04 : 38.340
이 슬라이드의 제목은 보시는 것처럼

109
00 : 04 : 36.180 -> 00 : 04 : 40.740
'딥러닝을 적용하는 것은 경험 기반 프로세스' 입니다.

110
00 : 04 : 38.340 -> 00 : 04 : 42.419
경험 기반 프로세스라는 의미는

111
00 : 04 : 40.740 -> 00 : 04 : 45.330
끊임없이 시도해서 최적의 값을 찾으라는 의미입니다.

112
00 : 04 : 42.419 -> 00 : 04 : 47.190
내가 본 또 다른 현상은

113
00 : 04 : 45.330 -> 00 : 04 : 48.810
딥러닝은 이제 많은 분야에서

114
00 : 04 : 47.190 -> 00 : 04 : 51.990
적용되고 있다는 겁니다.

115
00 : 04 : 48.810 -> 00 : 04 : 53.789
컴퓨터 비전, 음성 인식

116
00 : 04 : 51.990 -> 00 : 04 : 55.500
자연 언어 처리,

117
00 : 04 : 53.789 -> 00 : 04 : 59.250
또 많은 구조화 된 데이터 어플리케이션에도 
적용됩니다.

118
00 : 04 : 55.500 -> 00 : 05 : 02.430
온라인 광고 또는 웹 검색

119
00 : 04 : 59.250 -> 00 : 05 : 05.640
또는 제품 권장 사항 등

120
00 : 05 : 02.430 -> 00 : 05 : 08.190
제가 여기서 다음과 같은 상황을 관찰했습니다.

121
00 : 05 : 05.640 -> 00 : 05 : 10.080
현장에서 다른 영역으로의 확장을 

122
00 : 05 : 08.190 -> 00 : 05 : 12.060
시도하는 연구원의 경우에는

123
00 : 05 : 10.080 -> 00 : 05 : 14.400
때로는 자신이 이전에 종사했던 분야의 
Hyper parameter에 대한 직관을 완벽하게

124
00 : 05 : 12.060 -> 00 : 05 : 16.590
이어받을 수도 있습니다.
그러나 때로는 그렇지 않을 수도 있습니다.

125
00 : 05 : 14.400 -> 00 : 05 : 17.849
그래서 저는 특히 새로운 프로젝트를

126
00 : 05 : 16.590 -> 00 : 05 : 20.970
시작하시는 모두에게 추천합니다.

127
00 : 05 : 17.849 -> 00 : 05 : 23.550
다른 값들로 여러 번 시도하여

128
00 : 05 : 20.970 -> 00 : 05 : 25.500
결과가 만족스러운지 확인해 보시기 바랍니다.

129
00 : 05 : 23.550 -> 00 : 05 : 27.930
다음 단원에서는

130
00 : 05 : 25.500 -> 00 : 05 : 30.780
가장 적합한 값을 얻는 방법에 대한

131
00 : 05 : 27.930 -> 00 : 05 : 32.070
체계적인 접근 방법을 살펴보겠습니다.

132
00 : 05 : 30.780 -> 00 : 05 : 33.570
두 번째 상황은 이미 필드에서

133
00 : 05 : 32.070 -> 00 : 05 : 35.220
오랫동안 일 해왔다면

134
00 : 05 : 33.570 -> 00 : 05 : 37.979
예를 들어, 온라인 광고를 연구하고 있다면

135
00 : 05 : 35.220 -> 00 : 05 : 39.930
당신의 연구가 진행될수록

136
00 : 05 : 37.979 -> 00 : 05 : 41.580
learning rate나 숨겨진 유닛의 개수 등의

137
00 : 05 : 39.930 -> 00 : 05 : 43.830
하이퍼 파라미터 값들이

138
00 : 05 : 41.580 -> 00 : 05 : 46.440
변경 될 것이라는 겁니다.

139
00 : 05 : 43.830 -> 00 : 05 : 49.229
예를들어 모델의 매개 변수가

140
00 : 05 : 46.440 -> 00 : 05 : 51.750
현재에 최적 값으로 설정 되었지만

141
00 : 05 : 49.229 -> 00 : 05 : 53.430
1 년 후에는

142
00 : 05 : 51.750 -> 00 : 05 : 55.650
이것들은 최적의 값이 아닙니다.

143
00 : 05 : 53.430 -> 00 : 05 : 57.840
컴퓨터 기반 시설 때문일 수도 있습니다.

144
00 : 05 : 55.650 -> 00 : 05 : 59.789
예를 들어, CPU GPU 유형이

145
00 : 05 : 57.840 -> 00 : 06 : 01.560
더 좋아졌거나 등의 상황이죠.

146
00 : 05 : 59.789 -> 00 : 06 : 03.659
그래서 여기에 제 경험을 말씀 드리자면

147
00 : 06 : 01.560 -> 00 : 06 : 05.070
오랫동안 지속되는 프로젝트를 해결할때

148
00 : 06 : 03.659 -> 00 : 06 : 06.659
예를 들어, 몇 년 동안

149
00 : 06 : 05.070 -> 00 : 06 : 09.030
몇 달마다 수시로 할 수도 있습니다.

150
00 : 06 : 06.659 -> 00 : 06 : 10.800
몇 가지 다른 하이퍼 매개 변수 값을 시도해보고

151
00 : 06 : 09.030 -> 00 : 06 : 12.570
이것이 가장 적합한 값인지에 대해서

152
00 : 06 : 10.800 -> 00 : 06 : 15.150
계속 체크 해보시기 바랍니다.

153
00 : 06 : 12.570 -> 00 : 06 : 17.280
저는 이와 같은

154
00 : 06 : 15.150 -> 00 : 06 : 18.779
경험이 축적 된 후

155
00 : 06 : 17.280 -> 00 : 06 : 19.870
최적의 하이퍼 파라미터 값에 대한 직감이

156
00 : 06 : 18.779 -> 00 : 06 : 21.820
천천히 발전 할 것이라고 생각합니다.

157
00 : 06 : 19.870 -> 00 : 06 : 24.010
솔직히 말하면 하이퍼 매개 변수의 값을

158
00 : 06 : 21.820 -> 00 : 06 : 25.510
하나씩 넣어봐야 된다는 것은

159
00 : 06 : 24.010 -> 00 : 06 : 27.940
딥러닝이라는 분야에서 상당히 불만족스러운

160
00 : 06 : 25.510 -> 00 : 06 : 30.160
부분일 수 있다고 생각합니다.

161
00 : 06 : 27.940 -> 00 : 06 : 32.200
그러나 이것은 딥러닝에 대한 연구가
162
00 : 06 : 30.160 -> 00 : 06 : 33.850
아직 초기 단계이기 때문입니다.

163
00 : 06 : 32.200 -> 00 : 06 : 36.190
아마 곧

164
00 : 06 : 33.850 -> 00 : 06 : 38.350
하이퍼 파라미터의 최적 값을 
선택하는 것에 관한

165
00 : 06 : 36.190 -> 00 : 06 : 41.260
더 좋은 방법이 있을거라고 생각합니다.

166
00 : 06 : 38.350 -> 00 : 06 : 43.630
이것은 끊임없이 변화하는

167
00 : 06 : 41.260 -> 00 : 06 : 45.910
CPU GPU 네트워크 및 데이터베이스로 인해 가능합니다.

168
00 : 06 : 43.630 -> 00 : 06 : 47.680
하지만 이러한 방법은 일정 시간 동안 
일관성이없는 경향이 있습니다.

169
00 : 06 : 45.910 -> 00 : 06 : 49.360
그래서 여전히 계속 노력할 필요가 있습니다.

170
00 : 06 : 47.680 -> 00 : 06 : 50.860
하이퍼 매개 변수에 대해 계속해서 
다른 값을 넣어보는 노력 말이죠.

171
00 : 06 : 49.360 -> 00 : 06 : 52.479
cross validation set이나

172
00 : 06 : 50.860 -> 00 : 06 : 54.100
또는 다른 컬렉션을 이용해서

173
00 : 06 : 52.479 -> 00 : 06 : 56.350
최적의 솔루션을 선택하시기 바랍니다.

174
00 : 06 : 54.100 -> 00 : 06 : 58.870
여기까지가 현재 비디오에서 얘기한 내용입니다.

175
00 : 06 : 56.350 -> 00 : 07 : 01.030
하이퍼 파라미터에 대한 간단한 설명이었습니다.

176
00 : 06 : 58.870 -> 00 : 07 : 03.280
두 번째 수업에서 우리는

177
00 : 07 : 01.030 -> 00 : 07 : 06.040
체계적으로 하이퍼 파라미터의 공간을 
탐색하는 방법에 대해 배울겁니다.

178
00 : 07 : 03.280 -> 00 : 07 : 07.570
그러나 이 비디오를 배운 후에

179
00 : 07 : 06.040 -> 00 : 07 : 09.430
사실, 당신은 이미 프로그래밍 작업을 완료하는데

180
00 : 07 : 07.570 -> 00 : 07 : 11.470
필요한 도구와 방법에 대한
모든 것을 가지고 있습니다.

181
00 : 07 : 09.430 -> 00 : 07 : 14.050
다음 비디오에서는 다음에 대한 질문에 대해 
공부할 겁니다.

182
00 : 07 : 11.470 -> 00 : 07 : 16.150
딥러닝과 인간 두뇌 사이의 관계는 무엇입니까?

183
00 : 07 : 14.050 -> 00 : 07 : 18.660
저는 또한 몇 가지 아이디어를 공유 할 것힙니다.