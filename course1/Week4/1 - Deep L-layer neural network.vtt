WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 02.389
이 코스의 4 주째에
오신 것을 환영합니다

2
00 : 00 : 02.389 -> 00 : 00 : 06.354
지금까지 숨겨진 레이어가 하나인 신경망에서


3
00 : 00 : 06.354 -> 00 : 00 : 10.807
Forward propagation과 Back propagation과
로지스틱 회귀를보고 왔습니다

4
00 : 00 : 10.807 -> 00 : 00 : 13.064
그리고 벡터화에 대해서도 배웠고,
무작위로 초기화 하는 것이

5
00 : 00 : 13.064 -> 00 : 00 : 15.936
중요한 순간에 대해 배웠습니다

6
00 : 00 : 15.936 -> 00 : 00 : 19.417
지난 몇 주 동안 숙제를 하면서
구현을 잘 해봤다면,

7
00 : 00 : 19.417 -> 00 : 00 : 21.378
자신의 생각대로
잘 작동하는 것을 보고 왔을 겁니다.

8
00 : 00 : 21.378 -> 00 : 00 : 21.977
지금까지

9
00 : 00 : 21.977 -> 00 : 00 : 26.718
신경망을 구현하는 데 필요한
아이디어들은 거의 보고 왔습니다

10
00 : 00 : 26.718 -> 00 : 00 : 30.453
이번 주에는 이러한 아이디어 들을
연결해서

11
00 : 00 : 30.453 -> 00 : 00 : 33.669
자신의 신경망을 실제로
구현 할 수 있도록 해볼겁니다.

12
00 : 00 : 33.669 -> 00 : 00 : 36.406
이번주의 프로그래밍 과제가
길기 때문에

13
00 : 00 : 36.406 -> 00 : 00 : 39.457
작업이 많아지고 있습니다
따라서 이번 주 비디오는

14
00 : 00 : 39.457 -> 00 : 00 : 43.784
짧게 해 두었습니다
비디오는 빠르게보고

15
00 : 00 : 43.784 -> 00 : 00 : 48.107
더 많은 시간을 프로그램 연습에
투자 하시라는 의미입니다.

16
00 : 00 : 48.107 -> 00 : 00 : 52.537
이에 따라 자기 스스로의 신경망을 만드는 
자랑스러운 경험을 해보셨으면 좋겠습니다.

17
00 : 00 : 52.537 -> 00 : 00 : 55.576
심층 신경망은
무엇일까요?

18
00 : 00 : 55.576 -> 00 : 00 : 59.225
로지스틱 회귀에서 이 그림은
보신적이 있을겁니다.

19
00 : 00 : 59.225 -> 00 : 01 : 03.439
숨겨진 레이어가 1 개인 신경망도
보고 왔습니다

20
00 : 01 : 03.439 -> 00 : 01 : 07.925
이것은 숨겨진 레이어가 2 개인
신경망의 예와

21
00 : 01 : 07.925 -> 00 : 01 : 10.661
숨겨진 레이어가 5 개인 신경망의 예입니다

22
00 : 01 : 10.661 -> 00 : 01 : 15.744
이것을 보면 로지스틱 모델은
매우 얕은 모델이고

23
00 : 01 : 15.744 -> 00 : 01 : 19.947
한편 여기 모델은
그것보다 훨씬 깊은 모델 이라고 할 수 있죠.

24
00 : 01 : 19.947 -> 00 : 01 : 23.585
얕고 깊고는
정도의 문제입니다

25
00 : 01 : 23.585 -> 00 : 01 : 26.952
오른쪽은 숨겨진 레이어 1 개인
신경망이고

26
00 : 01 : 26.952 -> 00 : 01 : 30.052
여기 있는건 2개인 신경망입니다

27
00 : 01 : 30.052 -> 00 : 01 : 34.880
레이어를 세는 방법을 기억 하십니까?
입력 층은 세지 않고

28
00 : 01 : 34.880 -> 00 : 01 : 38.076
숨겨진 레이어 및 출력 계층 만
셉니다

29
00 : 01 : 38.076 -> 00 : 01 : 42.849
이것은 2 층 신경망이라
얕은 정도 이지만,

30
00 : 01 : 42.849 -> 00 : 01 : 45.961
로지스틱 회귀 보다는
얕지 않습니다

31
00 : 01 : 45.961 -> 00 : 01 : 50.086
기술적으로 말하면 로지스틱 회귀는
레이어 하나의 신경망입니다

32
00 : 01 : 50.086 -> 00 : 01 : 53.536
지난 몇년 동안
AI 또는

33
00 : 01 : 53.536 -> 00 : 01 : 58.561
머신러닝 커뮤니티에서는
매우 깊은 신경망에서만 학습이 가능하고,

34
00 : 01 : 58.561 -> 00 : 02 : 03.590
얕은 신경망 모델에서는
학습할 수 없는 경우가 많다고 실감 해오고 있습니다

35
00 : 02 : 03.590 -> 00 : 02 : 08.119
물론 어떤 문제라도, 사전에 어느 정도
깊이의 네트워크가 좋은지를 예측하는 것은

36
00 : 02 : 08.119 -> 00 : 02 : 10.163
어려울 것입니다

37
00 : 02 : 10.163 -> 00 : 02 : 14.305
따라서 로지스틱 회귀를 시도해보고,
숨겨진 레이어 하나로 시도해보고,

38
00 : 02 : 14.305 -> 00 : 02 : 19.200
2 개로도 시도 해보고,
숨겨진 레이어의 수로 다양한 값을 시도해 보면서

39
00 : 02 : 19.200 -> 00 : 02 : 22.739
그냥 하나의 Hyper parameter 처럼 
생각하시면 될겁니다.

40
00 : 02 : 22.739 -> 00 : 02 : 27.515
Cross validation set이나 Developement set을 
통해 평가해 보시면 될겁니다.

41
00 : 02 : 27.515 -> 00 : 02 : 29.447
나중에
자세히 살펴 보겠습니다.

42
00 : 02 : 29.447 -> 00 : 02 : 33.998
지금은 심층 신경망을 나타내기 위한
표기 방법을 살펴 보겠습니다.

43
00 : 02 : 33.998 -> 00 : 02 : 39.147
여기 (1,2,3) 4층 짜리
신경망이 있습니다

44
00 : 02 : 40.974 -> 00 : 02 : 45.729
3 개의 숨겨진 레이어에서
숨겨진 레이어 단위의 수는

45
00 : 02 : 45.729 -> 00 : 02 : 50.842
5,5,3 이고
하나의 출력 장치가 있습니다

46
00 : 02 : 50.842 -> 00 : 02 : 52.731
우리가 사용할 표기 방법은

47
00 : 02 : 52.731 -> 00 : 02 : 56.591
대문자 L로
네트워크 계층의 수를 나타냅니다

48
00 : 02 : 56.591 -> 00 : 03 : 03.881
이 경우에는 L = 4 라고 써서
레이어의 개수를 나타냅니다.

49
00 : 03 : 03.881 -> 00 : 03 : 11.880
n[l]를 사용하여
소문자 l의 층에서

50
00 : 03 : 11.880 -> 00 : 03 : 17.101
노드 나 유닛의 수를 나타냅니다

51
00 : 03 : 17.101 -> 00 : 03 : 22.501
이에 색인을 붙이면
입력 층을 0으로

52
00 : 03 : 22.501 -> 00 : 03 : 28.950
여기는 1이고 여기는 2이고
여기는 3이고 여기는 4번째 레이어 입니다.

53
00 : 03 : 28.950 -> 00 : 03 : 33.822
이렇게하면 n[1]은

54
00 : 03 : 33.822 -> 00 : 03 : 39.529
첫 번째 숨겨진 레이어에서 n [1] = 5입니다
5 개의 숨겨진 유닛이 있기 때문입니다

55
00 : 03 : 39.529 -> 00 : 03 : 43.623
이것에 대해서는 n [2] = 5 이고,

56
00 : 03 : 43.623 -> 00 : 03 : 48.810
두 번째 숨겨진 레이어 단위의 수 입니다.

57
00 : 03 : 48.810 -> 00 : 03 : 53.315
n [3] = 3입니다

58
00 : 03 : 53.315 -> 00 : 03 : 59.459
n [4] = n [L] = 1 이고
출력 유닛의 개수는 1입니다

59
00 : 03 : 59.459 -> 00 : 04 : 04.101
대문자 L은 4이기 때문입니다

60
00 : 04 : 04.101 -> 00 : 04 : 08.878
이 입력 층 부분은

61
00 : 04 : 08.878 -> 00 : 04 : 13.003
n [0] = nx = 3입니다

62
00 : 04 : 13.003 -> 00 : 04 : 17.879
이것이 각 계층에있는 노드의 수를
나타내는 표기 방법

63
00 : 04 : 17.879 -> 00 : 04 : 18.463
입니다

64
00 : 04 : 18.463 -> 00 : 04 : 23.913
각 계층 L에 대해

65
00 : 04 : 23.913 -> 00 : 04 : 30.196
a[l] 라고 써서
l 층의 activation을 나타냅니다

66
00 : 04 : 30.196 -> 00 : 04 : 34.669
추후에 Forward propagation에서
이것을 볼 수 있습니다

67
00 : 04 : 34.669 -> 00 : 04 : 40.791
a [l] =  g [l] (Z [l])에서

68
00 : 04 : 40.791 -> 00 : 04 : 46.440
아마도 활성화는
l를 레이어의 번호로 색인화 됩니다.

69
00 : 04 : 46.440 -> 00 : 04 : 51.736
W [l]을 사용하여 weight를 나타냅니다

70
00 : 04 : 51.736 -> 00 : 04 : 55.973
weight로 z [l]로 계산합니다

71
00 : 04 : 55.973 -> 00 : 05 : 00.714
b [l]도 z [l]을 계산하는 데 사용됩니다

72
00 : 05 : 00.714 -> 00 : 05 : 07.114
마지막으로 표기 방법을 얘기하면
입력 feature은 X은

73
00 : 05 : 07.114 -> 00 : 05 : 12.215
레이어 0의 activation 이니까
a [0] = X입니다

74
00 : 05 : 12.215 -> 00 : 05 : 17.133
마지막 층의 활성화는
a [L] = y hat 입니다

75
00 : 05 : 17.133 -> 00 : 05 : 25.275
즉, a [L]은 신경망 예측치인 
y hat 예측 값과 같습니다.

76
00 : 05 : 25.275 -> 00 : 05 : 28.200
이제 심층 신경망이
무엇인지 알 수 있었고

77
00 : 05 : 28.200 -> 00 : 05 : 32.427
심층 네트워크를 계산해 나갈때
향후 사용해가는 표기도 배웠습니다

78
00 : 05 : 32.427 -> 00 : 05 : 36.457
이 비디오에서 많은 표현을
소개했지만

79
00 : 05 : 36.457 -> 00 : 05 : 40.916
만약 간단한 표기를 잊어 버리셨다면, 코스의 
사이트에 표기 시트를 올려놨으니까 참고하세요.

80
00 : 05 : 40.916 -> 00 : 05 : 45.089
다양한 표현을 찾을 수 있습니다
표기 방법의 시트와 가이드도 있습니다

81
00 : 05 : 45.089 -> 00 : 05 : 48.958
다음은 Forward propagation은
이런 종류의 네트워크에서 어떻게되는지

82
00 : 05 : 48.958 -> 00 : 05 : 49.620합니다
보여주려고 합니다.

83
00 : 05 : 49.620 -> 00 : 05 : 51.019
다음 비디오로 가봅시다.