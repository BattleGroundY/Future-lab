WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 03.339
깊은 신경망이 많은 문제에 대해 
정말 효과적으로 적용된다는 사실은

2
00 : 00 : 03.339 -> 00 : 00 : 07.073
여러분도 알고 있을겁니다. 
다만, 이런 경우 신경망이 크기만 해선 안되고

3
00 : 00 : 07.073 -> 00 : 00 : 10.718
특히 숨겨진 레이어를 많이 가진 심층 
네트워크로 구성되야 합니다.

4
00 : 00 : 10.718 -> 00 : 00 : 12.208
이것은 무엇일까요?

5
00 : 00 : 12.208 -> 00 : 00 : 15.833
몇 개 예를 보고 왜 심층 네트워크의 
효율이 좋은지 통찰력을

6
00 : 00 : 15.833 -> 00 : 00 : 17.720
얻어 봅시다.

7
00 : 00 : 17.720 -> 00 : 00 : 22.181
우선 심층 네트워크 계산은
무엇일까요

8
00 : 00 : 22.181 -> 00 : 00 : 25.393
만약 얼굴 인식 및 얼굴 위치 인식 시스템을
만들고자하는 경우에는

9
00 : 00 : 25.393 -> 00 : 00 : 29.631
이런 느낌의 것을
심층 신경망 이라고 했을 것입니다

10
00 : 00 : 29.631 -> 00 : 00 : 35.059
얼굴의 사진을 입력하면
신경망의 첫 번째 계층은

11
00 : 00 : 35.059 -> 00 : 00 : 40.000
feature detector와 edge detector가
될겁니다.

12
00 : 00 : 40.000 -> 00 : 00 : 45.519
이 예제에서는 이 이미지에
20 개 상당의 숨겨진 레이어 단위가 있는 신경망이

13
00 : 00 : 45.519 -> 00 : 00 : 48.017
계산하는 것 이라고 하겠습니다.

14
00 : 00 : 48.017 -> 00 : 00 : 52.357
20 개의 숨겨진 유닛은
이 작은 사각형으로 표현합니다

15
00 : 00 : 52.357 -> 00 : 00 : 57.325
예를 들어 이 왼쪽에 있는 
숨겨진 레이어 단위는

16
00 : 00 : 57.325 -> 00 : 01 : 01.978
이미지의 어느 곳에 방향의 경계가 있는지를
찾으려고합니다

17
00 : 01 : 01.978 -> 00 : 01 : 05.914
이 오른쪽 아래 있는 Hidden unit은

18
00 : 01 : 05.914 -> 00 : 01 : 09.955
아마 이 수평 경계가
이미지의 어디에 있는지를 찾고 있습니다

19
00 : 01 : 09.955 -> 00 : 01 : 13.184
후의 과정에서
convolutional network를 이야기 할 때

20
00 : 01 : 13.184 -> 00 : 01 : 16.129
이 표현들에 대해
더 잘 이해할 수 있을겁니다.

21
00 : 01 : 16.129 -> 00 : 01 : 19.562
첫 번째 신경망의 층이 하는 것은
이 이미지의 가운데를 보고

22
00 : 01 : 19.562 -> 00 : 01 : 22.690
어디에 경계가 있는지를
찾고자 하는 것이라고 생각하면 됩니다.

23
00 : 01 : 22.690 -> 00 : 01 : 27.356
픽셀을 그룹화하여
이 이미지의 경계가

24
00 : 01 : 27.356 -> 00 : 01 : 28.730
어디있는지 한번 찾아봅시다.

25
00 : 01 : 28.730 -> 00 : 01 : 34.670
검출 된 경계들을 사용하여 그룹화하여
얼굴의 부분들을 형성 할 수 있습니다

26
00 : 01 : 34.670 -> 00 : 01 : 40.289
예를 들어 눈을 찾으려고 하는
신경 뉴런이 있고

27
00 : 01 : 40.289 -> 00 : 01 : 44.480
이 코 일부를 찾으려는 
다른 뉴런있을 것입니다

28
00 : 01 : 44.480 -> 00 : 01 : 47.463
즉 많은 경계를 결합해서

29
00 : 01 : 47.463 -> 00 : 01 : 50.970
다양한 얼굴의 부분을
찾을 수 있습니다

30
00 : 01 : 50.970 -> 00 : 01 : 56.035
마지막으로 눈과 코와 귀와 턱 등
다양한 얼굴의 부분을 결합해서

31
00 : 01 : 56.035 -> 00 : 02 : 01.006
다양한 종류의 얼굴을
인식하거나 감지할 수

32
00 : 02 : 01.006 -> 00 : 02 : 03.564
있다고 생각합니다.

33
00 : 02 : 03.564 -> 00 : 02 : 07.755
직관적이긴 하지만
시작 부분의 신경망 레이어는

34
00 : 02 : 07.755 -> 00 : 02 : 10.190
경계와 같은 간단한 기능을
검출하는 것으로 간주합니다

35
00 : 02 : 10.190 -> 00 : 02 : 14.573
다음 나중의 레이어에서
조립하고

36
00 : 02 : 14.573 -> 00 : 02 : 17.625
더 복잡한 함수를
학습할 수도 있을겁니다.

37
00 : 02 : 17.625 -> 00 : 02 : 23.640
회선 네트워크에 대해 이야기할 때,
이러한 표현들을 더 이해할 수 있을겁니다.

38
00 : 02 : 23.640 -> 00 : 02 : 26.203
이 표현의 기술적 인 측면을 
하나 이야기하자면,

39
00 : 02 : 26.203 -> 00 : 02 : 29.802
경계 감지기는
이미지의 비교적 좁은 범위를 보고 있습니다.

40
00 : 02 : 29.802 -> 00 : 02 : 31.703
아마 이미지의 이 부분 정도입니다

41
00 : 02 : 31.703 -> 00 : 02 : 36.616
얼굴 부분 탐지기는
아마 더 넓은 영역을 볼 수 있습니다

42
00 : 02 : 36.616 -> 00 : 02 : 41.308
이 예에서 이해해 주었으면 하는 것은
경계와 같은 간단한 것을 먼저 찾아

43
00 : 02 : 41.308 -> 00 : 02 : 43.675
그것을 쌓아 간다는 것입니다

44
00 : 02 : 43.675 -> 00 : 02 : 47.216
그리고 그것을 조합해서
눈이나 코 등의보다 복잡한 것을 감지하고,

45
00 : 02 : 47.216 -> 00 : 02 : 50.530
검출 한 것을 또 조합해서
더 복잡한 것을 조립하는 겁니다.

46
00 : 02 : 50.530 -> 00 : 02 : 55.665
간단한 것부터 복잡한 것으로 이동하는
계층적 표현과 구성적 표현 등의

47
00 : 02 : 55.665 -> 00 : 02 : 58.508
이런 유형은

48
00 : 02 : 58.508 -> 00 : 03 : 04.114
이미지와 얼굴 인식 이외에도 또
적용 할 수 있습니다

49
00 : 03 : 04.114 -> 00 : 03 : 08.593
예를 들어 음성 인식 시스템을 
만들려고 할 때입니다

50
00 : 03 : 08.593 -> 00 : 03 : 10.908
음성을 표현하는 것은 어렵지만

51
00 : 03 : 10.908 -> 00 : 03 : 15.684
소리 파일을 입력하면
첫 번째 신경망 레이어에서는

52
00 : 03 : 15.684 -> 00 : 03 : 20.863
볼륨이 위아래 같은 음파의 형태 등의 
특징을 학습하려고

53
00 : 03 : 20.863 -> 00 : 03 : 21.703
할 것 입니다.

54
00 : 03 : 21.703 -> 00 : 03 : 26.869
화이트 노이즈 나
혀를 미끄러지는 소리이거나

55
00 : 03 : 26.869 -> 00 : 03 : 27.903
음의 높이는

56
00 : 03 : 27.903 -> 00 : 03 : 31.124
이러한 낮은 단계의 주파수
소리의 특징을 감지할 겁니다.

57
00 : 03 : 31.124 -> 00 : 03 : 34.233
세부적 소리의 형태를
결합해서

58
00 : 03 : 34.233 -> 00 : 03 : 37.937
소리의 기본 단위를
검출하는 것을 학습합니다

59
00 : 03 : 37.937 -> 00 : 03 : 40.297
언어학에서는
'음소' 입니다

60
00 : 03 : 40.297 -> 00 : 03 : 45.098
예를 들어 고양이의 단어 cat는
c와 a가 음소입니다

61
00 : 03 : 45.098 -> 00 : 03 : 46.787
t도 음소입니다

62
00 : 03 : 46.787 -> 00 : 03 : 49.987
소리의 기본 단위를 찾으면

63
00 : 03 : 49.987 -> 00 : 03 : 54.688
이를 조합해서
소리 속 단어를 학습합니다

64
00 : 03 : 54.688 -> 00 : 03 : 58.270
이 또한 조합해서

65
00 : 03 : 58.270 -> 00 : 04 : 02.912
절 전체 또는 문장 전체를
인식합니다

66
00 : 04 : 02.912 -> 00 : 04 : 07.572
이처럼 여러 숨겨진 레이어를 가진
심층 신경망 경우

67
00 : 04 : 07.572 -> 00 : 04 : 10.477
시작 부분의 레이어는
미세한 수준의 간단한 특징을 배우고

68
00 : 04 : 10.477 -> 00 : 04 : 15.339
나중에 깊은 층에서는
이미 검출한 간단한 특징을 조합해서

69
00 : 04 : 15.339 -> 00 : 04 : 19.392
특정 단어 나 또한 어구 또는 문장이나 전체 등
더 복잡한 것을

70
00 : 04 : 19.392 -> 00 : 04 : 21.040
감지하여

71
00 : 04 : 21.040 -> 00 : 04 : 24.745
음성 인식을 실시합니다

72
00 : 04 : 24.745 -> 00 : 04 : 30.168
여기에 볼 수 있는 것은
초기의 층이 계산 하고 있는 것은

73
00 : 04 : 30.168 -> 00 : 04 : 35.673
경계가 어디에 있는지 등
입력이 비교적 간단한 함수라는 겁니다.

74
00 : 04 : 35.673 -> 00 : 04 : 41.046
네트워크의 깊은 곳에는
놀라울정도로 복잡한 함수가 있습니다

75
00 : 04 : 41.046 -> 00 : 04 : 44.876
얼굴을 검출하고
단어 나 어구 또는 문장을 찾는 함수 등이죠.

76
00 : 04 : 44.876 -> 00 : 04 : 48.767
심층 신경망과
인간의 뇌가 유사하다고

77
00 : 04 : 48.767 -> 00 : 04 : 52.656
말하고 싶은 사람도 있습니다
우리와 신경 과학자들이 그렇게 믿듯이 말이죠.

78
00 : 04 : 52.656 -> 00 : 04 : 57.162
사람의 뇌도 눈에 들어오는 경계처럼
비교적 간단한 것을 먼저 감지하고

79
00 : 04 : 57.162 -> 00 : 05 : 00.370
그런 다음 얼굴과 같은보다 복잡한 것을
검색하도록 쌓아 간다고

80
00 : 05 : 00.370 -> 00 : 05 : 02.440
생각합니다

81
00 : 05 : 02.440 -> 00 : 05 : 05.038
이처럼 딥러닝과
인간의 뇌가

82
00 : 05 : 05.038 -> 00 : 05 : 08.276
유사하다라고 생각하는 것은
조금 위험하다고 생각합니다

83
00 : 05 : 08.276 -> 00 : 05 : 13.301
물론 인간의 뇌가 작동하는 방법에 대한 
이런 생각들은, 사실인 부분도 있습니다

84
00 : 05 : 13.301 -> 00 : 05 : 18.102
아마도 인간의 뇌는 '경계' 처럼 
단순한 형태를 감지하고

85
00 : 05 : 18.102 -> 00 : 05 : 22.598
그것을 쌓아 더 복잡한 것을
구성한다는 사실입니다

86
00 : 05 : 22.598 -> 00 : 05 : 27.430
이것이 딥러닝에는 지속적으로
도움이 되었습니다.

87
00 : 05 : 27.430 -> 00 : 05 : 29.850
인간의 뇌와 생물학적 두뇌에 관해서는

88
00 : 05 : 29.850 -> 00 : 05 : 33.065
이번주 후에 나오는 동영상에서
더 깊이 살펴 보겠습니다

89
00 : 05 : 35.534 -> 00 : 05 : 40.407
심층 네트워크가 좋은 효율을 내는 
또 하나의  이유는

90
00 : 05 : 40.407 -> 00 : 05 : 42.756
다음과 같습니다

91
00 : 05 : 42.756 -> 00 : 05 : 47.868
이 결과는 회로 이론에서 비롯됩니다

92
00 : 05 : 47.868 -> 00 : 05 : 53.760
AND, OR, NOT 게이트 등의 다양한 논리 게이트에 의해
어떤 계산이 가능한가에 관한 것입니다

93
00 : 05 : 53.760 -> 00 : 05 : 58.860
간단히 말하자면, 비교적 좁지 만 
깊은 신경망으로 계산 가능한 함수들이 있습니다

94
00 : 05 : 58.860 -> 00 : 06 : 03.595
'좁다' 가 의미하는 것은
숨겨진 유닛의 수가 상대적으로 적다는 것입니다

95
00 : 06 : 03.595 -> 00 : 06 : 07.553
하지만 같은 함수를 얕은 네트워크
에서 계산할 경우에는

96
00 : 06 : 07.553 -> 00 : 06 : 09.178
즉, 충분한 숨겨진 레이어가 없는 경우

97
00 : 06 : 09.178 -> 00 : 06 : 13.296
기하 급수적으로 많은 Hidden unit들이
계산에 필요한 것입니다

98
00 : 06 : 13.296 -> 00 : 06 : 18.109
간단히 설명하기 위해
하나 예를 보겠습니다.

99
00 : 06 : 18.109 -> 00 : 06 : 21.423
모든 입력 특징에 대해
XOR 또는 parity(동등성)을

100
00 : 06 : 21.423 -> 00 : 06 : 23.349
계산을 하려고 합니다

101
00 : 06 : 23.349 -> 00 : 06 : 28.430
X1 XOR X2 XOR X3에 이어

102
00 : 06 : 28.430 -> 00 : 06 : 33.064
XOR Xn까지 계산합니다
nx의 feature가 있는 경우입니다

103
00 : 06 : 33.064 -> 00 : 06 : 39.924
XOR 트리를 만드는 경우
우선 X1과 X2의 XOR를 계산

104
00 : 06 : 39.924 -> 00 : 06 : 44.586
X3와 X4의 XOR를 계산합니다

105
00 : 06 : 44.586 -> 00 : 06 : 49.392
정확하게 말하면 AND 게이트와
NOT 게이트만을 사용하는 경우에는

106
00 : 06 : 49.392 -> 00 : 06 : 54.196
XOR를 계산하려면 1 층이 아닌
여러 계층이 필요하지만

107
00 : 06 : 54.196 -> 00 : 06 : 58.791
XOR 게이트를 이용하면 비교적 작은 회로로
XOR을 계산 할 수도 있습니다

108
00 : 06 : 58.791 -> 00 : 07 : 03.987
이처럼 계속 XOR 트리를 만들어 가고

109
00 : 07 : 03.987 -> 00 : 07 : 12.090
최종적으로 출력하는 회로가 나올 때까지 
계속해서 출력 y를 도출합니다.

110
00 : 07 : 12.090 -> 00 : 07 : 15.236
출력 y hat = y를 출력합니다

111
00 : 07 : 15.236 -> 00 : 07 : 18.398
모든 input bit(feature)의 XOR 
또는 parity 값입니다

112
00 : 07 : 18.398 -> 00 : 07 : 24.790
XOR를 계산하는 깊이는
O(log n)의 층수 만큼 일겁니다.

113
00 : 07 : 24.790 -> 00 : 07 : 27.410
이 형태의 XOR 트리입니다

114
00 : 07 : 27.410 -> 00 : 07 : 30.836
이 네트워크는 노드 수
또는 회로의 부품 수

115
00 : 07 : 30.836 -> 00 : 07 : 33.929
또는 게이트의 수는
그만큼 많지는 않습니다

116
00 : 07 : 33.929 -> 00 : 07 : 38.452
XOR를 계산하는데
게이트는 많이는 필요 없습니다

117
00 : 07 : 38.452 -> 00 : 07 : 43.458
하지만 만약 여러 개의 Hidden layer를 
가진 신경망을 사용할 수 없는 경우

118
00 : 07 : 43.458 -> 00 : 07 : 48.203
이 경우 O(log n)만큼의 레이어 이었지만
하나의 숨겨진 레이어에서

119
00 : 07 : 48.203 -> 00 : 07 : 53.382
이 함수를 계산하는 것을
강요하면 어떨까요?

120
00 : 07 : 53.382 -> 00 : 07 : 57.912
이 경우 모든 입력이
하나의 숨겨진 레이어로 간 후에

121
00 : 07 : 57.912 -> 00 : 08 : 02.116
이것이 Y를 출력합니다

122
00 : 08 : 02.116 -> 00 : 08 : 07.120
그리고 XOR 함수를 계산하려면

123
00 : 08 : 07.120 -> 00 : 08 : 12.124
이 숨겨진 레이어가
기하 급수적으로 커야합니다

124
00 : 08 : 12.124 -> 00 : 08 : 18.397
포괄적으로 2 N제곱 개의 조합을 
열거 할 필요가 있습니다.

125
00 : 08 : 18.397 -> 00 : 08 : 23.139
따라서 입력 비트의 가능한 조합은

126
00 : 08 : 23.139 -> 00 : 08 : 27.898
XOR 1 또는 0의 두 가지 결과가 있습니다.

127
00 : 08 : 27.898 -> 00 : 08 : 32.213
따라서 숨겨진 레이어는
비트 수가 기하 급수적으로 큰 것이

128
00 : 08 : 32.213 -> 00 : 08 : 33.554
필요합니다

129
00 : 08 : 33.554 -> 00 : 08 : 38.229
엄밀하게는 2의 n-1승 개의
숨겨진 유닛이 있고

130
00 : 08 : 38.229 -> 00 : 08 : 43.948
O(2 n) 개의 기하 급수적으로
많은 수의 비트 수가 됩니다

131
00 : 08 : 43.948 -> 00 : 08 : 49.149
깊은 네트워크에서 계산하는 것이
얕은 네트워크에서 계산하는 것보다

132
00 : 08 : 49.149 -> 00 : 08 : 55.275
왜 더 쉽게 계산할 수있는 지에 대한 
직관을 얻으셨나요?

133
00 : 08 : 55.275 -> 00 : 09 : 01.028
회로 이론의 결과는
심층 네트워크의 가치에 대한

134
00 : 09 : 01.028 -> 00 : 09 : 05.985
통찰력을 얻기 위해

135
00 : 09 : 05.985 -> 00 : 09 : 11.223
일반적으로 사용되는 결과 중 하나입니다

136
00 : 09 : 11.223 -> 00 : 09 : 13.600
추가적으로 심층 신경망을

137
00 : 09 : 13.600 -> 00 : 09 : 16.897
선호하는 이유 중 하나는

138
00 : 09 : 16.897 -> 00 : 09 : 22.204
딥러닝 이라는 용어가
브랜딩 되었기 때문 이기도합니다

139
00 : 09 : 22.204 -> 00 : 09 : 26.776
이들은 예전에는 다층의 숨겨진 레이어의 
신경망을 부르는 용어 였습니다.

140
00 : 09 : 26.776 -> 00 : 09 : 31.198
딥러닝 이라는 용어는 그저 굉장한 브랜드입니다
굉장히 깊다는 소리 입니다

141
00 : 09 : 31.198 -> 00 : 09 : 36.284
이 단어가 유행되면서
다층의 숨겨진 레이어의 신경망도

142
00 : 09 : 36.284 -> 00 : 09 : 39.622
새롭게 브랜드화 된 것이

143
00 : 09 : 39.622 -> 00 : 09 : 42.970
사람들의 관심을 잡는 데 
한몫했다고 생각합니다

144
00 : 09 : 42.970 -> 00 : 09 : 47.479
PR, 브랜딩과 관계없이
심층 네트워크는 실로 성적이 좋습니다.

145
00 : 09 : 47.479 -> 00 : 09 : 51.342
너무 많은 숨겨진 레이어를 사용하는 것을
주장 할 정도로 과장된 사람도 가끔 있는데

146
00 : 09 : 51.342 -> 00 : 09 : 55.500
나는 새로운 문제를 해결하기 위해서
우선 로지스틱 회귀에서

147
00 : 09 : 55.500 -> 00 : 09 : 58.803
시작 하는 경우도 많습니다. 그리고 
하나 나 두개의 숨겨진 레이어를 시도하고

148
00 : 09 : 58.803 -> 00 : 10 : 01.722
이것을 하이퍼 매개 변수로 사용합니다

149
00 : 10 : 01.722 -> 00 : 10 : 05.731
이것을 조정하는 매개 변수나
하이퍼 매개 변수를 사용해서

150
00 : 10 : 05.731 -> 00 : 10 : 07.935
신경망의
적절한 깊이를 찾는 데 사용합니다

151
00 : 10 : 07.935 -> 00 : 10 : 12.800
지난 몇 년 동안

152
00 : 10 : 12.800 -> 00 : 10 : 17.590
많은 수의 레이어를 가진 
매우 깊은 신경망이

153
00 : 10 : 17.590 -> 00 : 10 : 22.264
이 문제에 대한 최선의 모델이라는 것을
발견하는 흐름이 있어 왔습니다.

154
00 : 10 : 22.264 -> 00 : 10 : 27.605
이번에는 딥러닝이 효과가 좋은 이유에 대한
직관적인 이해를 해봤습니다.

155
00 : 10 : 27.605 -> 00 : 10 : 31.411
다음은 Forward prop 뿐만 아니라 Back prop을
구현하는 방법의 구조를

156
00 : 10 : 31.411 -> 00 : 10 : 33.769
보고 갑시다