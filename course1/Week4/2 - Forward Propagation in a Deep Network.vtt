WEBVTT

1
00 : 00 : 00.060 -> 00 : 00 : 04.380
마지막 동영상에서 L 개의 층을 가진

2
00 : 00 : 02.250 -> 00 : 00 : 06.150
심층 신경망이란 무엇인가,

3
00 : 00 : 04.380 -> 00 : 00 : 08.550
그리고 그런 네트워크를
표현하기 위해 사용되는

4
00 : 00 : 06.150 -> 00 : 00 : 10.650
표기법을 이야기했습니다.

5
00 : 00 : 08.550 -> 00 : 00 : 13.769
이 동영상에서는
딥 네트워크에서

6
00 : 00 : 10.650 -> 00 : 00 : 16.440
어떻게 Forward propagation을 할 것인지 
살펴 보겠습니다.

7
00 : 00 : 13.769 -> 00 : 00 : 18.660
우선 언제나처럼,
한 개의 training 데이터 X에서

8
00 : 00 : 16.440 -> 00 : 00 : 21.330
Forward prop가 어떻게 진행되는지 배우고,

9
00 : 00 : 18.660 -> 00 : 00 : 22.920
그 후, 모든 training 샘플에 대해

10
00 : 00 : 21.330 -> 00 : 00 : 24.810
단번에 Forward prop 하기 위해

11
00 : 00 : 22.920 -> 00 : 00 : 26.849
벡터화하는 방법에 대해서

12
00 : 00 : 24.810 -> 00 : 00 : 29.660
이야기합니다.

13
00 : 00 : 26.849 -> 00 : 00 : 32.579
한개의 training 샘플 x가 주어 졌을 때,

14
00 : 00 : 29.660 -> 00 : 00 : 34.800
첫 번째 층의 활성화를 계산하는 방법입니다.

15
00 : 00 : 32.579 -> 00 : 00 : 42.329
첫 번째 층의 계산은

16
00 : 00 : 34.800 -> 00 : 00 : 48.239
z [1] = W [1] x + b [1]입니다.

17
00 : 00 : 42.329 -> 00 : 00 : 51.120
W [1] 과 b [1]은

18
00 : 00 : 48.239 -> 00 : 00 : 53.879
첫번째 층의 활성화에 영향을 
미치는 매개 변수입니다.

19
00 : 00 : 51.120 -> 00 : 00 : 56.899
이것이 신경망의 첫번째 층에서의 계산 입니다. 

20
00 : 00 : 53.879 -> 00 : 00 : 59.280
다음은 이 층의 활성화를 계산합니다.

21
00 : 00 : 56.899 -> 00 : 01 : 04.979
g (z [1]) 네요.

22
00 : 00 : 59.280 -> 00 : 01 : 06.810
활성화 함수 g는

23
00 : 01 : 04.979 -> 00 : 01 : 09.090
층에 따라 다르기 때문에

24
00 : 01 : 06.810 -> 00 : 01 : 11.010
[1]이 라는 인덱스를 붙여

25
00 : 01 : 09.090 -> 00 : 01 : 12.689
첫번째 층의 활성화 함수 임을 나타냅니다.

26
00 : 01 : 11.010 -> 00 : 01 : 13.320
자, 첫번째 층의 활성화 함수

27
00 : 01 : 12.689 -> 00 : 01 : 18.360
계산을 끝냈습니다.

28
00 : 01 : 13.320 -> 00 : 01 : 24.470
2번째 층은 어떨까요?

29
00 : 01 : 18.360 -> 00 : 01 : 32.189
z [2] = W [2] a [1] + b [2]이므로,

30
00 : 01 : 24.470 -> 00 : 01 : 34.950
2 층 째의 활성화는

31
00 : 01 : 32.189 -> 00 : 01 : 39.180
weight 행렬 × 첫번째 층의 활성화 값

32
00 : 01 : 34.950 -> 00 : 01 : 44.270
이군요,

33
00 : 01 : 39.180 -> 00 : 01 : 49.579
거기에 2 층째의 바이어스 벡터를
더한 것입니다.

34
00 : 01 : 44.270 -> 00 : 01 : 55.770
그리고 a [2]는
z [2]에 활성화 함수를 적용한 것입니다.

35
00 : 01 : 49.579 -> 00 : 01 : 57.990
2 층째는 이상으로 끝입니다.

36
00 : 01 : 55.770 -> 00 : 02 : 00.299
여기에서도 마찬가지로 진행 될거고,

37
00 : 01 : 57.990 -> 00 : 02 : 06.240
출력 레이어, 즉 네 번째 레이어까지 갑니다.

38
00 : 02 : 00.299 -> 00 : 02 : 09.959
z [4]는

39
00 : 02 : 06.240 -> 00 : 02 : 11.780
그 층을 위한 매개 변수

40
00 : 02 : 09.959 -> 00 : 02 : 14.569
× 바로 전 계층(3번째)에서의 활성화

41
00 : 02 : 11.780 -> 00 : 02 : 23.930
+ 바이어스 벡터

42
00 : 02 : 14.569 -> 00 : 02 : 26.720
그리고 이번에도 a [4] = g [4] (z [4])입니다.

43
00 : 02 : 23.930 -> 00 : 02 : 29.900
이것이 예상되는 출력
즉 y hat을 계산하는 방법입니다.

44
00 : 02 : 26.720 -> 00 : 02 : 35.390
하나 말해두고 싶은 것은,

45
00 : 02 : 29.900 -> 00 : 02 : 38.270
여기서 x는 a[0]와 동일하다는 것입니다.

46
00 : 02 : 35.390 -> 00 : 02 : 41.209
왜냐하면, 입력 벡터 x는

47
00 : 02 : 38.270 -> 00 : 02 : 44.000
0 층째의 활성화이기 때문입니다.

48
00 : 02 : 41.209 -> 00 : 02 : 47.000
그래서 x를 지우고 a[0]라고 써보십시오.

49
00 : 02 : 44.000 -> 00 : 02 : 48.709
그러면 이 모든 등식이

50
00 : 02 : 47.000 -> 00 : 02 : 53.980
같은 형태가됩니다.

51
00 : 02 : 48.709 -> 00 : 03 : 02.750
일반적인 규칙은

52
00 : 02 : 53.980 -> 00 : 03 : 05.750
z [l] = W [l] a [l-1] + b [l]

53
00 : 03 : 02.750 -> 00 : 03 : 10.630
그리고 이 층의 활성화는

54
00 : 03 : 05.750 -> 00 : 03 : 16.850
z 값에 활성화 함수가 적용된 것입니다.

55
00 : 03 : 10.630 -> 00 : 03 : 20.120
이것이 일반적인 Forward propagation의 
계산입니다.

56
00 : 03 : 16.850 -> 00 : 03 : 23.540
이제

57
00 : 03 : 20.120 -> 00 : 03 : 26.299
단일 training 샘플의 이야기는 끝났습니다.

58
00 : 03 : 23.540 -> 00 : 03 : 29.660
이제 모든 training 샘플에서
적용하기 위해서

59
00 : 03 : 26.299 -> 00 : 03 : 32.720
벡터화하면 어떻게 될까요.

60
00 : 03 : 29.660 -> 00 : 03 : 35.030
식은 지금까지와 비슷한 것입니다.

61
00 : 03 : 32.720 -> 00 : 03 : 40.060
첫 번째 층에서는

62
00 : 03 : 35.030 -> 00 : 03 : 48.410
대문자 Z [1] = W [1] X + b [1]에서

63
00 : 03 : 40.060 -> 00 : 03 : 54.650
A [1] = g [1] (Z [1])입니다.

64
00 : 03 : 48.410 -> 00 : 03 : 57.920
X는 A [0]이라는 것을

65
00 : 03 : 54.650 -> 00 : 03 : 59.959
기억하십시오.

66
00 : 03 : 57.920 -> 00 : 04 : 01.850
이것은 열마다 training 샘플이
줄 지어있는 것입니다.

67
00 : 03 : 59.959 -> 00 : 04 : 05.450
이것을 적용해서

68
00 : 04 : 01.850 -> 00 : 04 : 08.269
X를 A[0]로 다시 씁니다.

69
00 : 04 : 05.450 -> 00 : 04 : 08.720
다음 층도

70
00 : 04 : 08.269 -> 00 : 04 : 16.720
같은 느낌입니다.

71
00 : 04 : 08.720 -> 00 : 04 : 21.980
Z [2] = W [2] A [1] + b [2]

72
00 : 04 : 16.720 -> 00 : 04 : 24.530
A [2] = g [2] (Z [2])

73
00 : 04 : 21.980 -> 00 : 04 : 28.370
우리는 벡터 z 와 a 값들을 

74
00 : 04 : 24.530 -> 00 : 04 : 29.810
정렬 할 뿐입니다.

75
00 : 04 : 28.370 -> 00 : 04 : 34.310
이것은 첫 번째 샘플 z 벡터,

76
00 : 04 : 29.810 -> 00 : 04 : 37.310
이것은 두 번째 샘플 z 벡터 ...

77
00 : 04 : 34.310 -> 00 : 04 : 39.830
라는 느낌으로 m 개의 층까지 이어서

78
00 : 04 : 37.310 -> 00 : 04 : 43.700
그들이 열로 정렬 된 것이

79
00 : 04 : 39.830 -> 00 : 04 : 47.390
대문자 Z라는 행렬입니다.

80
00 : 04 : 43.700 -> 00 : 04 : 50.000
대문자 A도 대문자 X와 마찬가지로

81
00 : 04 : 47.390 -> 00 : 04 : 52.040
training 샘플이 열이 되어

82
00 : 04 : 50.000 -> 00 : 04 : 53.720
왼쪽에서 오른쪽에 줄 지어있는 것입니다.

83
00 : 04 : 52.040 -> 00 : 04 : 59.450
이 공정의 마지막에는

84
00 : 04 : 53.720 -> 00 : 05 : 03.200
Y hat = g (Z [4])이고,

85
00 : 04 : 59.450 -> 00 : 05 : 04.670
이것은 A [4]와 동일합니다.

86
00 : 05 : 03.200 -> 00 : 05 : 08.000
이것이 모든 training 샘플의 예측 값을

87
00 : 05 : 04.670 -> 00 : 05 : 09.980
수평 방향으로 정렬 한 것입니다.

88
00 : 05 : 08.000 -> 00 : 05 : 12.590
이제 표기법을 정리합시다.

89
00 : 05 : 09.980 -> 00 : 05 : 17.720
위에 적혀있는 표기법이지만,

90
00 : 05 : 12.590 -> 00 : 05 : 19.820
소문자 z 및 a를

91
00 : 05 : 17.720 -> 00 : 05 : 22.070
대문자로 바꿀 수 있습니다.

92
00 : 05 : 19.820 -> 00 : 05 : 23.810
즉 여기가 대문자 Z가 되네요,

93
00 : 05 : 22.070 -> 00 : 05 : 25.790
지금까지 벡터화를 사용하여
training 샘플 전체에 대해서

94
00 : 05 : 23.810 -> 00 : 05 : 29.060
단번에 Forward prop하는 
방법을 보고 왔습니다.

95
00 : 05 : 25.790 -> 00 : 05 : 32.990
A[0]는 X 였습니다.

96
00 : 05 : 29.060 -> 00 : 05 : 35.240
이 벡터화 구현을 보면,

97
00 : 05 : 32.990 -> 00 : 05 : 37.670
여기에 for 루프를 붙이면

98
00 : 05 : 35.240 -> 00 : 05 : 40.370
좋을듯합니다.

99
00 : 05 : 37.670 -> 00 : 05 : 44.360
for l = 1 ... 4, 즉

100
00 : 05 : 40.370 -> 00 : 05 : 47.000
l이 1에서 대문자 L이 될 때까지 반복합니다.

101
00 : 05 : 44.360 -> 00 : 05 : 48.950
그리고 첫번째 층의 활성화를 계산하고

102
00 : 05 : 47.000 -> 00 : 05 : 51.860
2 층째, 3 층째 네 번째 레이어 ... 계속됩니다.

103
00 : 05 : 48.950 -> 00 : 05 : 54.370
그래서, 여기에 for 루프를두면

104
00 : 05 : 51.860 -> 00 : 05 : 56.660
문제 될 것이 없습니다.

105
00 : 05 : 54.370 -> 00 : 05 : 58.550
물론,
신경망을 구현할 때

106
00 : 05 : 56.660 -> 00 : 06 : 00.770
가급적 for 루프를 피하고 싶은 것은
알고 있습니다.

107
00 : 05 : 58.550 -> 00 : 06 : 03.290
그러나 여기에서는 for 루프를 사용하는것 말고는

108
00 : 06 : 00.770 -> 00 : 06 : 05.060
구현하는 방법이 없다고

109
00 : 06 : 03.290 -> 00 : 06 : 06.590
생각 되는 장소이므로,

110
00 : 06 : 05.060 -> 00 : 06 : 09.080
Forward prop를 구현할 때

111
00 : 06 : 06.590 -> 00 : 06 : 10.700
여기에 for 루프를 써도 전혀 상관 없습니다.

112
00 : 06 : 09.080 -> 00 : 06 : 12.740
이렇게함으로써,

113
00 : 06 : 10.700 -> 00 : 06 : 15.050
1 층째, 2 층째, 3 층째 ...를
계산 할 수 있습니다.

114
00 : 06 : 12.740 -> 00 : 06 : 17.210
for 루프를 사용하는것 이외에

115
00 : 06 : 15.050 -> 00 : 06 : 19.970
l을 1부터 대문자 L,

116
00 : 06 : 17.210 -> 00 : 06 : 23.060
즉 신경망의
층 수만큼 돌릴 방법을

117
00 : 06 : 19.970 -> 00 : 06 : 24.620
아는 사람은 없고

118
00 : 06 : 23.060 -> 00 : 06 : 27.830
나도 모릅니다.

119
00 : 06 : 24.620 -> 00 : 06 : 30.980
그래서, 여기에 for 루프를 사용하는 것은

120
00 : 06 : 27.830 -> 00 : 06 : 32.690
아무 문제가 없습니다.

121
00 : 06 : 30.980 -> 00 : 06 : 35.300
지금까지,

122
00 : 06 : 32.690 -> 00 : 06 : 37.760
딥 신경망의 표기법과

123
00 : 06 : 35.300 -> 00 : 06 : 39.680
Forward prop이 끝났습니다.

124
00 : 06 : 37.760 -> 00 : 06 : 41.900
이러한 것을 당신이 조금

125
00 : 06 : 39.680 -> 00 : 06 : 44.000
친밀감을 느낀다면,

126
00 : 06 : 41.900 -> 00 : 06 : 45.830
숨겨진 레이어가 하나인 신경망에서

127
00 : 06 : 44.000 -> 00 : 06 : 47.750
한 것을 가지고 와서

128
00 : 06 : 45.830 -> 00 : 06 : 50.750
이와 유사한 것을

129
00 : 06 : 47.750 -> 00 : 06 : 53.420
반복했을 뿐이기 때문입니다.

130
00 : 06 : 50.750 -> 00 : 06 : 55.420
딥 신경망을

131
00 : 06 : 53.420 -> 00 : 06 : 57.860
구현하는 가운데,

132
00 : 06 : 55.420 -> 00 : 06 : 59.450
버그가 없이 구현하는 방법 중 하나는

133
00 : 06 : 57.860 -> 00 : 07 : 01.580
행렬의 크기에 대해

134
00 : 06 : 59.450 -> 00 : 07 : 03.500
신중하게

135
00 : 07 : 01.580 -> 00 : 07 : 05.300
생각하는 것입니다.

136
00 : 07 : 03.500 -> 00 : 07 : 07.280
제가 코드를 쓸 때는,

137
00 : 07 : 05.300 -> 00 : 07 : 08.960
종이에 써보면서

138
00 : 07 : 07.280 -> 00 : 07 : 11.480
지금 다루고있는 행렬의 크기를

139
00 : 07 : 08.960 -> 00 : 07 : 13.940
항상 주의 깊게 생각합니다.

140
00 : 07 : 11.480 -> 00 : 07 : 16.570
다음 동영상에서

141
00 : 07 : 13.940 -> 00 : 07 : 16.570
그 방법을 알아보겠습니다.