WEBVTT

1
00 : 00 : 00.000 -> 00 : 00 : 01.350
이전 강의에서

2
00 : 00 : 01.350 -> 00 : 00 : 05.065
우리는 심층 신경 네트워크를 구축하는 데 
필요한 기본 모듈을 배웠습니다.

3
00 : 00 : 05.065 -> 00 : 00 : 07.696
각 레이어에 해당하는 Forward prop 단계,

4
00 : 00 : 07.696 -> 00 : 00 : 09.975
그리고 대응하는 backward prop 단계가 있죠.

5
00 : 00 : 09.975 -> 00 : 00 : 12.370
이 단원에서는 이러한 단계를 구현하는 
방법에 대해 설명합니다.

6
00 : 00 : 12.370 -> 00 : 00 : 14.230
우선, Forward prop에 대해 복습해보죠.

7
00 : 00 : 14.230 -> 00 : 00 : 20.045
이 단계는 a [l-1]을 입력하고 
a[l]을 출력합니다.

8
00 : 00 : 20.045 -> 00 : 00 : 21.795
그리고 z [l]을 cache에 넣습니다.

9
00 : 00 : 21.795 -> 00 : 00 : 24.615
실제로는

10
00 : 00 : 24.615 -> 00 : 00 : 28.145
w [l]과 b [l]도 캐시에 넣을 것입니다.

11
00 : 00 : 28.145 -> 00 : 00 : 31.585
이렇게 해야 함수를 활용하기 쉬울 것입니다.

12
00 : 00 : 31.585 -> 00 : 00 : 35.438
다음 공식은 이미 익숙할 겁니다.

13
00 : 00 : 35.438 -> 00 : 00 : 46.813
forward 함수는 z [1] = w [l] * a [l-1] + b [l]

14
00 : 00 : 46.813 -> 00 : 00 : 53.620
여기서 a [1]은 z [1]에 작용하는 활성화 함수이고,

15
00 : 00 : 53.620 -> 00 : 00 : 57.250
벡터화를 사용하려면

16
00 : 00 : 57.250 -> 00 : 01 : 06.218
즉, Z [1] = W [1] * A [1- 1] + b [1]

17
00 : 01 : 06.218 -> 00 : 01 : 09.930
여기에 첨부 된 b는 Python broadcasting을 
통해 구현 될 수 있습니다.

18
00 : 01 : 09.930 -> 00 : 01 : 15.296
A [1]은 행렬 Z [l]의 각 원소에 
활성화 함수를 적용한 결과입니다.

19
00 : 01 : 15.296 -> 00 : 01 : 20.128
여러분은 Forward prop 맵에서 
기억해야 할 것이 있습니다,

20
00 : 01 : 20.128 -> 00 : 01 : 22.565
우리가 이 같은 Forward prop 흐름도를 
작성했던 것을 기억할 겁니다.

21
00 : 01 : 22.565 -> 00 : 01 : 26.775
a[0]을 입력하여 초기화 하는 겁니다.

22
00 : 01 : 26.775 -> 00 : 01 : 29.305
여기서 a [0]은 X입니다.

23
00 : 01 : 29.305 -> 00 : 01 : 31.440
그래서 이것은 초기화되었습니다.

24
00 : 01 : 31.440 -> 00 : 01 : 33.380
이것은 처음에 시작되는 입력 값입니다.

25
00 : 01 : 33.380 -> 00 : 01 : 40.207
training 샘플이 하나 뿐인 경우

26
00 : 01 : 40.207 -> 00 : 01 : 42.255
a [0]은 이 한개의 training 샘플의 
입력 feature 입니다.

27
00 : 01 : 42.255 -> 00 : 01 : 45.800
전체 training 세트를 다루는 경우

28
00 : 01 : 45.800 -> 00 : 01 : 48.215
A[0]가 전체 training 세트의 

29
00 : 01 : 48.215 -> 00 : 01 : 51.990
입력 feature을 나타냅니다.

30
00 : 01 : 51.990 -> 00 : 01 : 53.970
Forward prop 맵의 초기 입력 값입니다.

31
00 : 01 : 53.970 -> 00 : 01 : 57.085
이를 통해서 왼쪽에서 오른쪽으로 
Forward prop를 계산할 수 있습니다.

32
00 : 01 : 57.085 -> 00 : 02 : 00.110
그 다음은 backward prop에 대해 얘기해보죠. 

33
00 : 02 : 00.110 -> 00 : 02 : 03.585
입력 값으로는 da [l]

34
00 : 02 : 03.585 -> 00 : 02 : 08.040
출력 값은 da [l-1] 및 dW [l] 및 db [l]

35
00 : 02 : 08.040 -> 00 : 02 : 16.240
이것 들을 계산하는 단계는 

36
00 : 02 : 16.240 -> 00 : 02 : 23.866
dz [1] = da [l] * g [l]' (z [1])

37
00 : 02 : 23.866 -> 00 : 02 : 27.405
그런 다음

38
00 : 02 : 27.405 -> 00 : 02 : 34.420
dW [l] = dz [l] * a [l-1]

39
00 : 02 : 34.420 -> 00 : 02 : 37.445
저는 이 a[l-1]을 cache에 넣지는 않을겁니다.

40
00 : 02 : 37.445 -> 00 : 02 : 39.114
이것이 반드시 필요할 것이기 때문입니다.

41
00 : 02 : 39.114 -> 00 : 02 : 47.560
그런 다음 db [l] = dz [l]

42
00 : 02 : 47.560 -> 00 : 02 : 59.089
마지막으로 da [l-1] = W [l] transpose X dz [l]

43
00 : 02 : 59.089 -> 00 : 03 : 02.310
구체적인 미분 과정은 여기에 쓰지 않을 것입니다.

44
00 : 03 : 02.310 -> 00 : 03 : 06.605
하지만 당신이 da의 정의를 여기에 넣으면 됩니다.

45
00 : 03 : 06.605 -> 00 : 03 : 10.260
그러면 우리는 이전에 수업에서 
본 수식을 얻을 것입니다.

46
00 : 03 : 10.260 -> 00 : 03 : 16.617
즉, 이전 dz [l]을 사용하여 
새로운 dz [l]을 만들어 냅니다.

47
00 : 03 : 16.617 -> 00 : 03 : 18.135
da[l-1]을 여기로 대입 하면

48
00 : 03 : 18.135 -> 00 : 03 : 33.817
dz [l] = w [l + 1] transpose X dz [l + 1] * g [l]'(z [l])

49
00 : 03 : 33.817 -> 00 : 03 : 36.165
많은 수식이 있는 것처럼 보입니다.

50
00 : 03 : 36.165 -> 00 : 03 : 38.360
걱정하지 마세요. 이 수식은 전에 본 적이 있습니다.

51
00 : 03 : 38.360 -> 00 : 03 : 40.820
지난 주 우리는

52
00 : 03 : 40.820 -> 00 : 03 : 43.055
1 개의 숨겨진 레이어의 신경망을 학습 할 때

53
00 : 03 : 43.055 -> 00 : 03 : 45.930
back prop의 방정식을 배웠습니다.

54
00 : 03 : 45.930 -> 00 : 03 : 48.602
여기서 기억할 것은 element wise
(행렬요소를 하나씩 곱하는 연산) 정도입니다.

55
00 : 03 : 48.602 -> 00 : 03 : 54.950
따라서 사실상 back prop 함수를 계산하기 
위해서는 이 네 개의 방정식만 필요합니다.

56
00 : 03 : 54.950 -> 00 : 03 : 58.735
마지막으로 벡터화 된 버전을 작성해 보겠습니다.

57
00 : 03 : 58.735 -> 00 : 04 : 04.135
첫 번째 줄은

58
00 : 04 : 04.135 -> 00 : 04 : 11.045
dZ [1] = dA [1] * g [1]'(Z [1])

59
00 : 04 : 11.045 -> 00 : 04 : 13.060
이해가시죠?

60
00 : 04 : 13.060 -> 00 : 04 : 23.715
dW [l] = 1 / m dZ [l] * A [l-1] transpose

61
00 : 04 : 23.715 -> 00 : 04 : 30.844
다음으로

62
00 : 04 : 30.844 -> 00 : 04 : 37.970
db [1] = 1 / m Np.sum (dZ [1], axis = 1, keepdims = True)

63
00 : 04 : 37.970 -> 00 : 04 : 44.095
지난 주에 np.sum을 사용하여 db를 
계산하는 방법에 대해 이야기했습니다.

64
00 : 04 : 44.095 -> 00 : 04 : 56.155
마지막으로 
dA [l - 1] = W [l] transpose X dZ [l]

65
00 : 04 : 56.155 -> 00 : 05 : 02.435
이 모델은 da [l]를 여기에 입력하고

66
00 : 05 : 02.435 -> 00 : 05 : 07.950
dW [l] 및 db [l]를 이렇게 출력합니다.

67
00 : 05 : 07.950 -> 00 : 05 : 10.100
그리고 da [l-1] 까지.

68
00 : 05 : 10.100 -> 00 : 05 : 16.022
이것들은 당신이 필요로 하는 미분들 입니다.

69
00 : 05 : 16.022 -> 00 : 05 : 18.905
이것은 backward 함수를 만드는 방법입니다.

70
00 : 05 : 18.905 -> 00 : 05 : 20.560
정리해 보겠습니다.

71
00 : 05 : 20.560 -> 00 : 05 : 23.585
입력 x를 가져와서

72
00 : 05 : 23.585 -> 00 : 05 : 25.060
ReLU 활성화 함수 등이 있는

73
00 : 05 : 25.060 -> 00 : 05 : 28.445
첫번째 레이어에 넣고

74
00 : 05 : 28.445 -> 00 : 05 : 30.570
다른 ReLU 활성화 함수를 사용할 수도 있는

75
00 : 05 : 30.570 -> 00 : 05 : 33.340
2번째 레이어에 넣고

76
00 : 05 : 33.340 -> 00 : 05 : 35.175
다음 3번째 레이어에 넣습니다.

77
00 : 05 : 35.175 -> 00 : 05 : 39.596
binary classification을 원한다면 여기서 
Sigmoid 활성화 함수를 사용할 수도 있겠죠.

78
00 : 05 : 39.596 -> 00 : 05 : 41.875
그리고 출력 값인 y hat이 나오고

79
00 : 05 : 41.875 -> 00 : 05 : 43.890
y hat을 통해

80
00 : 05 : 43.890 -> 00 : 05 : 46.265
손실을 계산할 수 있습니다.

81
00 : 05 : 46.265 -> 00 : 05 : 49.685
그런 다음 뒤로 반복 할 수 있습니다.

82
00 : 05 : 49.685 -> 00 : 05 : 51.775
먼저 화살표를 그립니다.

83
00 : 05 : 51.775 -> 00 : 05 : 54.320
펜을 바꾸는게 귀찮거든요

84
00 : 05 : 54.320 -> 00 : 06 : 03.430
여기에서 back prop를 사용하여 미분을 계산합니다.

85
00 : 06 : 03.430 -> 00 : 06 : 16.063
dw [3] db [3] dw [2] db [2] dw [1] db [1]

86
00 : 06 : 16.063 -> 00 : 06 : 18.865
이 과정에서

87
00 : 06 : 18.865 -> 00 : 06 : 24.820
캐시에서 z [1] z [2] z [3]을 이용합니다.

88
00 : 06 : 24.820 -> 00 : 06 : 32.250
뒤로가면서 da [2]와 da [1]

89
00 : 06 : 32.250 -> 00 : 06 : 34.730
그런 다음 da [0]이 나오지만

90
00 : 06 : 34.730 -> 00 : 06 : 35.880
그러나 이것은 필요가 없으니까.

91
00 : 06 : 35.880 -> 00 : 06 : 37.935
그래서 그것을 잘 제거합니다.

92
00 : 06 : 37.935 -> 00 : 06 : 40.785
여기까지가 위의 3 레이어 신경망의

93
00 : 06 : 40.785 -> 00 : 06 : 44.040
forward prop과 backward prop을 실현하는 방법입니다.

94
00 : 06 : 44.040 -> 00 : 06 : 46.140
여전히 마지막 세부 사항이 있습니다.

95
00 : 06 : 46.140 -> 00 : 06 : 48.735
우리는 Forward prop의 반복에서

96
00 : 06 : 48.735 -> 00 : 06 : 52.420
입력 데이터 X로 초기화 합니다.

97
00 : 06 : 52.420 -> 00 : 06 : 54.090
그렇다면 bacr prop는 어떻게 초기화 될까요?

98
00 : 06 : 54.090 -> 00 : 06 : 59.235
로지스틱 회귀 분석을 사용할 때

99
00 : 06 : 59.235 -> 00 : 07 : 01.065
binary classification을 수행 할 때

100
00 : 07 : 01.065 -> 00 : 07 : 02.977
다음의 식인

101
00 : 07 : 02.977 -> 00 : 07 : 09.685
Da [1] = -y / a + (1-y) / (1-a)이

102
00 : 07 : 09.685 -> 00 : 07 : 12.575
나오게 됩니다.

103
00 : 07 : 12.575 -> 00 : 07 : 14.180
y hat 에 관한 손실 함수를 미분한

104
00 : 07 : 14.180 -> 00 : 07 : 17.475
결과가 바로 이 양식입니다.

105
00 : 07 : 17.475 -> 00 : 07 : 19.105
미적분에 익숙하다면

106
00 : 07 : 19.105 -> 00 : 07 : 21.328
손실 함수 L의 y hat 이나 a에 대한

107
00 : 07 : 21.328 -> 00 : 07 : 24.150
미분을 유도할 수 있을겁니다.

108
00 : 07 : 24.150 -> 00 : 07 : 26.505
그러면 당신은 이 공식을 얻을 것입니다.

109
00 : 07 : 26.505 -> 00 : 07 : 31.350
이 수식은 최종 레이어 L의 da 값에 
사용되는 공식입니다.

110
00 : 07 : 31.350 -> 00 : 07 : 35.715
물론 벡터화를 사용하려는 경우

111
00 : 07 : 35.715 -> 00 : 07 : 38.351
초기화는 da[l]이 아니라 dA[1]로 합니다.

112
00 : 07 : 38.351 -> 00 : 07 : 43.706
물론 이 공식을 레이어 L에서 
dA [1]로 나타내면

113
00 : 07 : 43.706 -> 00 : 07 : 48.465
다른 예도 마찬가지로 적용될 겁니다.

114
00 : 07 : 48.465 -> 00 : 07 : 54.011
첫 번째 training 샘플 에서는
dA [1] = (- y [1] / a [1]) + (1-y [1]) / (1-a [1])

115
00 : 07 : 54.011 -> 00 : 07 : 55.285
이고

116
00 : 07 : 55.285 -> 00 : 07 : 58.153
첫 번째 traing 샘플부터

117
00 : 07 : 58.153 -> 00 : 08 : 05.185
m 번째 training 샘플의 값까지 더합니다.

118
00 : 08 : 05.185 -> 00 : 08 : 09.423
이것이 벡터화를 달성하는 방법입니다.

119
00 : 08 : 09.423 -> 00 : 08 : 13.055
즉, backward prop의 벡터화 된 버전을 
초기화 하는 방법 입니다.

120
00 : 08 : 13.055 -> 00 : 08 : 16.100
이제 우리는 forward prop와

121
00 : 08 : 16.100 -> 00 : 08 : 20.030
back prop의 기초에 대해 배웠습니다.

122
00 : 08 : 20.030 -> 00 : 08 : 22.340
이 수식들을 사용한다면

123
00 : 08 : 22.340 -> 00 : 08 : 24.530
여러분은 필요한 미분값을 찾기 위한

124
00 : 08 : 24.530 -> 00 : 08 : 27.640
forward prop과 backward prop를 
잘 구현할 수 있을겁니다.

125
00 : 08 : 27.640 -> 00 : 08 : 29.660
너무 많은 수식이 이해하기 쉽지 않다고 
생각할 수도 있습니다.

126
00 : 08 : 29.660 -> 00 : 08 : 32.080
혼란 스럽고, 이 공식이 
무엇을 할 수 있는지 모르겠다 던지 등의

127
00 : 08 : 32.080 -> 00 : 08 : 34.645
문제가 있다면,

128
00 : 08 : 34.645 -> 00 : 08 : 37.205
이번 주 프로그래밍 연습을 할 때

129
00 : 08 : 37.205 -> 00 : 08 : 40.175
이 단계들을 직접 구현해 본다면,

130
00 : 08 : 40.175 -> 00 : 08 : 42.020
이것은 당신의 이해를 더욱 깊게 할 것입니다.

131
00 : 08 : 42.020 -> 00 : 08 : 43.805
이 단원에는 많은 수식이 있습니다.

132
00 : 08 : 43.805 -> 00 : 08 : 46.265
일부 수식은 이해하기 쉽지 않습니다.

133
00 : 08 : 46.265 -> 00 : 08 : 49.055
가능하다면 직접 계산을 해보는 것이 좋습니다.

134
00 : 08 : 49.055 -> 00 : 08 : 50.905
선형 대수 지식으로 말이죠.

135
00 : 08 : 50.905 -> 00 : 08 : 52.430
하지만 이것이 어렵다는 것을 알기 때문에 
반드시 해야 된다는건 아닙니다.

136
00 : 08 : 52.430 -> 00 : 08 : 56.390
사실, 이것은 머신러닝에서는 더 어려운 미분이 됩니다.

137
00 : 08 : 56.390 -> 00 : 08 : 57.950
이 단원에 나열된 수식

138
00 : 08 : 57.950 -> 00 : 09 : 02.685
또는 미적분 공식은 backward prop의 미분값 일뿐입니다.

139
00 : 09 : 02.685 -> 00 : 09 : 04.750
다시 말하지만,이 수식이 조금 추상적으로 보인다면

140
00 : 09 : 04.750 -> 00 : 09 : 06.400
이해 하기 쉽지 않다면,

141
00 : 09 : 06.400 -> 00 : 09 : 09.108
내 제안은 프로그래밍 과제를 끝내는 것입니다.

142
00 : 09 : 09.108 -> 00 : 09 : 11.465
그러면 갑자기 보일 것입니다.

143
00 : 09 : 11.465 -> 00 : 09 : 14.120
저는 요즘에도 제가

144
00 : 09 : 14.120 -> 00 : 09 : 16.805
머신러닝 알고리즘을 구현할 때

145
00 : 09 : 16.805 -> 00 : 09 : 18.000
때때로 놀랐습니다.

146
00 : 09 : 18.000 -> 00 : 09 : 21.170
내 머신러닝 알고리즘이 효과적임이 
입증되기 때문입니다.

147
00 : 09 : 21.170 -> 00 : 09 : 25.670
머신러닝의 복잡성은 라인 단위 코드가 
아닌 데이터에서 비롯되기 때문에

148
00 : 09 : 25.670 -> 00 : 09 : 27.095
그래서 때때로 당신은

149
00 : 09 : 27.095 -> 00 : 09 : 28.685
몇 줄의 코드를 썼습니다.

150
00 : 09 : 28.685 -> 00 : 09 : 30.110
이 코드들이 하는 일을 확신하지는 못했지만,

151
00 : 09 : 30.110 -> 00 : 09 : 31.625
결국 그들은 놀라운 결과를 만들어 냈습니다.

152
00 : 09 : 31.625 -> 00 : 09 : 35.296
사실 대부분의 작업이 일어나는 장소는

153
00 : 09 : 35.296 -> 00 : 09 : 37.090
당신이 쓴 몇 줄의 짧은 코드는 아닙니다.

154
00 : 09 : 37.090 -> 00 : 09 : 38.705
(이게 짧은건 아닐지도 모르겠네요)

155
00 : 09 : 38.705 -> 00 : 09 : 40.730
(근데 수천, 수만줄은 아니니까요 ㅎㅎ)

156
00 : 09 : 40.730 -> 00 : 09 : 42.115
이와 같은 코드에서 비롯되는게 아니라

157
00 : 09 : 42.115 -> 00 : 09 : 44.750
많은 양의 데이터에서 비롯되는 겁니다.

158
00 : 09 : 44.750 -> 00 : 09 : 46.850
제가 수년 동안 머신러닝에 종사했지만

159
00 : 09 : 46.850 -> 00 : 09 : 49.400
알고리즘의 복잡성은

160
00 : 09 : 49.400 -> 00 : 09 : 53.274
데이터에서 비롯 되어 

161
00 : 09 : 53.274 -> 00 : 09 : 55.970
제 머신러닝 알고리즘이 작동한다는 사실에

162
00 : 09 : 55.970 -> 00 : 10 : 01.020
때때로 저는 아직도 놀랍니다

163
00 : 10 : 01.020 -> 00 : 10 : 05.935
그래요, 이것이 심층 신경 네트워크를 
구현하는 방법입니다.

164
00 : 10 : 05.935 -> 00 : 10 : 10.325
수업 후에 프로그래밍 과제를 하고 나면 
더 많이 알게 될 것임을 모두 기억 하십시오.

165
00 : 10 : 10.325 -> 00 : 10 : 14.220
다음 동영상에서 다음 주제로 이동하기 전에

166
00 : 10 : 14.220 -> 00 : 10 : 17.480
하이퍼 파라미터와 파라미터에 대해 논의 할 것입니다.

167
00 : 10 : 17.480 -> 00 : 10 : 19.681
네트워크에서 깊이 훈련 할 때

168
00 : 10 : 19.681 -> 00 : 10 : 22.225
하이퍼 매개 변수를 올바르게 조정할 수 있다면

169
00 : 10 : 22.225 -> 00 : 10 : 25.400
심층 신경 네트워크 개발 작업을보다 
효율적으로 만들 것입니다.

170
00 : 10 : 25.400 -> 00 : 10 : 29.000
다음 비디오로 넘어갑시다.