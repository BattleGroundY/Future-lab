WEBVTT

1
00 : 00 : 00.028 -> 00 : 00 : 04.605
심층 신경망을 구현할 때
코드가 맞는지 확인하기 위해

2
00 : 00 : 04.605 -> 00 : 00 : 08.118
제가 자주 사용하는 디버깅(버그를 확인) 
도구 중 하나는, 종이를 가져와

3
00 : 00 : 08.118 -> 00 : 00 : 11.727
자신이 다루고있는 행렬의 차원을
확인 해 나가는 방법입니다

4
00 : 00 : 11.727 -> 00 : 00 : 15.895
저는 당신도 보다 쉽게 심층 신경망을 
구현할 수 있게 되길 바라기 때문에

5
00 : 00 : 15.895 -> 00 : 00 : 18.275
그 방법을 설명합니다.

6
00 : 00 : 18.275 -> 00 : 00 : 23.174
대문자 L은 입력 층을 포함하지 않기 때문에
5가 되네요

7
00 : 00 : 23.174 -> 00 : 00 : 27.390
5 개의 층이 있습니다
4 개의 숨겨진 레이어와 1 개의 출력 계층입니다

8
00 : 00 : 27.390 -> 00 : 00 : 34.878
그래서 Forward prop를 구현하면

9
00 : 00 : 34.878 -> 00 : 00 : 41.408
첫 번째 단계는
z [1] = W [1] x + b [1]입니다

10
00 : 00 : 41.408 -> 00 : 00 : 48.144
바이어스 항 b를 무시하고
매개 변수 W에 주목합시다.

11
00 : 00 : 48.144 -> 00 : 00 : 54.501
이 첫 번째의 숨겨진 레이어는
3개의 유닛이 있습니다

12
00 : 00 : 54.501 -> 00 : 00 : 59.517
이것이 0 레이어,
그리고 1,2,3,4,5 번째 레이어입니다

13
00 : 00 : 59.517 -> 00 : 01 : 05.741
이전 동영상에서의 표기법을 사용하면

14
00 : 01 : 05.741 -> 00 : 01 : 11.265
1 층 째의 숨겨진 유닛의 수 인 n[1]은
3입니다

15
00 : 01 : 11.265 -> 00 : 01 : 16.202
n [2] = 5

16
00 : 01 : 16.202 -> 00 : 01 : 23.018
n [3] = 4, n [4]는 2,
n [5]은 1입니다

17
00 : 01 : 23.018 -> 00 : 01 : 27.715
여기까지는 출력 장치가 하나 뿐인
신경망 이었지만

18
00 : 01 : 27.715 -> 00 : 01 : 32.497
이후 과정은 출력 계층이 여러개
있는 것에 대해서도 이야기합니다

19
00 : 01 : 32.497 -> 00 : 01 : 36.989
마지막으로 입력 층은

20
00 : 01 : 36.989 -> 00 : 01 : 40.443
n [0] = nx = 2입니다

21
00 : 01 : 40.443 -> 00 : 01 : 45.860
이제 z, W x의 차원을 생각합시다

22
00 : 01 : 45.860 -> 00 : 01 : 49.120
z는 첫 번째의 숨겨진 레이어의
활성화 벡터이므로

23
00 : 01 : 49.120 -> 00 : 01 : 54.244
z는 3 × 1이됩니다

24
00 : 01 : 54.244 -> 00 : 01 : 58.675
즉 3 차원 벡터입니다

25
00 : 01 : 58.675 -> 00 : 02 : 03.093
n [1] × 1 차원 벡터라고 쓰겠습니다.

26
00 : 02 : 03.093 -> 00 : 02 : 08.546
n [1] × 1 차원 행렬 입니다.
이 경우 3 × 1입니다

27
00 : 02 : 08.546 -> 00 : 02 : 12.319
그러면 입력 feature x는 어떨까요
두 개의 feature가 있습니다

28
00 : 02 : 12.319 -> 00 : 02 : 18.622
따라서 이 예에서는 x는 2 × 1이고
일반화하면 n [0] × 1이됩니다

29
00 : 02 : 18.622 -> 00 : 02 : 24.082
그러자 W [1] 과
n [0] × 1 를 곱할 경우에

30
00 : 02 : 24.082 -> 00 : 02 : 30.181
n [1] × 1 벡터인 행렬이 되는군요

31
00 : 02 : 30.181 -> 00 : 02 : 34.747
3 차원 벡터는

32
00 : 02 : 34.747 -> 00 : 02 : 38.600
뭔가에 2 차원 벡터를 곱한 것과
같습니다

33
00 : 02 : 38.600 -> 00 : 02 : 42.993
행렬 곱셈의 법칙을 생각하면

34
00 : 02 : 42.993 -> 00 : 02 : 46.041
이것은 3 × 2 행렬이 됩니다

35
00 : 02 : 46.041 -> 00 : 02 : 51.138
3 × 2 행렬에
2 × 1 행렬 또는 벡터를 곱하면

36
00 : 02 : 51.138 -> 00 : 02 : 56.249
3 × 1 벡터를 얻을 수 있기 때문입니다

37
00 : 02 : 56.249 -> 00 : 03 : 02.771
일반화하면 이것은
n [1] × n [0] 차원의 행렬입니다

38
00 : 03 : 02.771 -> 00 : 03 : 07.167
여기에서 알 수 있는 것은.

39
00 : 03 : 07.167 -> 00 : 03 : 12.665
W [1]은 n [1] × n [0] 벡터라는 것입니다.

40
00 : 03 : 12.665 -> 00 : 03 : 20.191
더 일반적으로 말하면
W [l]은 n [l] × n [l-1] 차원이 됩니다

41
00 : 03 : 20.191 -> 00 : 03 : 26.021
예를 들어 이 경우 W [2]는

42
00 : 03 : 26.021 -> 00 : 03 : 31.508
5 × 3 차원입니다

43
00 : 03 : 31.508 -> 00 : 03 : 35.119
다시말하면,
n [2] × n [1] 차원인 것입니다

44
00 : 03 : 35.119 -> 00 : 03 : 40.036
왜냐하면 z [2]는

45
00 : 03 : 40.036 -> 00 : 03 : 45.132
W [2] × a [1]으로 계산 되기 때문입니다

46
00 : 03 : 45.132 -> 00 : 03 : 50.059
여기에서도 바이어스 항은 무시합시다

47
00 : 03 : 50.059 -> 00 : 03 : 54.584
이것은 3 × 1이고

48
00 : 03 : 54.584 -> 00 : 03 : 59.432
이것은 5 × 1이 되므로

49
00 : 03 : 59.432 -> 00 : 04 : 03.169
이것은 5 × 3이 될 것이기 때문입니다

50
00 : 04 : 03.169 -> 00 : 04 : 10.273
마찬가지로 W [3]도

51
00 : 04 : 10.273 -> 00 : 04 : 15.501
다음 계층의 차원 수 쉼표
전 층의 차수가되므로

52
00 : 04 : 15.501 -> 00 : 04 : 19.266
이것은 4 × 5 네요

53
00 : 04 : 22.055 -> 00 : 04 : 27.489
W [4]는 2 × 4

54
00 : 04 : 27.489 -> 00 : 04 : 34.405
그리고 W [5]는 1 × 2가됩니다
아시겠나요?

55
00 : 04 : 34.405 -> 00 : 04 : 38.730
즉 확인해야 할 것은

56
00 : 04 : 38.730 -> 00 : 04 : 43.416
l 레이어에 대한 행렬을
구현할 때

57
00 : 04 : 43.416 -> 00 : 04 : 48.475
그 행렬이 n [l] × n [l-1]로되어 있는지
를 확인하는 것입니다.

58
00 : 04 : 48.475 -> 00 : 04 : 55.362
이제 벡터 b의 차수에 대해서도
생각합시다

59
00 : 04 : 55.362 -> 00 : 05 : 01.017
이것은 3 × 1 벡터이므로
3 × 1 벡터를 얻으려면

60
00 : 05 : 01.017 -> 00 : 05 : 06.008
다른 3 × 1 벡터
더해야 합니다

61
00 : 05 : 06.008 -> 00 : 05 : 11.287
이 예 에서는 5 × 1이므로

62
00 : 05 : 11.287 -> 00 : 05 : 14.823
이것도 5 × 1 벡터가 됩니다.

63
00 : 05 : 14.823 -> 00 : 05 : 19.122
그러자 사각형 두 개의 총합이

64
00 : 05 : 19.122 -> 00 : 05 : 22.767
제대로 5 × 1이 됩니다

65
00 : 05 : 22.767 -> 00 : 05 : 30.090
보다 일반적인 규칙으로는
왼쪽의 예 에서는

66
00 : 05 : 30.090 -> 00 : 05 : 35.470
b [1] 은 n [1] × 1 즉 3 × 1 네요

67
00 : 05 : 35.470 -> 00 : 05 : 41.156
두 번째 예 에서는 n [2] × 1입니다

68
00 : 05 : 41.156 -> 00 : 05 : 45.891
일반화하면

69
00 : 05 : 45.891 -> 00 : 05 : 50.637
b [l]은 n [l] × 1 차원이 됩니다.

70
00 : 05 : 50.637 -> 00 : 05 : 56.402
이 두식이
행렬 W와 벡터 b의 차원을

71
00 : 05 : 56.402 -> 00 : 06 : 02.091
확인하는 데 도움이 되었으면 합니다

72
00 : 06 : 02.091 -> 00 : 06 : 06.206
물론 back prop의 구현도

73
00 : 06 : 06.206 -> 00 : 06 : 10.657
dW는 W와 같은 차원이 될겁니다

74
00 : 06 : 10.657 -> 00 : 06 : 16.373
dW, W와 같은 이유로

75
00 : 06 : 16.373 -> 00 : 06 : 22.276
db는 b와 동일한 차원이 됩니다

76
00 : 06 : 22.276 -> 00 : 06 : 28.399
다른 차원 수를 확인해야 할 것은
여기에서는 별로 이야기하지 않았지만

77
00 : 06 : 28.399 -> 00 : 06 : 33.658
z, x, 그리고 a [l]입니다

78
00 : 06 : 33.658 -> 00 : 06 : 39.856
z [l]은 g [l]이 a [l]에
요소마다 적용된 것이므로

79
00 : 06 : 39.856 -> 00 : 06 : 46.914
이런 종류의 네트워크는
z와 a는 같은 차원 수를 가지는 것입니다

80
00 : 06 : 46.914 -> 00 : 06 : 51.582
이제 벡터화를 이용해서 
한 번에 여러 샘플을

81
00 : 06 : 51.582 -> 00 : 06 : 53.258
계산해 보도록 하죠.

82
00 : 06 : 53.258 -> 00 : 06 : 56.092
벡터화 된 구현도

83
00 : 06 : 56.092 -> 00 : 07 : 00.687
물론 W, b dW, db의
차수는 동일합니다

84
00 : 07 : 00.687 -> 00 : 07 : 04.929
그러나 z, a, 그리고 x의 차원은

85
00 : 07 : 04.929 -> 00 : 07 : 09.771
조금 바뀝니다

86
00 : 07 : 09.771 -> 00 : 07 : 13.420
조금 전에는

87
00 : 07 : 13.420 -> 00 : 07 : 18.372
z [1] = W [1] x + b [1]였습니다

88
00 : 07 : 18.372 -> 00 : 07 : 23.845
그리고 이것은 n [1] × 1

89
00 : 07 : 23.845 -> 00 : 07 : 28.276
이것은 n [1] × n [0]

90
00 : 07 : 28.276 -> 00 : 07 : 35.846
x는 n [0] × 1
b는 n [1] × 1이었습니다

91
00 : 07 : 35.846 -> 00 : 07 : 40.979
그리고 벡터화 된

92
00 : 07 : 40.979 -> 00 : 07 : 46.398
구현에서는

93
00 : 07 : 46.398 -> 00 : 07 : 53.536
Z [1] = W [1] X + b [1]라는식이됩니다

94
00 : 07 : 53.536 -> 00 : 07 : 58.023
Z [1]은 각각의 샘플에 대한
z [1]을 나열하여 얻을 수 있습니다

95
00 : 07 : 58.023 -> 00 : 08 : 03.575
즉 z [1] (1), z [1] (2)에서

96
00 : 08 : 03.575 -> 00 : 08 : 10.207
z [1] (m)까지를 이렇게 늘어 놓으면
Z [1]이되는 것입니다

97
00 : 08 : 10.207 -> 00 : 08 : 15.042
그러자 Z [1]은 n [1] × 1 대신

98
00 : 08 : 15.042 -> 00 : 08 : 20.285
n [1] × m입니다
여기서 m은 training 세트의 크기입니다

99
00 : 08 : 20.285 -> 00 : 08 : 26.140
W [1]의 차원 수는 그대로이므로
n [1] × n [0]입니다

100
00 : 08 : 26.140 -> 00 : 08 : 29.201
X는 n [0] × 1 이 아니라

101
00 : 08 : 29.201 -> 00 : 08 : 33.431
모든 training 샘플이
수평으로 정렬 된 것이므로

102
00 : 08 : 33.431 -> 00 : 08 : 38.565
지금은 n [0] × m 차원이 되어 있습니다

103
00 : 08 : 38.565 -> 00 : 08 : 43.833
n [1] × n [0] 행렬에
n [0] × m 행렬을 곱하면

104
00 : 08 : 43.833 -> 00 : 08 : 50.160
예상대로 n [1] × m 행렬을
얻을 수 있습니다.

105
00 : 08 : 50.160 -> 00 : 08 : 55.030
마지막으로
b [1] 은 n [1] × 1으로 남아 있지만

106
00 : 08 : 55.030 -> 00 : 09 : 01.147
이것을 b로 더하면
Python broadcasting에 의해

107
00 : 09 : 01.147 -> 00 : 09 : 08.218
n [1] × m으로 복제되어
요소마다 더해지게됩니다

108
00 : 09 : 08.218 -> 00 : 09 : 14.977
이전 슬라이드에서는 W, b, dW, db의
차원 수에 대해 이야기했습니다

109
00 : 09 : 14.977 -> 00 : 09 : 21.143
여기에서 이야기하고있는 것은
z [l]과 a [l]은

110
00 : 09 : 21.143 -> 00 : 09 : 26.922
n [l] × 1인데 비해

111
00 : 09 : 26.922 -> 00 : 09 : 34.650
대문자 Z [l]과 A [l]은
n [l] × m 이라는 것입니다

112
00 : 09 : 34.650 -> 00 : 09 : 40.410
특별한 것은 l가 0 일 때입니다

113
00 : 09 : 40.410 -> 00 : 09 : 45.188
그 때 A [0]은

114
00 : 09 : 45.188 -> 00 : 09 : 49.543
training 샘플의 feature X와 동일하고

115
00 : 09 : 49.543 -> 00 : 09 : 54.616
예상 하시듯 n [0] × m 차원입니다

116
00 : 09 : 54.616 -> 00 : 10 : 01.259
물론 back prop로 구현하는 경우에도

117
00 : 10 : 01.259 -> 00 : 10 : 06.749
나중에 배우지만
dZ와 dA를 계산합니다

118
00 : 10 : 06.749 -> 00 : 10 : 11.327
이들은 당연히

119
00 : 10 : 11.327 -> 00 : 10 : 15.736
Z와 A와 동일 차수입니다

120
00 : 10 : 15.736 -> 00 : 10 : 19.467
지금까지 본 예제로

121
00 : 10 : 19.467 -> 00 : 10 : 21.685
취급 행렬의 차원 수를 명확하게
구별할 수 있으면 좋겠군요

122
00 : 10 : 21.685 -> 00 : 10 : 25.947
심층 신경망을
구현할 때

123
00 : 10 : 25.947 -> 00 : 10 : 30.350
모든 행렬이 일관되 있는지
확인하는 것은

124
00 : 10 : 30.350 -> 00 : 10 : 31.825
도움이 될 것입니다

125
00 : 10 : 31.825 -> 00 : 10 : 35.908
분명 있을 수 있는 버그의 원인을
잡아줍니다

126
00 : 10 : 35.908 -> 00 : 10 : 40.325
여러가지 행렬의 차원 수를 결정하는
연습들이

127
00 : 10 : 40.325 -> 00 : 10 : 41.979
도움이 되고 있으면 다행입니다

128
00 : 10 : 41.979 -> 00 : 10 : 44.788
심층 신경망을 구현할 때,

129
00 : 10 : 44.788 -> 00 : 10 : 48.241
이러한 행렬과 벡터의 차원 수를
제대로 갖추고 있으면

130
00 : 10 : 48.241 -> 00 : 10 : 52.162
몇 가지 버그의 원인을 지우는 데
도움이 됩니다

131
00 : 10 : 52.162 -> 00 : 10 : 54.467
실제로 내 코드의 버그를 잡는걸
도와주고 있습니다

132
00 : 10 : 54.467 -> 00 : 10 : 58.882
여기까지는 신경망에서의
forward prop

133
00 : 10 : 58.882 -> 00 : 11 : 01.227
방법을 보고 왔습니다

134
00 : 11 : 01.227 -> 00 : 11 : 04.163
그러나 왜 심층 신경망이
이렇게 효과적이고,

135
00 : 11 : 04.163 -> 00 : 11 : 07.243
얕은 것보다 효과가 있는 것인지

136
00 : 11 : 07.243 -> 00 : 11 : 09.939
다음 동영상에서 그것에 대해 얘기합시다.